# üîß Fix: User Feedback Attached to Wrong Span

**Date**: January 3, 2026  
**Issue**: User feedback evaluations not appearing in Datadog LLMObs  
**Root Cause**: Evaluations submitted to HTTP request span instead of LLMObs workflow span  
**Status**: ‚úÖ **FIXED**

---

## üêõ The Problem

### What Was Happening

1. ‚ùå Backend returned **HTTP request span ID** (11233447522943489327)
2. ‚ùå Frontend submitted feedback to this **non-LLMObs span**
3. ‚ùå Datadog rejected/ignored evaluations (not attached to LLMObs span)
4. ‚ùå Evaluations **did not appear** in Datadog LLMObs Evaluations dashboard

### Trace Structure

```
Root: fastapi.request (span_id: 11233447522943489327) ‚Üê ‚ùå Backend WAS returning THIS
  ‚Ü≥ extract_from_images workflow (span_id: 2670203640094394356) ‚Üê ‚úÖ Should return THIS
      ‚Ü≥ google_genai.request (span_id: 10697656502426040050)
          ‚Ü≥ http.request (span_id: 5756564848325201936)
```

### Why Validation Evaluations Worked

- ‚úÖ Submitted from **inside** the `extract_from_images` workflow
- ‚úÖ Used `LLMObs.export_span(span=None)` to get active workflow span automatically
- ‚úÖ Correctly attached to workflow span (2670203640094394356)
- ‚úÖ **Appeared in Datadog LLMObs**

### Why User Feedback Evaluations Failed

- ‚ùå Submitted **after** workflow completed (from endpoint)
- ‚ùå Used `tracer.current_span()` which returned HTTP request span (11233447522943489327)
- ‚ùå Sent to **wrong span** - not an LLMObs span!
- ‚ùå **Did NOT appear in Datadog LLMObs**

---

## ‚úÖ The Fix

### Implementation

**Modified Files**:
1. `services/fastapi-backend/app/services/vote_extraction_service.py`
2. `services/fastapi-backend/app/api/v1/endpoints/vote_extraction.py`

### Step 1: Store Workflow Span Context in Service

Added a field to store the workflow span context:

```python
class VoteExtractionService:
    def __init__(self):
        # ...existing init code...
        self._last_workflow_span_context: dict[str, str] | None = None  # Store span context from workflow
```

### Step 2: Capture Span Context Inside Workflow

Captured the workflow span context **before the workflow returns** (while it's still active):

```python
@workflow
async def extract_from_images(...):
    # ...extraction logic...
    
    # Validate extracted data BEFORE leaving workflow span
    await self._validate_within_workflow(result)

    # ‚úÖ Capture workflow span context for user feedback submission
    # Must be done INSIDE the workflow while the span is still active
    if self._llmobs_enabled and DDTRACE_AVAILABLE:
        try:
            self._last_workflow_span_context = LLMObs.export_span(span=None)
            if self._last_workflow_span_context:
                logger.debug(
                    f"üìä Captured workflow span context: "
                    f"span_id={self._last_workflow_span_context.get('span_id')}, "
                    f"trace_id={self._last_workflow_span_context.get('trace_id')}"
                )
        except Exception as e:
            logger.warning(f"Failed to capture workflow span context: {e}")
            self._last_workflow_span_context = None

    return result
```

### Step 3: Add Method to Retrieve Span Context

```python
def get_workflow_span_context(self) -> dict[str, str] | None:
    """
    Get the most recent workflow span context.

    This returns the span context captured from the last workflow execution.
    Used to associate user feedback with the correct LLMObs workflow span.

    Returns:
        Dictionary with 'span_id' and 'trace_id' as strings, or None if not available
    """
    return self._last_workflow_span_context
```

### Step 4: Update Endpoint to Use Workflow Span

Modified the endpoint to use the workflow span context from the service:

```python
# Get workflow span context for feedback submission
# This was captured inside the workflow before it closed
workflow_span_context = vote_extraction_service.get_workflow_span_context()
span_context = None
if workflow_span_context and "span_id" in workflow_span_context and "trace_id" in workflow_span_context:
    span_context = SpanContext(
        span_id=str(workflow_span_context["span_id"]),
        trace_id=str(workflow_span_context["trace_id"])
    )
    logger.info(
        f"‚úÖ Retrieved workflow span context: "
        f"span_id={span_context.span_id}, trace_id={span_context.trace_id}"
    )
else:
    logger.warning("‚ö†Ô∏è No workflow span context available for feedback submission")
```

---

## üß™ Testing the Fix

### 1. Check Backend Logs

After extraction, you should see:

```json
{
  "message": "üìä Captured workflow span context: span_id=2670203640094394356, trace_id=6959432000000000d1f51f98d49b97c8"
}

{
  "message": "‚úÖ Retrieved workflow span context: span_id=2670203640094394356, trace_id=6959432000000000d1f51f98d49b97c8"
}
```

### 2. Verify Span ID is Different

**Before Fix** (HTTP span):
```
span_id: 11233447522943489327
```

**After Fix** (workflow span):
```
span_id: 2670203640094394356
```

### 3. Check Datadog

Wait 1-2 minutes for Datadog to process, then:

1. Go to: https://app.datadoghq.com/llm/evaluations
2. Filter by:
   - **ML App**: `vote-extractor`
   - **Time Range**: Last 1 hour
3. Search for evaluations:
   - `label:user_rating`
   - `label:user_thumbs`
   - `label:user_comment`

---

## üìä Expected Results

### Before Fix
- ‚ùå Validation evaluations: **Visible** (submitted inside workflow)
- ‚ùå User feedback evaluations: **NOT visible** (wrong span)

### After Fix
- ‚úÖ Validation evaluations: **Visible** (submitted inside workflow)
- ‚úÖ User feedback evaluations: **Visible** (correct workflow span!)

---

## üîç Troubleshooting

### If evaluations still don't appear:

1. **Verify correct span ID**:
   ```bash
   docker logs genai-fastapi-backend --tail 100 | grep "Captured workflow span context"
   ```

2. **Check evaluation submission**:
   ```bash
   docker logs genai-fastapi-backend --tail 100 | grep "Submitted feedback"
   ```

3. **Verify Datadog writer**:
   ```bash
   docker logs genai-fastapi-backend --tail 100 | grep "evaluation_metric events"
   ```

4. **Wait longer**: Datadog LLMObs has a 1-3 minute ingestion delay

5. **Check environment variables**:
   ```bash
   docker exec genai-fastapi-backend env | grep -E "(DD_LLMOBS|DD_API_KEY)"
   ```

---

## üí° Key Learnings

### 1. LLMObs Evaluations Must Attach to LLMObs Spans

- ‚úÖ **Workflow spans** (decorated with `@workflow`)
- ‚úÖ **LLM spans** (auto-created by LLMObs integrations)
- ‚ùå **HTTP request spans** (regular APM spans)
- ‚ùå **Generic spans** (not LLMObs-specific)

### 2. Timing is Critical

- ‚úÖ Capture span context **inside the workflow** (while active)
- ‚ùå Capture span context **after workflow returns** (span closed)

### 3. Use LLMObs.export_span() for LLMObs Spans

- ‚úÖ `LLMObs.export_span(span=None)` - gets active LLMObs span
- ‚ùå `tracer.current_span()` - gets current APM span (might not be LLMObs)

### 4. Trace ID Format from LLMObs

- `LLMObs.export_span()` returns trace_id in a **mixed format**: `6959432000000000d1f51f98d49b97c8`
- First part (16 hex chars): high 64 bits of 128-bit trace ID
- Second part (16 hex chars): low 64 bits of 128-bit trace ID
- **Don't convert to int** - use as-is for Datadog API

---

## üìö Related Documentation

- [Datadog LLMObs Custom Evaluations](../features/VALIDATION_CUSTOM_EVALUATIONS.md)
- [User Feedback Implementation](../features/USER_FEEDBACK_LLMOBS_IMPLEMENTATION_SUMMARY.md)
- [Evaluation Metric Types Guide](../../guides/llmobs/03_EVALUATION_METRIC_TYPES.md)
- [LLMObs Export Span Refactoring](../features/LLMOBS_EXPORT_SPAN_REFACTORING.md)

---

## ‚úÖ Summary

- **Problem**: User feedback sent to HTTP span instead of workflow span
- **Fix**: Capture workflow span context inside the workflow before it closes
- **Result**: User feedback evaluations now appear in Datadog LLMObs ‚úÖ
- **Date Fixed**: January 3, 2026

**Status**: ‚úÖ **PRODUCTION READY**

---

**Next Step**: Test in Streamlit frontend to verify user feedback evaluations now appear! üöÄ

