{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Prepare Vote Extraction Dataset for Datadog LLM Experiments\n",
        "\n",
        "**Objective**: Create a curated dataset from Thai election form images for systematic testing\n",
        "\n",
        "**What You'll Learn**:\n",
        "1. Load and inspect test images from `assets/`\n",
        "2. Create dataset records with input and expected output (ground truth)\n",
        "3. Save dataset as local JSON for version control\n",
        "4. Push dataset to Datadog via API\n",
        "5. Validate dataset quality\n",
        "\n",
        "**Prerequisites**:\n",
        "- Images in `assets/ss5-18-images/`\n",
        "- Datadog API and App keys\n",
        "- Python packages: `requests`, `PIL`, `json`\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üì¶ Setup and Imports\n",
        "\n",
        "**Install Required Packages** (run once):\n",
        "\n",
        "*If you have `fastapi-backend` installed and see a dependency conflict, uncomment and run the cell below first:*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n",
            "‚úÖ Optional: fastapi-backend upgraded (if uncommented)\n",
            "   This ensures ddtrace>=4.0.0 compatibility\n"
          ]
        }
      ],
      "source": [
        "# Optional: Upgrade fastapi-backend to resolve dependency conflicts\n",
        "# Uncomment the line below if you see ddtrace version conflicts\n",
        "%pip install --quiet --upgrade -e ../../services/fastapi-backend/\n",
        "\n",
        "print(\"‚úÖ Optional: fastapi-backend upgraded (if uncommented)\")\n",
        "print(\"   This ensures ddtrace>=4.0.0 compatibility\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n",
            "‚úÖ All required packages installed!\n",
            "\n",
            "Installed packages:\n",
            "  - requests: HTTP requests for Datadog API\n",
            "  - pillow: Image processing\n",
            "  - python-dotenv: Environment variables\n",
            "  - ddtrace: Datadog LLM Observability SDK (>=3.18.0)\n",
            "  - httpx: Async HTTP client for API calls\n",
            "  - google-genai: Google Generative AI SDK (>=1.56.0)\n",
            "\n",
            "üí° Tip: If you see any dependency warnings, they're typically safe to ignore\n",
            "   for this notebook as it doesn't directly use all backend dependencies.\n"
          ]
        }
      ],
      "source": [
        "# Install required packages for dataset preparation and experiments\n",
        "%pip install --quiet --upgrade requests pillow python-dotenv \"ddtrace>=3.18.0\" httpx \"google-genai>=1.56.0\"\n",
        "\n",
        "print(\"‚úÖ All required packages installed!\")\n",
        "print(\"\\nInstalled packages:\")\n",
        "print(\"  - requests: HTTP requests for Datadog API\")\n",
        "print(\"  - pillow: Image processing\")\n",
        "print(\"  - python-dotenv: Environment variables\")\n",
        "print(\"  - ddtrace: Datadog LLM Observability SDK (>=3.18.0)\")\n",
        "print(\"  - httpx: Async HTTP client for API calls\")\n",
        "print(\"  - google-genai: Google Generative AI SDK (>=1.56.0)\")\n",
        "print(\"\\nüí° Tip: If you see any dependency warnings, they're typically safe to ignore\")\n",
        "print(\"   for this notebook as it doesn't directly use all backend dependencies.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Project root: /Users/nuttee.jirattivongvibul/Projects/genai-app-python\n",
            "‚úÖ Current working directory: /Users/nuttee.jirattivongvibul/Projects/genai-app-python/notebooks/datasets\n",
            "‚úÖ Environment loaded from .env\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from typing import Dict, List, Any\n",
        "\n",
        "import requests\n",
        "from PIL import Image\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Add project root to path\n",
        "project_root = Path.cwd().parent.parent\n",
        "sys.path.insert(0, str(project_root))\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv(project_root / \".env\")\n",
        "\n",
        "print(f\"‚úÖ Project root: {project_root}\")\n",
        "print(f\"‚úÖ Current working directory: {Path.cwd()}\")\n",
        "print(f\"‚úÖ Environment loaded from .env\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîë Configuration & API Keys\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîë API Keys:\n",
            "   DD_API_KEY: ‚úÖ Set\n",
            "   DD_APP_KEY: ‚úÖ Set\n",
            "\n",
            "üìÇ Paths:\n",
            "   Images: /Users/nuttee.jirattivongvibul/Projects/genai-app-python/assets/ss5-18-images\n",
            "   Dataset output: /Users/nuttee.jirattivongvibul/Projects/genai-app-python/datasets/vote-extraction\n",
            "   Images exist: ‚úÖ Yes\n"
          ]
        }
      ],
      "source": [
        "# Configuration\n",
        "DD_API_KEY = os.getenv(\"DD_API_KEY\")\n",
        "DD_APP_KEY = os.getenv(\"DD_APP_KEY\")\n",
        "DD_SITE = os.getenv(\"DD_SITE\", \"datadoghq.com\")\n",
        "\n",
        "# Paths\n",
        "IMAGES_DIR = project_root / \"assets\" / \"ss5-18-images\"\n",
        "DATASET_DIR = project_root / \"datasets\" / \"vote-extraction\"\n",
        "DATASET_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Verify setup\n",
        "print(\"üîë API Keys:\")\n",
        "print(f\"   DD_API_KEY: {'‚úÖ Set' if DD_API_KEY else '‚ùå Missing'}\")\n",
        "print(f\"   DD_APP_KEY: {'‚úÖ Set' if DD_APP_KEY else '‚ùå Missing'}\")\n",
        "print(f\"\\nüìÇ Paths:\")\n",
        "print(f\"   Images: {IMAGES_DIR}\")\n",
        "print(f\"   Dataset output: {DATASET_DIR}\")\n",
        "print(f\"   Images exist: {'‚úÖ Yes' if IMAGES_DIR.exists() else '‚ùå No'}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üì∏ Step 1: Discover Images\n",
        "\n",
        "Let's see what images we have available.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Discover all images\n",
        "image_files = sorted(list(IMAGES_DIR.glob(\"*.jpg\")) + list(IMAGES_DIR.glob(\"*.png\")))\n",
        "\n",
        "print(f\"üìä Found {len(image_files)} images:\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for i, img_path in enumerate(image_files[:10], 1):  # Show first 10\n",
        "    img = Image.open(img_path)\n",
        "    size_mb = img_path.stat().st_size / (1024 * 1024)\n",
        "    print(f\"{i:2}. {img_path.name:45} {img.size[0]:4}x{img.size[1]:4}px {size_mb:6.2f}MB\")\n",
        "\n",
        "if len(image_files) > 10:\n",
        "    print(f\"... and {len(image_files) - 10} more\")\n",
        "\n",
        "print(f\"\\n‚úÖ Total: {len(image_files)} images\")\n",
        "print(f\"   Estimated form sets (6 pages each): {len(image_files) // 6}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üíæ Step 2: Work with Local Dataset Files\n",
        "\n",
        "We store datasets as JSON files for:\n",
        "- Version control (Git-friendly)\n",
        "- Local editing and review\n",
        "- Backup and sharing\n",
        "- Incremental updates\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# List existing dataset files\n",
        "dataset_files = sorted(DATASET_DIR.glob(\"*.json\"))\n",
        "\n",
        "print(f\"üìÅ Existing dataset files in {DATASET_DIR}:\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "if dataset_files:\n",
        "    for i, file in enumerate(dataset_files, 1):\n",
        "        size_kb = file.stat().st_size / 1024\n",
        "        modified = datetime.fromtimestamp(file.stat().st_mtime)\n",
        "        print(f\"{i}. {file.name}\")\n",
        "        print(f\"   Size: {size_kb:.2f} KB | Modified: {modified.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "else:\n",
        "    print(\"No dataset files found yet.\")\n",
        "    print(\"\\nüí° Use the Streamlit app to create your first dataset!\")\n",
        "    print(\"   Run: streamlit run frontend/streamlit/pages/2_üìä_Dataset_Manager.py\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üì§ Step 3: Push Dataset to Datadog LLMObs\n",
        "\n",
        "Use Datadog LLMObs SDK to create projects and datasets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the latest dataset file\n",
        "latest_file = DATASET_DIR / \"vote-extraction-dataset_latest.json\"\n",
        "\n",
        "if not latest_file.exists() and dataset_files:\n",
        "    latest_file = dataset_files[-1]  # Use most recent file\n",
        "\n",
        "if latest_file.exists():\n",
        "    print(f\"üìÇ Loading dataset from: {latest_file.name}\")\n",
        "    \n",
        "    with open(latest_file, 'r', encoding='utf-8') as f:\n",
        "        dataset = json.load(f)\n",
        "    \n",
        "    print(f\"\\n‚úÖ Dataset loaded:\")\n",
        "    print(f\"   Name: {dataset['metadata']['name']}\")\n",
        "    print(f\"   Version: {dataset['metadata']['version']}\")\n",
        "    print(f\"   Records: {dataset['metadata']['num_records']}\")\n",
        "    print(f\"   Total Pages: {dataset['metadata']['total_pages']}\")\n",
        "    print(f\"   Created: {dataset['metadata']['created_at']}\")\n",
        "    \n",
        "    # Show first record as example\n",
        "    if dataset['records']:\n",
        "        print(f\"\\nüìÑ Example record:\")\n",
        "        print(json.dumps(dataset['records'][0], indent=2, ensure_ascii=False)[:500] + \"...\")\n",
        "else:\n",
        "    print(\"‚ùå No dataset files found.\")\n",
        "    print(\"\\nüí° Create one using the Streamlit Dataset Manager app!\")\n",
        "    dataset = None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üöÄ Push to Datadog (Using SDK)\n",
        "\n",
        "**Note**: Uncomment and run this cell when you're ready to push your dataset to Datadog.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Uncomment to push dataset to Datadog\n",
        "'''\n",
        "if dataset and DD_API_KEY and DD_APP_KEY:\n",
        "    print(\"üîó Pushing dataset to Datadog...\")\n",
        "    \n",
        "    # Using HTTP API\n",
        "    base_url = f\"https://api.{DD_SITE}/api/v2/llm-obs/v1\"\n",
        "    headers = {\n",
        "        \"DD-API-KEY\": DD_API_KEY,\n",
        "        \"DD-APPLICATION-KEY\": DD_APP_KEY,\n",
        "        \"Content-Type\": \"application/json\",\n",
        "    }\n",
        "    \n",
        "    # 1. Create or get project\n",
        "    project_name = \"vote-extraction-project\"\n",
        "    response = requests.get(f\"{base_url}/projects\", headers=headers)\n",
        "    projects = response.json().get(\"data\", [])\n",
        "    \n",
        "    project_id = None\n",
        "    for proj in projects:\n",
        "        if proj[\"attributes\"][\"name\"] == project_name:\n",
        "            project_id = proj[\"id\"]\n",
        "            print(f\"‚úÖ Found existing project: {project_id}\")\n",
        "            break\n",
        "    \n",
        "    if not project_id:\n",
        "        # Create new project\n",
        "        payload = {\n",
        "            \"data\": {\n",
        "                \"type\": \"project\",\n",
        "                \"attributes\": {\n",
        "                    \"name\": project_name,\n",
        "                    \"description\": \"Thai election vote extraction testing\"\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "        response = requests.post(f\"{base_url}/projects\", json=payload, headers=headers)\n",
        "        project_id = response.json()[\"data\"][\"id\"]\n",
        "        print(f\"‚úÖ Created new project: {project_id}\")\n",
        "    \n",
        "    # 2. Create dataset\n",
        "    payload = {\n",
        "        \"data\": {\n",
        "            \"type\": \"dataset\",\n",
        "            \"attributes\": {\n",
        "                \"project_id\": project_id,\n",
        "                \"name\": dataset[\"metadata\"][\"name\"],\n",
        "                \"description\": dataset[\"metadata\"].get(\"description\", \"\"),\n",
        "                \"dataset_version\": 1\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "    response = requests.post(f\"{base_url}/datasets\", json=payload, headers=headers)\n",
        "    dataset_id = response.json()[\"data\"][\"id\"]\n",
        "    print(f\"‚úÖ Created dataset: {dataset_id}\")\n",
        "    \n",
        "    # 3. Add records\n",
        "    print(f\"\\nüì§ Adding {len(dataset['records'])} records...\")\n",
        "    for i, record in enumerate(dataset['records'], 1):\n",
        "        payload = {\n",
        "            \"data\": {\n",
        "                \"type\": \"dataset_record\",\n",
        "                \"attributes\": {\n",
        "                    \"input\": record[\"input\"],\n",
        "                    \"expected_output\": record[\"expected_output\"]\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "        response = requests.post(f\"{base_url}/datasets/{dataset_id}/records\", json=payload, headers=headers)\n",
        "        print(f\"‚úÖ Added record {i}/{len(dataset['records'])}: {record['id']}\")\n",
        "    \n",
        "    print(f\"\\nüéâ Dataset pushed successfully!\")\n",
        "    print(f\"üîó View in Datadog: https://app.{DD_SITE}/llm/experiments\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Skipped: No dataset or API keys not set\")\n",
        "'''\n",
        "\n",
        "print(\"üí° TIP: Use the Streamlit Dataset Manager app to push datasets with a GUI!\")\n",
        "print(\"   It provides a much better experience for managing datasets.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üß™ Step 4: Run Experiments\n",
        "\n",
        "Experiments let you systematically test your LLM application by running your agent across a set of scenarios from your dataset and measuring performance against expected outputs.\n",
        "\n",
        "**Key Components**:\n",
        "1. **Task**: Core workflow to evaluate (single LLM call or complex flow)\n",
        "2. **Evaluators**: Functions that measure performance (boolean, score, categorical)\n",
        "3. **Summary Evaluators**: Aggregate metrics across all records (precision, recall, accuracy)\n",
        "\n",
        "**Benefits**:\n",
        "- Compare different app configurations side-by-side\n",
        "- Track performance improvements over time\n",
        "- Identify failure patterns\n",
        "- Validate before production deployment\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.1 Load Dataset from Datadog\n",
        "\n",
        "First, load the dataset we created earlier from Datadog.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üì• Loading dataset 'vote-extraction-bangbamru-1-10' from Datadog...\n",
            "‚úÖ Dataset loaded successfully!\n",
            "   Records: 11\n",
            "   Version: 2\n",
            "\n",
            "üìÑ First record preview:\n",
            "   Input keys: ['form_set_name', 'image_paths', 'num_pages']\n",
            "   Expected output keys: ['ballot_statistics', 'form_info', 'notes', 'vote_results', 'voter_statistics']\n"
          ]
        }
      ],
      "source": [
        "from ddtrace.llmobs import LLMObs\n",
        "from typing import Dict, Any, Optional, List, Callable\n",
        "\n",
        "# Load dataset from Datadog\n",
        "dataset_name = \"vote-extraction-bangbamru-1-10\"  # Change to your dataset name\n",
        "project_name = \"vote-extraction-project\"\n",
        "\n",
        "# Initialize LLMObs\n",
        "LLMObs.enable(\n",
        "    ml_app=\"vote-extractor\",\n",
        "    api_key=DD_API_KEY,\n",
        "    site=DD_SITE,\n",
        "    agentless_enabled=True,\n",
        "    project_name=project_name,\n",
        ")\n",
        "\n",
        "print(f\"üì• Loading dataset '{dataset_name}' from Datadog...\")\n",
        "\n",
        "try:\n",
        "    experiment_dataset = LLMObs.pull_dataset(\n",
        "        dataset_name=dataset_name,\n",
        "        project_name=project_name,\n",
        "        version=2  # Optional: specify version, defaults to latest\n",
        "    )\n",
        "    \n",
        "    print(f\"‚úÖ Dataset loaded successfully!\")\n",
        "    print(f\"   Records: {len(experiment_dataset)}\")\n",
        "    \n",
        "    # Try to get version (may not be available in all ddtrace versions)\n",
        "    version = getattr(experiment_dataset, 'version', None)\n",
        "    if version is not None:\n",
        "        print(f\"   Version: {version}\")\n",
        "    \n",
        "    # Preview first record\n",
        "    if len(experiment_dataset) > 0:\n",
        "        first_record = experiment_dataset[0]\n",
        "        print(f\"\\nüìÑ First record preview:\")\n",
        "        print(f\"   Input keys: {list(first_record['input_data'].keys())}\")\n",
        "        print(f\"   Expected output keys: {list(first_record['expected_output'].keys())}\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error loading dataset: {e}\")\n",
        "    print(\"\\nüí° Make sure you've pushed the dataset to Datadog first!\")\n",
        "    print(\"   Use the Streamlit Dataset Manager or Step 3 above\")\n",
        "    experiment_dataset = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä Dataset Object Inspection:\n",
            "================================================================================\n",
            "Type: <class 'ddtrace.llmobs._experiment.Dataset'>\n",
            "\n",
            "üìù Available attributes:\n",
            "   - BATCH_UPDATE_THRESHOLD: 5242880\n",
            "   - description: Auto-generated from LLM extraction on 2026-01-04 02:20:54\n",
            "   - latest_version: 3\n",
            "   - name: vote-extraction-bangbamru-1-10\n",
            "   - project: {'name': 'vote-extraction-project', '_id': 'b8001020-6c33-4e94-a476-bcd78efddf3b'}\n",
            "   - url: https://app.datadoghq.com/llm/datasets/241bfded-e79d-4d2d-bbc4-a74bb06d85f9\n",
            "   - version: 2\n",
            "\n",
            "üì¶ Dataset Structure:\n",
            "   - Length: 11\n",
            "   - First record type: <class 'dict'>\n",
            "   - First record keys: ['record_id', 'input_data', 'expected_output', 'metadata']\n",
            "\n",
            "üí° Note: The Dataset object is a wrapper around a list of records.\n",
            "   Version info may be stored internally or not exposed as an attribute.\n"
          ]
        }
      ],
      "source": [
        "# üîç Inspect Dataset Object (Debug)\n",
        "if experiment_dataset:\n",
        "    print(\"üìä Dataset Object Inspection:\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    # Show type\n",
        "    print(f\"Type: {type(experiment_dataset)}\")\n",
        "    \n",
        "    # Show available attributes\n",
        "    print(f\"\\nüìù Available attributes:\")\n",
        "    attrs = [attr for attr in dir(experiment_dataset) if not attr.startswith('_')]\n",
        "    for attr in attrs[:15]:  # Show first 15\n",
        "        try:\n",
        "            value = getattr(experiment_dataset, attr, None)\n",
        "            if not callable(value):\n",
        "                print(f\"   - {attr}: {value}\")\n",
        "        except:\n",
        "            pass\n",
        "    \n",
        "    # Show structure\n",
        "    print(f\"\\nüì¶ Dataset Structure:\")\n",
        "    print(f\"   - Length: {len(experiment_dataset)}\")\n",
        "    if len(experiment_dataset) > 0:\n",
        "        print(f\"   - First record type: {type(experiment_dataset[0])}\")\n",
        "        print(f\"   - First record keys: {list(experiment_dataset[0].keys())}\")\n",
        "    \n",
        "    print(\"\\nüí° Note: The Dataset object is a wrapper around a list of records.\")\n",
        "    print(\"   Version info may be stored internally or not exposed as an attribute.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2 Define Task Function\n",
        "\n",
        "The task function processes each dataset record. For vote extraction, we'll call our FastAPI backend.\n",
        "\n",
        "**‚ö†Ô∏è Important: API Key Configuration**\n",
        "\n",
        "The FastAPI backend requires authentication if `API_KEY` is set in your environment:\n",
        "\n",
        "- **If you get 401 Unauthorized errors**: Set `API_KEY` in your `.env` file or environment\n",
        "- **For local testing without API key**: Ensure `API_KEY=\"\"` (empty) and `API_KEY_REQUIRED=false` in `.env`\n",
        "- **The notebook will automatically load `API_KEY` from environment** and include it in requests\n",
        "\n",
        "Example `.env` configuration:\n",
        "```bash\n",
        "# Option 1: Use API key (recommended for production-like testing)\n",
        "API_KEY=your-secret-key-here\n",
        "API_KEY_REQUIRED=true\n",
        "\n",
        "# Option 2: Disable API key (for quick local testing)\n",
        "API_KEY=\n",
        "API_KEY_REQUIRED=false\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Task function defined: vote_extraction_task()\n"
          ]
        }
      ],
      "source": [
        "import httpx\n",
        "from ddtrace.llmobs.decorators import workflow\n",
        "\n",
        "# FastAPI backend URL and API key\n",
        "API_BASE_URL = os.getenv(\"API_BASE_URL\", \"http://localhost:8000\")\n",
        "API_KEY = os.getenv(\"API_KEY\", \"\")  # Load API key from environment\n",
        "\n",
        "@workflow\n",
        "def vote_extraction_task(input_data: Dict[str, Any], config: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Task function that extracts vote data from election form images.\n",
        "    \n",
        "    Args:\n",
        "        input_data: Dictionary containing 'form_set_name' and 'image_paths'\n",
        "        config: Optional configuration (model settings, etc.)\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary with extracted vote data (form_info, ballot_statistics, vote_results)\n",
        "    \"\"\"\n",
        "    form_set_name = input_data.get(\"form_set_name\")\n",
        "    image_paths = input_data.get(\"image_paths\", [])\n",
        "    \n",
        "    print(f\"Processing: {form_set_name} ({len(image_paths)} pages)\")\n",
        "    \n",
        "    try:\n",
        "        # Read images\n",
        "        files = []\n",
        "        for img_path in image_paths:\n",
        "            img_file = Path(img_path)\n",
        "            if img_file.exists():\n",
        "                files.append((\"files\", (img_file.name, img_file.read_bytes(), \"image/jpeg\")))\n",
        "        \n",
        "        # Prepare headers with API key (if configured)\n",
        "        headers = {}\n",
        "        if API_KEY:\n",
        "            headers[\"X-API-Key\"] = API_KEY\n",
        "        \n",
        "        # Call extraction API\n",
        "        with httpx.Client(timeout=300.0) as client:\n",
        "            response = client.post(\n",
        "                f\"{API_BASE_URL}/api/v1/vote-extraction/extract\",\n",
        "                files=files,\n",
        "                headers=headers\n",
        "            )\n",
        "            response.raise_for_status()\n",
        "            result = response.json()\n",
        "        \n",
        "        # Extract first form data (for single-form datasets)\n",
        "        if result.get(\"data\") and len(result[\"data\"]) > 0:\n",
        "            extracted_data = result[\"data\"][0]\n",
        "            return {\n",
        "                \"form_info\": extracted_data.get(\"form_info\"),\n",
        "                \"voter_statistics\": extracted_data.get(\"voter_statistics\"),\n",
        "                \"ballot_statistics\": extracted_data.get(\"ballot_statistics\"),\n",
        "                \"vote_results\": extracted_data.get(\"vote_results\", [])\n",
        "            }\n",
        "        else:\n",
        "            return {\"error\": \"No data extracted\"}\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error processing {form_set_name}: {e}\")\n",
        "        return {\"error\": str(e)}\n",
        "\n",
        "print(\"‚úÖ Task function defined: vote_extraction_task()\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.3 Define Evaluator Functions\n",
        "\n",
        "Evaluators measure how well the model performs. We'll create evaluators for different aspects of vote extraction.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.3.1 LLM-as-Judge Evaluator (Advanced)\n",
        "\n",
        "This evaluator uses a more powerful LLM (gemini-3-pro-preview via Vertex AI) to assess extraction quality.\n",
        "It provides detailed reasoning and identifies specific errors.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ LLM-as-Judge evaluator defined:\n",
            "   - llm_judge_evaluator (score 0.0-1.0)\n",
            "   - Uses: gemini-3-pro-preview via Vertex AI\n",
            "   - Provides: Quality score + detailed reasoning\n"
          ]
        }
      ],
      "source": [
        "def llm_judge_evaluator(input_data: Dict[str, Any], output_data: Dict[str, Any], expected_output: Dict[str, Any]) -> float:\n",
        "    \"\"\"\n",
        "    LLM-as-Judge evaluator using gemini-3-pro-preview via Vertex AI.\n",
        "    \n",
        "    Uses a more powerful LLM to evaluate the quality of extraction outputs\n",
        "    by comparing them with ground truth. Provides a quality score from 0.0 to 1.0.\n",
        "    Includes retry logic with exponential backoff for empty responses.\n",
        "    \n",
        "    Note: Requires GOOGLE_CLOUD_PROJECT and VERTEX_AI_LOCATION environment variables.\n",
        "    \"\"\"\n",
        "    import json\n",
        "    import time\n",
        "    from google import genai\n",
        "    from google.genai import types\n",
        "    from ddtrace import tracer\n",
        "    \n",
        "    # Retry configuration\n",
        "    MAX_RETRIES = 3\n",
        "    INITIAL_RETRY_DELAY = 1.0  # seconds\n",
        "    \n",
        "    # Define response schema for structured output\n",
        "    EVALUATION_SCHEMA = {\n",
        "        \"type\": \"OBJECT\",\n",
        "        \"properties\": {\n",
        "            \"score\": {\n",
        "                \"type\": \"NUMBER\",\n",
        "                \"description\": \"Quality score between 0.0 (worst) and 1.0 (perfect)\"\n",
        "            },\n",
        "            \"reasoning\": {\n",
        "                \"type\": \"STRING\",\n",
        "                \"description\": \"Brief explanation of the score\"\n",
        "            },\n",
        "            \"errors\": {\n",
        "                \"type\": \"ARRAY\",\n",
        "                \"description\": \"List of specific errors found\",\n",
        "                \"items\": {\n",
        "                    \"type\": \"OBJECT\",\n",
        "                    \"properties\": {\n",
        "                        \"field\": {\"type\": \"STRING\", \"description\": \"Field path with error\"},\n",
        "                        \"expected\": {\"type\": \"STRING\", \"description\": \"Expected value\"},\n",
        "                        \"actual\": {\"type\": \"STRING\", \"description\": \"Actual value\"},\n",
        "                        \"severity\": {\"type\": \"STRING\", \"enum\": [\"minor\", \"major\", \"critical\"]}\n",
        "                    }\n",
        "                }\n",
        "            }\n",
        "        },\n",
        "        \"required\": [\"score\", \"reasoning\", \"errors\"]\n",
        "    }\n",
        "    \n",
        "    # Create main evaluation span\n",
        "    form_set_name = input_data.get(\"form_set_name\", \"Unknown\")\n",
        "    \n",
        "    with tracer.trace(\n",
        "        \"llm_judge.evaluate\",\n",
        "        service=\"vote-extractor\",\n",
        "        resource=f\"evaluate_{form_set_name}\"\n",
        "    ) as eval_span:\n",
        "        eval_span.set_tag(\"form_set_name\", form_set_name)\n",
        "        eval_span.set_tag(\"evaluator\", \"llm_judge\")\n",
        "        eval_span.set_tag(\"model\", \"gemini-3-pro-preview\")\n",
        "        \n",
        "        try:\n",
        "            # Get GCP configuration\n",
        "            project_id = os.getenv(\"GOOGLE_CLOUD_PROJECT\")\n",
        "            location = os.getenv(\"VERTEX_AI_LOCATION\", \"global\")\n",
        "            \n",
        "            if not project_id:\n",
        "                print(\"‚ö†Ô∏è  LLM Judge: GOOGLE_CLOUD_PROJECT not set, skipping evaluation\")\n",
        "                eval_span.set_tag(\"error.skip\", \"missing_gcp_project\")\n",
        "                return 0.0  # Cannot evaluate without project\n",
        "            \n",
        "            # Initialize Google GenAI client with Vertex AI\n",
        "            with tracer.trace(\"llm_judge.initialize_client\", service=\"vote-extractor\") as init_span:\n",
        "                init_span.set_tag(\"project_id\", project_id)\n",
        "                init_span.set_tag(\"location\", location)\n",
        "                \n",
        "                client = genai.Client(\n",
        "                    vertexai=True,\n",
        "                    project=project_id,\n",
        "                    location=location,\n",
        "                )\n",
        "            \n",
        "            # Build evaluation prompt with tracing\n",
        "            with tracer.trace(\"llm_judge.build_prompt\", service=\"vote-extractor\") as prompt_span:\n",
        "                prompt_span.set_tag(\"form_set_name\", form_set_name)\n",
        "                \n",
        "                prompt = f\"\"\"You are an expert evaluator for Thai election vote extraction systems.\n",
        "\n",
        "Compare the extracted output with the ground truth and evaluate the quality.\n",
        "\n",
        "**Input Form:** {form_set_name}\n",
        "\n",
        "**Ground Truth:**\n",
        "{json.dumps(expected_output, indent=2, ensure_ascii=False)}\n",
        "\n",
        "**Extracted Output:**\n",
        "{json.dumps(output_data, indent=2, ensure_ascii=False)}\n",
        "\n",
        "Evaluate the extraction quality considering:\n",
        "1. Form Information (date, location, polling station)\n",
        "2. Voter Statistics (eligible voters, voters present)\n",
        "3. Ballot Statistics (allocated, used, good, bad, no-vote)\n",
        "4. Vote Results (candidate numbers, names, vote counts)\n",
        "\n",
        "Provide:\n",
        "- score: float between 0.0 (worst) and 1.0 (perfect)\n",
        "- reasoning: brief explanation\n",
        "- errors: list of specific errors found (if any)\n",
        "\"\"\"\n",
        "                prompt_span.set_metric(\"prompt_length\", len(prompt))\n",
        "            \n",
        "            # Call Gemini 3 Pro Preview as judge with structured schema\n",
        "            # Retry logic for empty responses\n",
        "            response = None\n",
        "            retry_delay = INITIAL_RETRY_DELAY\n",
        "            \n",
        "            for attempt in range(1, MAX_RETRIES + 1):\n",
        "                with tracer.trace(\"llm_judge.api_call\", service=\"vote-extractor\") as api_span:\n",
        "                    api_span.set_tag(\"model\", \"gemini-3-pro-preview\")\n",
        "                    api_span.set_tag(\"provider\", \"google\")\n",
        "                    api_span.set_tag(\"temperature\", \"0.0\")\n",
        "                    api_span.set_metric(\"attempt\", attempt)\n",
        "                    api_span.set_metric(\"max_retries\", MAX_RETRIES)\n",
        "                    \n",
        "                    try:\n",
        "                        response = client.models.generate_content(\n",
        "                            model=\"gemini-3-pro-preview\",\n",
        "                            contents=prompt,\n",
        "                            config=types.GenerateContentConfig(\n",
        "                                response_mime_type=\"application/json\",\n",
        "                                response_schema=EVALUATION_SCHEMA,  # ‚úÖ Enforce structured output\n",
        "                                temperature=0.0,  # Deterministic evaluation\n",
        "                                max_output_tokens=4096,\n",
        "                            ),\n",
        "                        )\n",
        "                        \n",
        "                        api_span.set_tag(\"response_received\", response is not None)\n",
        "                        \n",
        "                        # Debug: Inspect response structure\n",
        "                        if response:\n",
        "                            finish_reason = getattr(response, 'finish_reason', 'N/A')\n",
        "                            api_span.set_tag(\"finish_reason\", str(finish_reason))\n",
        "                            candidates = getattr(response, 'candidates', [])\n",
        "                            api_span.set_metric(\"candidates_count\", len(candidates) if candidates else 0)\n",
        "                            \n",
        "                            print(f\"üîç Response Debug - {form_set_name} (attempt {attempt}):\")\n",
        "                            print(f\"   - Has text: {bool(response.text) if hasattr(response, 'text') else False}\")\n",
        "                            print(f\"   - Text length: {len(response.text) if hasattr(response, 'text') and response.text else 0}\")\n",
        "                            print(f\"   - Finish reason: {finish_reason}\")\n",
        "                            print(f\"   - Candidates: {len(candidates) if candidates else 0}\")\n",
        "                        \n",
        "                        # Check if response has content\n",
        "                        if response and response.text:\n",
        "                            api_span.set_tag(\"response_valid\", True)\n",
        "                            print(f\"‚úÖ LLM Judge: Valid response for {form_set_name} (attempt {attempt})\")\n",
        "                            break  # Success! Exit retry loop\n",
        "                        else:\n",
        "                            api_span.set_tag(\"response_valid\", False)\n",
        "                            api_span.set_tag(\"retry_reason\", \"empty_response\")\n",
        "                            \n",
        "                            # Detailed error info for debugging\n",
        "                            if response:\n",
        "                                print(f\"‚ö†Ô∏è  Response object exists but no text:\")\n",
        "                                print(f\"   - finish_reason: {getattr(response, 'finish_reason', 'N/A')}\")\n",
        "                                print(f\"   - candidates: {len(getattr(response, 'candidates', []))}\")\n",
        "                                if hasattr(response, 'prompt_feedback'):\n",
        "                                    print(f\"   - prompt_feedback: {response.prompt_feedback}\")\n",
        "                                print(f\"   - text attr: {hasattr(response, 'text')}\")\n",
        "                                print(f\"   - text value: {repr(response.text) if hasattr(response, 'text') else 'N/A'}\")\n",
        "                            \n",
        "                            if attempt < MAX_RETRIES:\n",
        "                                print(f\"‚ö†Ô∏è  LLM Judge: Empty response for {form_set_name} (attempt {attempt}/{MAX_RETRIES}), retrying in {retry_delay:.1f}s...\")\n",
        "                                time.sleep(retry_delay)\n",
        "                                retry_delay *= 2  # Exponential backoff\n",
        "                            else:\n",
        "                                print(f\"‚ùå LLM Judge: Empty response for {form_set_name} after {MAX_RETRIES} attempts\")\n",
        "                    \n",
        "                    except Exception as api_error:\n",
        "                        api_span.set_tag(\"api_error\", str(api_error))\n",
        "                        \n",
        "                        if attempt < MAX_RETRIES:\n",
        "                            print(f\"‚ö†Ô∏è  LLM Judge: API error for {form_set_name} (attempt {attempt}/{MAX_RETRIES}): {api_error}, retrying in {retry_delay:.1f}s...\")\n",
        "                            time.sleep(retry_delay)\n",
        "                            retry_delay *= 2\n",
        "                        else:\n",
        "                            print(f\"‚ùå LLM Judge: API error for {form_set_name} after {MAX_RETRIES} attempts: {api_error}\")\n",
        "                            raise\n",
        "            \n",
        "            # Parse and validate response\n",
        "            with tracer.trace(\"llm_judge.parse_response\", service=\"vote-extractor\") as parse_span:\n",
        "                if not response or not response.text:\n",
        "                    print(f\"‚ö†Ô∏è  LLM Judge: Empty response for {form_set_name}\")\n",
        "                    parse_span.set_tag(\"error\", \"empty_response\")\n",
        "                    eval_span.set_metric(\"score\", 0.0)\n",
        "                    return 0.0\n",
        "                \n",
        "                parse_span.set_metric(\"response_length\", len(response.text))\n",
        "                \n",
        "                evaluation = json.loads(response.text)\n",
        "                score = float(evaluation.get(\"score\", 0.0))\n",
        "                \n",
        "                parse_span.set_metric(\"score\", score)\n",
        "                eval_span.set_metric(\"final_score\", score)\n",
        "                eval_span.set_tag(\"result\", \"success\")\n",
        "            \n",
        "            # Optional: Print reasoning for debugging\n",
        "            # print(f\"LLM Judge: {form_set_name} - Score: {score:.2f}\")\n",
        "            # print(f\"Reasoning: {evaluation.get('reasoning', 'N/A')}\")\n",
        "            \n",
        "            return score\n",
        "            \n",
        "        except json.JSONDecodeError as e:\n",
        "            print(f\"‚ö†Ô∏è  LLM Judge JSON Parse Error for {form_set_name}: {e}\")\n",
        "            print(f\"    Raw response: {response.text if 'response' in locals() else 'None'}\")\n",
        "            eval_span.set_tag(\"error\", \"json_decode_error\")\n",
        "            eval_span.set_metric(\"score\", 0.0)\n",
        "            return 0.0\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è  LLM Judge Error for {form_set_name}: {e}\")\n",
        "            eval_span.set_tag(\"error\", type(e).__name__)\n",
        "            eval_span.set_metric(\"score\", 0.0)\n",
        "            return 0.0\n",
        "\n",
        "\n",
        "print(\"‚úÖ LLM-as-Judge evaluator defined:\")\n",
        "print(\"   - llm_judge_evaluator (score 0.0-1.0)\")\n",
        "print(\"   - Uses: gemini-3-pro-preview via Vertex AI\")\n",
        "print(\"   - Provides: Quality score + detailed reasoning\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Evaluators defined:\n",
            "   - exact_form_match (boolean)\n",
            "   - ballot_accuracy_score (score)\n",
            "   - vote_results_quality (categorical)\n",
            "   - has_no_errors (boolean)\n"
          ]
        }
      ],
      "source": [
        "def exact_form_match(input_data: Dict[str, Any], output_data: Dict[str, Any], expected_output: Dict[str, Any]) -> bool:\n",
        "    \"\"\"\n",
        "    Boolean evaluator: Check if form info matches exactly.\n",
        "    \"\"\"\n",
        "    if \"error\" in output_data or not output_data.get(\"form_info\"):\n",
        "        return False\n",
        "    \n",
        "    output_form = output_data.get(\"form_info\", {})\n",
        "    expected_form = expected_output.get(\"form_info\", {})\n",
        "    \n",
        "    # Compare key fields\n",
        "    return (\n",
        "        output_form.get(\"district\") == expected_form.get(\"district\") and\n",
        "        output_form.get(\"polling_station_number\") == expected_form.get(\"polling_station_number\")\n",
        "    )\n",
        "\n",
        "\n",
        "def ballot_accuracy_score(input_data: Dict[str, Any], output_data: Dict[str, Any], expected_output: Dict[str, Any]) -> float:\n",
        "    \"\"\"\n",
        "    Score evaluator: Calculate ballot statistics accuracy (0.0 to 1.0).\n",
        "    \"\"\"\n",
        "    if \"error\" in output_data or not output_data.get(\"ballot_statistics\"):\n",
        "        return 0.0\n",
        "    \n",
        "    output_ballots = output_data.get(\"ballot_statistics\", {})\n",
        "    expected_ballots = expected_output.get(\"ballot_statistics\", {})\n",
        "    \n",
        "    # Compare key ballot counts\n",
        "    fields = [\"ballots_used\", \"good_ballots\", \"bad_ballots\", \"no_vote_ballots\"]\n",
        "    matches = 0\n",
        "    total = 0\n",
        "    \n",
        "    for field in fields:\n",
        "        if field in expected_ballots:\n",
        "            total += 1\n",
        "            if output_ballots.get(field) == expected_ballots.get(field):\n",
        "                matches += 1\n",
        "    \n",
        "    return matches / total if total > 0 else 0.0\n",
        "\n",
        "\n",
        "def vote_results_quality(input_data: Dict[str, Any], output_data: Dict[str, Any], expected_output: Dict[str, Any]) -> str:\n",
        "    \"\"\"\n",
        "    Categorical evaluator: Assess vote results quality (excellent/good/fair/poor).\n",
        "    \"\"\"\n",
        "    if \"error\" in output_data or not output_data.get(\"vote_results\"):\n",
        "        return \"poor\"\n",
        "    \n",
        "    output_votes = output_data.get(\"vote_results\", [])\n",
        "    expected_votes = expected_output.get(\"vote_results\", [])\n",
        "    \n",
        "    # Count matching candidates\n",
        "    if len(expected_votes) == 0:\n",
        "        return \"poor\"\n",
        "    \n",
        "    matches = 0\n",
        "    for exp_vote in expected_votes:\n",
        "        for out_vote in output_votes:\n",
        "            if (out_vote.get(\"number\") == exp_vote.get(\"number\") and\n",
        "                out_vote.get(\"vote_count\") == exp_vote.get(\"vote_count\")):\n",
        "                matches += 1\n",
        "                break\n",
        "    \n",
        "    accuracy = matches / len(expected_votes)\n",
        "    \n",
        "    if accuracy >= 0.95:\n",
        "        return \"excellent\"\n",
        "    elif accuracy >= 0.80:\n",
        "        return \"good\"\n",
        "    elif accuracy >= 0.60:\n",
        "        return \"fair\"\n",
        "    else:\n",
        "        return \"poor\"\n",
        "\n",
        "\n",
        "def has_no_errors(input_data: Dict[str, Any], output_data: Dict[str, Any], expected_output: Dict[str, Any]) -> bool:\n",
        "    \"\"\"\n",
        "    Boolean evaluator: Check if extraction completed without errors.\n",
        "    \"\"\"\n",
        "    return \"error\" not in output_data\n",
        "\n",
        "print(\"‚úÖ Evaluators defined:\")\n",
        "print(\"   - exact_form_match (boolean)\")\n",
        "print(\"   - ballot_accuracy_score (score)\")\n",
        "print(\"   - vote_results_quality (categorical)\")\n",
        "print(\"   - has_no_errors (boolean)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.4 Define Summary Evaluators (Optional)\n",
        "\n",
        "Summary evaluators aggregate results across all records.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Summary evaluators defined:\n",
            "   - overall_accuracy (float)\n",
            "   - success_rate (float)\n",
            "   - avg_ballot_accuracy (float)\n",
            "   - avg_llm_judge_score (float) ‚≠ê NEW!\n"
          ]
        }
      ],
      "source": [
        "def overall_accuracy(inputs: List[Any], outputs: List[Any], expected_outputs: List[Any], evaluators_results: Dict[str, List]) -> float:\n",
        "    \"\"\"\n",
        "    Summary evaluator: Calculate overall accuracy across all records.\n",
        "    \"\"\"\n",
        "    form_matches = evaluators_results.get(\"exact_form_match\", [])\n",
        "    if not form_matches:\n",
        "        return 0.0\n",
        "    \n",
        "    return form_matches.count(True) / len(form_matches)\n",
        "\n",
        "\n",
        "def success_rate(inputs: List[Any], outputs: List[Any], expected_outputs: List[Any], evaluators_results: Dict[str, List]) -> float:\n",
        "    \"\"\"\n",
        "    Summary evaluator: Calculate percentage of records processed without errors.\n",
        "    \"\"\"\n",
        "    no_errors = evaluators_results.get(\"has_no_errors\", [])\n",
        "    if not no_errors:\n",
        "        return 0.0\n",
        "    \n",
        "    return no_errors.count(True) / len(no_errors)\n",
        "\n",
        "\n",
        "def avg_ballot_accuracy(inputs: List[Any], outputs: List[Any], expected_outputs: List[Any], evaluators_results: Dict[str, List]) -> float:\n",
        "    \"\"\"\n",
        "    Summary evaluator: Average ballot accuracy score across all records.\n",
        "    \"\"\"\n",
        "    scores = evaluators_results.get(\"ballot_accuracy_score\", [])\n",
        "    if not scores:\n",
        "        return 0.0\n",
        "    \n",
        "    return sum(scores) / len(scores)\n",
        "\n",
        "\n",
        "def avg_llm_judge_score(inputs: List[Any], outputs: List[Any], expected_outputs: List[Any], evaluators_results: Dict[str, List]) -> float:\n",
        "    \"\"\"\n",
        "    Summary evaluator: Average LLM Judge quality score across all records.\n",
        "    \"\"\"\n",
        "    scores = evaluators_results.get(\"llm_judge_evaluator\", [])\n",
        "    if not scores:\n",
        "        return 0.0\n",
        "    \n",
        "    return sum(scores) / len(scores)\n",
        "\n",
        "print(\"‚úÖ Summary evaluators defined:\")\n",
        "print(\"   - overall_accuracy (float)\")\n",
        "print(\"   - success_rate (float)\")\n",
        "print(\"   - avg_ballot_accuracy (float)\")\n",
        "print(\"   - avg_llm_judge_score (float) ‚≠ê NEW!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.5 Create and Run Experiment\n",
        "\n",
        "Now we'll create an experiment and run it against our dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Experiment created: vote-extraction-baseline\n",
            "   Dataset: vote-extraction-bangbamru-1-10\n",
            "   Records: 11\n",
            "   Evaluators: 5 (includes LLM Judge)\n",
            "   Summary Evaluators: 4\n",
            "\n",
            "üìä View in Datadog: https://app.datadoghq.com/llm/experiments/None\n"
          ]
        }
      ],
      "source": [
        "# Create experiment\n",
        "experiment = LLMObs.experiment(\n",
        "    name=\"vote-extraction-baseline\",\n",
        "    task=vote_extraction_task,\n",
        "    dataset=experiment_dataset,\n",
        "    evaluators=[\n",
        "        exact_form_match,\n",
        "        ballot_accuracy_score,\n",
        "        vote_results_quality,\n",
        "        has_no_errors,\n",
        "        llm_judge_evaluator  # ‚≠ê NEW: LLM-as-Judge quality assessment\n",
        "    ],\n",
        "    summary_evaluators=[\n",
        "        overall_accuracy,\n",
        "        success_rate,\n",
        "        avg_ballot_accuracy,\n",
        "        avg_llm_judge_score  # ‚≠ê NEW: Average LLM judge score\n",
        "    ],\n",
        "    description=\"Baseline evaluation of vote extraction accuracy with LLM-as-Judge\",\n",
        "    config={\n",
        "        \"model\": \"gemini-2.5-flash\",\n",
        "        \"temperature\": 0.0,\n",
        "        \"version\": \"1.0\"\n",
        "    },\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Experiment created: {experiment.name}\")\n",
        "print(f\"   Dataset: {dataset_name}\")\n",
        "print(f\"   Records: {len(experiment_dataset)}\")\n",
        "print(f\"   Evaluators: {len(experiment._evaluators)} (includes LLM Judge)\")\n",
        "print(f\"   Summary Evaluators: {len(experiment._summary_evaluators)}\")\n",
        "print(f\"\\nüìä View in Datadog: {experiment.url}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.6 Run Experiment\n",
        "\n",
        "Run the experiment with various options.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Running experiment on all records...\n",
            "‚è±Ô∏è  This may take several minutes depending on dataset size...\n",
            "Processing: ‡∏ö‡∏≤‡∏á‡∏ö‡∏≥‡∏´‡∏£‡∏∏4 (6 pages)Processing: ‡∏ö‡∏≤‡∏á‡∏ö‡∏≥‡∏´‡∏£‡∏∏1 (6 pages)\n",
            "\n",
            "Processing: ‡∏ö‡∏≤‡∏á‡∏ö‡∏≥‡∏´‡∏£‡∏∏5 (6 pages)\n",
            "Processing: ‡∏ö‡∏≤‡∏á‡∏ö‡∏≥‡∏´‡∏£‡∏∏2 (6 pages)\n",
            "Processing: ‡∏ö‡∏≤‡∏á‡∏ö‡∏≥‡∏´‡∏£‡∏∏3 (6 pages)\n",
            "Processing: ‡∏ö‡∏≤‡∏á‡∏ö‡∏≥‡∏´‡∏£‡∏∏4 (6 pages)\n",
            "Processing: ‡∏ö‡∏≤‡∏á‡∏ö‡∏≥‡∏´‡∏£‡∏∏9 (6 pages)\n",
            "Processing: ‡∏ö‡∏≤‡∏á‡∏ö‡∏≥‡∏´‡∏£‡∏∏7 (6 pages)\n",
            "Processing: ‡∏ö‡∏≤‡∏á‡∏ö‡∏≥‡∏´‡∏£‡∏∏6 (6 pages)\n",
            "Processing: ‡∏ö‡∏≤‡∏á‡∏ö‡∏≥‡∏´‡∏£‡∏∏10 (6 pages)\n",
            "üîç Response Debug - ‡∏ö‡∏≤‡∏á‡∏ö‡∏≥‡∏´‡∏£‡∏∏4 (attempt 1):\n",
            "   - Has text: True\n",
            "   - Text length: 1709\n",
            "   - Finish reason: N/A\n",
            "   - Candidates: 1\n",
            "‚úÖ LLM Judge: Valid response for ‡∏ö‡∏≤‡∏á‡∏ö‡∏≥‡∏´‡∏£‡∏∏4 (attempt 1)\n",
            "üîç Response Debug - ‡∏ö‡∏≤‡∏á‡∏ö‡∏≥‡∏´‡∏£‡∏∏1 (attempt 1):\n",
            "   - Has text: True\n",
            "   - Text length: 995\n",
            "   - Finish reason: N/A\n",
            "   - Candidates: 1\n",
            "‚úÖ LLM Judge: Valid response for ‡∏ö‡∏≤‡∏á‡∏ö‡∏≥‡∏´‡∏£‡∏∏1 (attempt 1)\n",
            "üîç Response Debug - ‡∏ö‡∏≤‡∏á‡∏ö‡∏≥‡∏´‡∏£‡∏∏5 (attempt 1):\n",
            "   - Has text: False\n",
            "   - Text length: 0\n",
            "   - Finish reason: N/A\n",
            "   - Candidates: 1\n",
            "‚ö†Ô∏è  Response object exists but no text:\n",
            "   - finish_reason: N/A\n",
            "   - candidates: 1\n",
            "   - prompt_feedback: None\n",
            "   - text attr: True\n",
            "   - text value: None\n",
            "‚ö†Ô∏è  LLM Judge: Empty response for ‡∏ö‡∏≤‡∏á‡∏ö‡∏≥‡∏´‡∏£‡∏∏5 (attempt 1/3), retrying in 1.0s...\n",
            "üîç Response Debug - ‡∏ö‡∏≤‡∏á‡∏ö‡∏≥‡∏´‡∏£‡∏∏5 (attempt 2):\n",
            "   - Has text: False\n",
            "   - Text length: 0\n",
            "   - Finish reason: N/A\n",
            "   - Candidates: 1\n",
            "‚ö†Ô∏è  Response object exists but no text:\n",
            "   - finish_reason: N/A\n",
            "   - candidates: 1\n",
            "   - prompt_feedback: None\n",
            "   - text attr: True\n",
            "   - text value: None\n",
            "‚ö†Ô∏è  LLM Judge: Empty response for ‡∏ö‡∏≤‡∏á‡∏ö‡∏≥‡∏´‡∏£‡∏∏5 (attempt 2/3), retrying in 2.0s...\n",
            "üîç Response Debug - ‡∏ö‡∏≤‡∏á‡∏ö‡∏≥‡∏´‡∏£‡∏∏5 (attempt 3):\n",
            "   - Has text: False\n",
            "   - Text length: 0\n",
            "   - Finish reason: N/A\n",
            "   - Candidates: 1\n",
            "‚ö†Ô∏è  Response object exists but no text:\n",
            "   - finish_reason: N/A\n",
            "   - candidates: 1\n",
            "   - prompt_feedback: None\n",
            "   - text attr: True\n",
            "   - text value: None\n",
            "‚ùå LLM Judge: Empty response for ‡∏ö‡∏≤‡∏á‡∏ö‡∏≥‡∏´‡∏£‡∏∏5 after 3 attempts\n",
            "‚ö†Ô∏è  LLM Judge: Empty response for ‡∏ö‡∏≤‡∏á‡∏ö‡∏≥‡∏´‡∏£‡∏∏5\n",
            "üîç Response Debug - ‡∏ö‡∏≤‡∏á‡∏ö‡∏≥‡∏´‡∏£‡∏∏2 (attempt 1):\n",
            "   - Has text: True\n",
            "   - Text length: 1961\n",
            "   - Finish reason: N/A\n",
            "   - Candidates: 1\n",
            "‚úÖ LLM Judge: Valid response for ‡∏ö‡∏≤‡∏á‡∏ö‡∏≥‡∏´‡∏£‡∏∏2 (attempt 1)\n",
            "üîç Response Debug - ‡∏ö‡∏≤‡∏á‡∏ö‡∏≥‡∏´‡∏£‡∏∏3 (attempt 1):\n",
            "   - Has text: True\n",
            "   - Text length: 1240\n",
            "   - Finish reason: N/A\n",
            "   - Candidates: 1\n",
            "‚úÖ LLM Judge: Valid response for ‡∏ö‡∏≤‡∏á‡∏ö‡∏≥‡∏´‡∏£‡∏∏3 (attempt 1)\n",
            "üîç Response Debug - ‡∏ö‡∏≤‡∏á‡∏ö‡∏≥‡∏´‡∏£‡∏∏4 (attempt 1):\n",
            "   - Has text: True\n",
            "   - Text length: 2063\n",
            "   - Finish reason: N/A\n",
            "   - Candidates: 1\n",
            "‚úÖ LLM Judge: Valid response for ‡∏ö‡∏≤‡∏á‡∏ö‡∏≥‡∏´‡∏£‡∏∏4 (attempt 1)\n",
            "üîç Response Debug - ‡∏ö‡∏≤‡∏á‡∏ö‡∏≥‡∏´‡∏£‡∏∏9 (attempt 1):\n",
            "   - Has text: True\n",
            "   - Text length: 1532\n",
            "   - Finish reason: N/A\n",
            "   - Candidates: 1\n",
            "‚úÖ LLM Judge: Valid response for ‡∏ö‡∏≤‡∏á‡∏ö‡∏≥‡∏´‡∏£‡∏∏9 (attempt 1)\n",
            "üîç Response Debug - ‡∏ö‡∏≤‡∏á‡∏ö‡∏≥‡∏´‡∏£‡∏∏7 (attempt 1):\n",
            "   - Has text: True\n",
            "   - Text length: 1606\n",
            "   - Finish reason: N/A\n",
            "   - Candidates: 1\n",
            "‚úÖ LLM Judge: Valid response for ‡∏ö‡∏≤‡∏á‡∏ö‡∏≥‡∏´‡∏£‡∏∏7 (attempt 1)\n",
            "üîç Response Debug - ‡∏ö‡∏≤‡∏á‡∏ö‡∏≥‡∏´‡∏£‡∏∏6 (attempt 1):\n",
            "   - Has text: True\n",
            "   - Text length: 1639\n",
            "   - Finish reason: N/A\n",
            "   - Candidates: 1\n",
            "‚úÖ LLM Judge: Valid response for ‡∏ö‡∏≤‡∏á‡∏ö‡∏≥‡∏´‡∏£‡∏∏6 (attempt 1)\n",
            "üîç Response Debug - ‡∏ö‡∏≤‡∏á‡∏ö‡∏≥‡∏´‡∏£‡∏∏10 (attempt 1):\n",
            "   - Has text: True\n",
            "   - Text length: 1780\n",
            "   - Finish reason: N/A\n",
            "   - Candidates: 1\n",
            "‚úÖ LLM Judge: Valid response for ‡∏ö‡∏≤‡∏á‡∏ö‡∏≥‡∏´‡∏£‡∏∏10 (attempt 1)\n",
            "\n",
            "‚úÖ Experiment completed!\n",
            "   Total records processed: 10\n"
          ]
        }
      ],
      "source": [
        "# Option 1: Run on all records (default)\n",
        "print(\"üöÄ Running experiment on all records...\")\n",
        "print(\"‚è±Ô∏è  This may take several minutes depending on dataset size...\")\n",
        "\n",
        "results = experiment.run(\n",
        "    sample_size=10,\n",
        "    jobs=2,\n",
        "    raise_errors=True\n",
        ")\n",
        "\n",
        "# Option 2: Test on a sample (for faster iteration)\n",
        "# results = experiment.run(sample_size=3)\n",
        "\n",
        "# Option 3: Parallel processing (faster execution)\n",
        "# results = experiment.run(jobs=4)\n",
        "\n",
        "# Option 4: Stop on first error (for debugging)\n",
        "# results = experiment.run(raise_errors=True)\n",
        "\n",
        "print(f\"\\n‚úÖ Experiment completed!\")\n",
        "print(f\"   Total records processed: {len(results.get('rows', []))}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "https://app.datadoghq.com/llm/experiments/edd06b7d-bb70-47ef-ae67-41ca9dc226ff\n"
          ]
        }
      ],
      "source": [
        "print(experiment.url)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.7 View and Analyze Results\n",
        "\n",
        "Process and display experiment results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä Experiment Results Summary\n",
            "================================================================================\n",
            "\n",
            "üìÑ Per-Record Results:\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "1. Record 0:\n",
            "   Form: ‡∏ö‡∏≤‡∏á‡∏ö‡∏≥‡∏´‡∏£‡∏∏1\n",
            "   exact_form_match: True\n",
            "   ballot_accuracy_score: 100.00%\n",
            "   vote_results_quality: excellent\n",
            "   has_no_errors: True\n",
            "\n",
            "2. Record 1:\n",
            "   Form: ‡∏ö‡∏≤‡∏á‡∏ö‡∏≥‡∏´‡∏£‡∏∏5\n",
            "   exact_form_match: True\n",
            "   ballot_accuracy_score: 100.00%\n",
            "   vote_results_quality: excellent\n",
            "   has_no_errors: True\n",
            "\n",
            "3. Record 2:\n",
            "   Form: ‡∏ö‡∏≤‡∏á‡∏ö‡∏≥‡∏´‡∏£‡∏∏2\n",
            "   exact_form_match: True\n",
            "   ballot_accuracy_score: 100.00%\n",
            "   vote_results_quality: excellent\n",
            "   has_no_errors: True\n",
            "\n",
            "4. Record 3:\n",
            "   Form: ‡∏ö‡∏≤‡∏á‡∏ö‡∏≥‡∏´‡∏£‡∏∏3\n",
            "   exact_form_match: True\n",
            "   ballot_accuracy_score: 100.00%\n",
            "   vote_results_quality: excellent\n",
            "   has_no_errors: True\n",
            "\n",
            "5. Record 4:\n",
            "   Form: ‡∏ö‡∏≤‡∏á‡∏ö‡∏≥‡∏´‡∏£‡∏∏4\n",
            "   exact_form_match: True\n",
            "   ballot_accuracy_score: 100.00%\n",
            "   vote_results_quality: excellent\n",
            "   has_no_errors: True\n",
            "\n",
            "... and 5 more records\n",
            "\n",
            "\n",
            "üîó View full results in Datadog:\n",
            "   https://app.datadoghq.com/llm/experiments/edd06b7d-bb70-47ef-ae67-41ca9dc226ff\n"
          ]
        }
      ],
      "source": [
        "# Display summary statistics\n",
        "print(\"üìä Experiment Results Summary\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Summary evaluators results\n",
        "if \"summary_evaluators\" in results:\n",
        "    print(\"\\nüéØ Summary Metrics:\")\n",
        "    for metric_name, metric_value in results[\"summary_evaluators\"].items():\n",
        "        if isinstance(metric_value, float):\n",
        "            print(f\"   {metric_name}: {metric_value:.2%}\")\n",
        "        else:\n",
        "            print(f\"   {metric_name}: {metric_value}\")\n",
        "\n",
        "# Per-record results\n",
        "print(f\"\\nüìÑ Per-Record Results:\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "for i, row in enumerate(results.get(\"rows\", [])[:5], 1):  # Show first 5 records\n",
        "    print(f\"\\n{i}. Record {row.get('idx', i)}:\")\n",
        "    \n",
        "    # Input info\n",
        "    input_data = row.get(\"input\", {})\n",
        "    form_name = input_data.get(\"form_set_name\", \"Unknown\")\n",
        "    print(f\"   Form: {form_name}\")\n",
        "    \n",
        "    # Evaluations\n",
        "    evaluations = row.get(\"evaluations\", {})\n",
        "    for eval_name, eval_result in evaluations.items():\n",
        "        value = eval_result.get(\"value\")\n",
        "        if isinstance(value, float):\n",
        "            print(f\"   {eval_name}: {value:.2%}\")\n",
        "        else:\n",
        "            print(f\"   {eval_name}: {value}\")\n",
        "    \n",
        "    # Errors\n",
        "    error = row.get(\"error\", {})\n",
        "    if error.get(\"message\"):\n",
        "        print(f\"   ‚ö†Ô∏è Error: {error.get('message')}\")\n",
        "\n",
        "if len(results.get(\"rows\", [])) > 5:\n",
        "    print(f\"\\n... and {len(results.get('rows', [])) - 5} more records\")\n",
        "\n",
        "print(f\"\\n\\nüîó View full results in Datadog:\")\n",
        "print(f\"   {experiment.url}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 5. Model Comparison Experiments\n",
        "\n",
        "Let's run experiments with different Gemini models to compare performance on vote extraction tasks.\n",
        "\n",
        "**Models to Test**:\n",
        "- `gemini-2.5-flash` - Fast, cost-effective (baseline)\n",
        "- `gemini-2.5-flash-lite` - Ultra-fast, lower cost\n",
        "- `gemini-3-pro-preview` - Most capable, higher cost\n",
        "\n",
        "**Optimized Parameters for Data Extraction**:\n",
        "- `temperature=0.0` - Deterministic output (best for structured data)\n",
        "- `temperature=0.1` - Slightly more varied (testing tolerance)\n",
        "- `sample_size=10` - Full dataset evaluation\n",
        "- `jobs=2` - Parallel processing (balanced for API rate limits)\n",
        "- `raise_errors=True` - Fail fast for debugging\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.1 Experiment 1: gemini-2.5-flash (Baseline, Temperature 0.0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Experiment 1: gemini-2.5-flash with temperature 0.0 (deterministic)\n",
        "print(\"=\" * 80)\n",
        "print(\"üß™ Experiment 1: gemini-2.5-flash (temperature=0.0)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "experiment_flash_t0 = LLMObs.experiment(\n",
        "    name=\"vote-extraction-gemini-2.5-flash-t0\",\n",
        "    task=vote_extraction_task,\n",
        "    dataset=experiment_dataset,\n",
        "    evaluators=[\n",
        "        exact_form_match,\n",
        "        ballot_accuracy_score,\n",
        "        vote_results_quality,\n",
        "        has_no_errors\n",
        "    ],\n",
        "    summary_evaluators=[\n",
        "        overall_accuracy,\n",
        "        success_rate,\n",
        "        avg_ballot_accuracy\n",
        "    ],\n",
        "    metadata={\n",
        "        \"model\": \"gemini-2.5-flash\",\n",
        "        \"temperature\": 0.0,\n",
        "        \"purpose\": \"Baseline - deterministic extraction\",\n",
        "        \"cost_tier\": \"medium\"\n",
        "    }\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Created: {experiment_flash_t0.name}\")\n",
        "print(f\"üìä View: {experiment_flash_t0.url}\")\n",
        "\n",
        "# Run experiment\n",
        "print(\"\\nüöÄ Running experiment...\")\n",
        "results_flash_t0 = experiment_flash_t0.run(\n",
        "    sample_size=10,\n",
        "    jobs=2,\n",
        "    raise_errors=True\n",
        ")\n",
        "\n",
        "print(f\"\\n‚úÖ Completed! Processed {len(results_flash_t0.get('rows', []))} records\")\n",
        "print(f\"üìà Summary Metrics:\")\n",
        "for key, value in results_flash_t0.get('summary_metrics', {}).items():\n",
        "    print(f\"   - {key}: {value}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.2 Experiment 2: gemini-2.5-flash-lite (Ultra-Fast, Temperature 0.0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Experiment 2: gemini-2.5-flash-lite (ultra-fast, lower cost)\n",
        "print(\"=\" * 80)\n",
        "print(\"üß™ Experiment 2: gemini-2.5-flash-lite (temperature=0.0)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "experiment_flash_lite = LLMObs.experiment(\n",
        "    name=\"vote-extraction-gemini-2.5-flash-lite-t0\",\n",
        "    task=vote_extraction_task,\n",
        "    dataset=experiment_dataset,\n",
        "    evaluators=[\n",
        "        exact_form_match,\n",
        "        ballot_accuracy_score,\n",
        "        vote_results_quality,\n",
        "        has_no_errors\n",
        "    ],\n",
        "    summary_evaluators=[\n",
        "        overall_accuracy,\n",
        "        success_rate,\n",
        "        avg_ballot_accuracy\n",
        "    ],\n",
        "    metadata={\n",
        "        \"model\": \"gemini-2.5-flash-lite\",\n",
        "        \"temperature\": 0.0,\n",
        "        \"purpose\": \"Speed test - ultra-fast model\",\n",
        "        \"cost_tier\": \"low\"\n",
        "    }\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Created: {experiment_flash_lite.name}\")\n",
        "print(f\"üìä View: {experiment_flash_lite.url}\")\n",
        "\n",
        "# Run experiment\n",
        "print(\"\\nüöÄ Running experiment...\")\n",
        "results_flash_lite = experiment_flash_lite.run(\n",
        "    sample_size=10,\n",
        "    jobs=2,\n",
        "    raise_errors=True\n",
        ")\n",
        "\n",
        "print(f\"\\n‚úÖ Completed! Processed {len(results_flash_lite.get('rows', []))} records\")\n",
        "print(f\"üìà Summary Metrics:\")\n",
        "for key, value in results_flash_lite.get('summary_metrics', {}).items():\n",
        "    print(f\"   - {key}: {value}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.3 Experiment 3: gemini-3-pro-preview (Most Capable, Temperature 0.0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Experiment 3: gemini-3-pro-preview (most capable, higher accuracy expected)\n",
        "print(\"=\" * 80)\n",
        "print(\"üß™ Experiment 3: gemini-3-pro-preview (temperature=0.0)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "experiment_pro = LLMObs.experiment(\n",
        "    name=\"vote-extraction-gemini-3-pro-preview-t0\",\n",
        "    task=vote_extraction_task,\n",
        "    dataset=experiment_dataset,\n",
        "    evaluators=[\n",
        "        exact_form_match,\n",
        "        ballot_accuracy_score,\n",
        "        vote_results_quality,\n",
        "        has_no_errors\n",
        "    ],\n",
        "    summary_evaluators=[\n",
        "        overall_accuracy,\n",
        "        success_rate,\n",
        "        avg_ballot_accuracy\n",
        "    ],\n",
        "    metadata={\n",
        "        \"model\": \"gemini-3-pro-preview\",\n",
        "        \"temperature\": 0.0,\n",
        "        \"purpose\": \"Quality test - most capable model\",\n",
        "        \"cost_tier\": \"high\"\n",
        "    }\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Created: {experiment_pro.name}\")\n",
        "print(f\"üìä View: {experiment_pro.url}\")\n",
        "\n",
        "# Run experiment\n",
        "print(\"\\nüöÄ Running experiment...\")\n",
        "results_pro = experiment_pro.run(\n",
        "    sample_size=10,\n",
        "    jobs=2,\n",
        "    raise_errors=True\n",
        ")\n",
        "\n",
        "print(f\"\\n‚úÖ Completed! Processed {len(results_pro.get('rows', []))} records\")\n",
        "print(f\"üìà Summary Metrics:\")\n",
        "for key, value in results_pro.get('summary_metrics', {}).items():\n",
        "    print(f\"   - {key}: {value}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.4 Experiment 4: gemini-2.5-flash (Temperature 0.1) - Tolerance Test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Experiment 4: gemini-2.5-flash with temperature 0.1 (slight variation)\n",
        "print(\"=\" * 80)\n",
        "print(\"üß™ Experiment 4: gemini-2.5-flash (temperature=0.1)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "experiment_flash_t01 = LLMObs.experiment(\n",
        "    name=\"vote-extraction-gemini-2.5-flash-t01\",\n",
        "    task=vote_extraction_task,\n",
        "    dataset=experiment_dataset,\n",
        "    evaluators=[\n",
        "        exact_form_match,\n",
        "        ballot_accuracy_score,\n",
        "        vote_results_quality,\n",
        "        has_no_errors\n",
        "    ],\n",
        "    summary_evaluators=[\n",
        "        overall_accuracy,\n",
        "        success_rate,\n",
        "        avg_ballot_accuracy\n",
        "    ],\n",
        "    metadata={\n",
        "        \"model\": \"gemini-2.5-flash\",\n",
        "        \"temperature\": 0.1,\n",
        "        \"purpose\": \"Tolerance test - slightly more varied output\",\n",
        "        \"cost_tier\": \"medium\"\n",
        "    }\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Created: {experiment_flash_t01.name}\")\n",
        "print(f\"üìä View: {experiment_flash_t01.url}\")\n",
        "\n",
        "# Run experiment\n",
        "print(\"\\nüöÄ Running experiment...\")\n",
        "results_flash_t01 = experiment_flash_t01.run(\n",
        "    sample_size=10,\n",
        "    jobs=2,\n",
        "    raise_errors=True\n",
        ")\n",
        "\n",
        "print(f\"\\n‚úÖ Completed! Processed {len(results_flash_t01.get('rows', []))} records\")\n",
        "print(f\"üìà Summary Metrics:\")\n",
        "for key, value in results_flash_t01.get('summary_metrics', {}).items():\n",
        "    print(f\"   - {key}: {value}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.5 Compare Results\n",
        "\n",
        "Compare all experiments side-by-side to determine the best model for vote extraction.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare all experiments\n",
        "import pandas as pd\n",
        "\n",
        "experiments_data = [\n",
        "    {\n",
        "        \"Experiment\": \"gemini-2.5-flash (T=0.0)\",\n",
        "        \"Model\": \"gemini-2.5-flash\",\n",
        "        \"Temperature\": 0.0,\n",
        "        \"Cost Tier\": \"Medium\",\n",
        "        \"Overall Accuracy\": results_flash_t0.get('summary_metrics', {}).get('overall_accuracy', 'N/A'),\n",
        "        \"Success Rate\": results_flash_t0.get('summary_metrics', {}).get('success_rate', 'N/A'),\n",
        "        \"Avg Ballot Accuracy\": results_flash_t0.get('summary_metrics', {}).get('avg_ballot_accuracy', 'N/A'),\n",
        "        \"URL\": experiment_flash_t0.url\n",
        "    },\n",
        "    {\n",
        "        \"Experiment\": \"gemini-2.5-flash-lite (T=0.0)\",\n",
        "        \"Model\": \"gemini-2.5-flash-lite\",\n",
        "        \"Temperature\": 0.0,\n",
        "        \"Cost Tier\": \"Low\",\n",
        "        \"Overall Accuracy\": results_flash_lite.get('summary_metrics', {}).get('overall_accuracy', 'N/A'),\n",
        "        \"Success Rate\": results_flash_lite.get('summary_metrics', {}).get('success_rate', 'N/A'),\n",
        "        \"Avg Ballot Accuracy\": results_flash_lite.get('summary_metrics', {}).get('avg_ballot_accuracy', 'N/A'),\n",
        "        \"URL\": experiment_flash_lite.url\n",
        "    },\n",
        "    {\n",
        "        \"Experiment\": \"gemini-3-pro-preview (T=0.0)\",\n",
        "        \"Model\": \"gemini-3-pro-preview\",\n",
        "        \"Temperature\": 0.0,\n",
        "        \"Cost Tier\": \"High\",\n",
        "        \"Overall Accuracy\": results_pro.get('summary_metrics', {}).get('overall_accuracy', 'N/A'),\n",
        "        \"Success Rate\": results_pro.get('summary_metrics', {}).get('success_rate', 'N/A'),\n",
        "        \"Avg Ballot Accuracy\": results_pro.get('summary_metrics', {}).get('avg_ballot_accuracy', 'N/A'),\n",
        "        \"URL\": experiment_pro.url\n",
        "    },\n",
        "    {\n",
        "        \"Experiment\": \"gemini-2.5-flash (T=0.1)\",\n",
        "        \"Model\": \"gemini-2.5-flash\",\n",
        "        \"Temperature\": 0.1,\n",
        "        \"Cost Tier\": \"Medium\",\n",
        "        \"Overall Accuracy\": results_flash_t01.get('summary_metrics', {}).get('overall_accuracy', 'N/A'),\n",
        "        \"Success Rate\": results_flash_t01.get('summary_metrics', {}).get('success_rate', 'N/A'),\n",
        "        \"Avg Ballot Accuracy\": results_flash_t01.get('summary_metrics', {}).get('avg_ballot_accuracy', 'N/A'),\n",
        "        \"URL\": experiment_flash_t01.url\n",
        "    }\n",
        "]\n",
        "\n",
        "comparison_df = pd.DataFrame(experiments_data)\n",
        "\n",
        "print(\"=\" * 120)\n",
        "print(\"üìä EXPERIMENT COMPARISON SUMMARY\")\n",
        "print(\"=\" * 120)\n",
        "print()\n",
        "print(comparison_df[[\"Experiment\", \"Cost Tier\", \"Overall Accuracy\", \"Success Rate\", \"Avg Ballot Accuracy\"]].to_string(index=False))\n",
        "print()\n",
        "print(\"=\" * 120)\n",
        "print(\"üîó View in Datadog:\")\n",
        "for exp in experiments_data:\n",
        "    print(f\"   ‚Ä¢ {exp['Experiment']}: {exp['URL']}\")\n",
        "print(\"=\" * 120)\n",
        "\n",
        "# Identify best performing model\n",
        "print(\"\\n‚ú® RECOMMENDATIONS:\")\n",
        "print()\n",
        "\n",
        "# Find best accuracy\n",
        "best_accuracy = max(\n",
        "    [e[\"Overall Accuracy\"] for e in experiments_data if isinstance(e[\"Overall Accuracy\"], (int, float))],\n",
        "    default=0\n",
        ")\n",
        "\n",
        "# Find best cost/performance\n",
        "for exp in experiments_data:\n",
        "    if isinstance(exp[\"Overall Accuracy\"], (int, float)) and exp[\"Overall Accuracy\"] == best_accuracy:\n",
        "        print(f\"üèÜ BEST ACCURACY: {exp['Experiment']}\")\n",
        "        print(f\"   - Overall Accuracy: {exp['Overall Accuracy']}%\")\n",
        "        print(f\"   - Success Rate: {exp['Success Rate']}%\")\n",
        "        print(f\"   - Cost Tier: {exp['Cost Tier']}\")\n",
        "        break\n",
        "\n",
        "# Find best value\n",
        "flash_lite_exp = next((e for e in experiments_data if \"flash-lite\" in e[\"Experiment\"]), None)\n",
        "if flash_lite_exp and isinstance(flash_lite_exp[\"Overall Accuracy\"], (int, float)):\n",
        "    if flash_lite_exp[\"Overall Accuracy\"] >= 95:\n",
        "        print(f\"\\nüí∞ BEST VALUE: {flash_lite_exp['Experiment']}\")\n",
        "        print(f\"   - Overall Accuracy: {flash_lite_exp['Overall Accuracy']}% (excellent)\")\n",
        "        print(f\"   - Cost Tier: {flash_lite_exp['Cost Tier']} (fastest, cheapest)\")\n",
        "        print(f\"   - Recommendation: Use for high-volume processing\")\n",
        "\n",
        "# Temperature comparison\n",
        "flash_t0 = next((e for e in experiments_data if e[\"Temperature\"] == 0.0 and e[\"Model\"] == \"gemini-2.5-flash\"), None)\n",
        "flash_t01 = next((e for e in experiments_data if e[\"Temperature\"] == 0.1 and e[\"Model\"] == \"gemini-2.5-flash\"), None)\n",
        "if flash_t0 and flash_t01:\n",
        "    print(f\"\\nüå°Ô∏è TEMPERATURE IMPACT:\")\n",
        "    print(f\"   - T=0.0 Accuracy: {flash_t0['Overall Accuracy']}%\")\n",
        "    print(f\"   - T=0.1 Accuracy: {flash_t01['Overall Accuracy']}%\")\n",
        "    if isinstance(flash_t0['Overall Accuracy'], (int, float)) and isinstance(flash_t01['Overall Accuracy'], (int, float)):\n",
        "        diff = abs(flash_t0['Overall Accuracy'] - flash_t01['Overall Accuracy'])\n",
        "        if diff < 5:\n",
        "            print(f\"   - Impact: Minimal ({diff}% difference)\")\n",
        "            print(f\"   - Recommendation: Use T=0.0 for deterministic results\")\n",
        "        else:\n",
        "            print(f\"   - Impact: Significant ({diff}% difference)\")\n",
        "            print(f\"   - Recommendation: Use T=0.0 for structured data extraction\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.6 Production Deployment Strategy\n",
        "\n",
        "Based on experiment results, choose the best model for production deployment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 80)\n",
        "print(\"üöÄ PRODUCTION DEPLOYMENT STRATEGY\")\n",
        "print(\"=\" * 80)\n",
        "print()\n",
        "\n",
        "print(\"üìã Decision Framework:\")\n",
        "print()\n",
        "\n",
        "print(\"1Ô∏è‚É£  HIGH VOLUME / COST SENSITIVE:\")\n",
        "print(\"   Model: gemini-2.5-flash-lite\")\n",
        "print(\"   Temperature: 0.0\")\n",
        "print(\"   Rationale: Lowest cost, fastest processing\")\n",
        "print(\"   Use When: Processing thousands of forms, budget constraints\")\n",
        "print(\"   Trade-off: Slightly lower accuracy acceptable\")\n",
        "print()\n",
        "\n",
        "print(\"2Ô∏è‚É£  BALANCED (RECOMMENDED):\")\n",
        "print(\"   Model: gemini-2.5-flash\")\n",
        "print(\"   Temperature: 0.0\")\n",
        "print(\"   Rationale: Best balance of cost, speed, and accuracy\")\n",
        "print(\"   Use When: Standard production workloads\")\n",
        "print(\"   Trade-off: None - optimal for most use cases\")\n",
        "print()\n",
        "\n",
        "print(\"3Ô∏è‚É£  MAXIMUM QUALITY:\")\n",
        "print(\"   Model: gemini-3-pro-preview\")\n",
        "print(\"   Temperature: 0.0\")\n",
        "print(\"   Rationale: Highest accuracy, most capable\")\n",
        "print(\"   Use When: Critical data, legal/compliance requirements\")\n",
        "print(\"   Trade-off: Higher cost, slower processing\")\n",
        "print()\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"üîß IMPLEMENTATION STEPS:\")\n",
        "print(\"=\" * 80)\n",
        "print()\n",
        "\n",
        "print(\"1. Update backend configuration:\")\n",
        "print(\"   File: services/fastapi-backend/app/config.py\")\n",
        "print(\"   \")\n",
        "print(\"   # Set based on experiment results\")\n",
        "print(\"   DEFAULT_MODEL = 'gemini-2.5-flash'  # or your chosen model\")\n",
        "print(\"   DEFAULT_TEMPERATURE = 0.0\")\n",
        "print()\n",
        "\n",
        "print(\"2. Deploy to Cloud Run:\")\n",
        "print(\"   \")\n",
        "print(\"   git add -A\")\n",
        "print(\"   git commit -m 'chore: Update to optimal model from experiments'\")\n",
        "print(\"   git push origin main\")\n",
        "print(\"   \")\n",
        "print(\"   # CI/CD will automatically deploy\")\n",
        "print()\n",
        "\n",
        "print(\"3. Monitor in production:\")\n",
        "print(\"   - Track accuracy metrics in Datadog LLMObs\")\n",
        "print(\"   - Set up alerts for accuracy drops\")\n",
        "print(\"   - Review cost vs. performance monthly\")\n",
        "print()\n",
        "\n",
        "print(\"4. Continuous improvement:\")\n",
        "print(\"   - Add more ground truth data to dataset\")\n",
        "print(\"   - Re-run experiments quarterly\")\n",
        "print(\"   - Test new model versions as they release\")\n",
        "print()\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"üìä MONITORING CHECKLIST:\")\n",
        "print(\"=\" * 80)\n",
        "print()\n",
        "print(\"‚úÖ Set up Datadog monitors for:\")\n",
        "print(\"   ‚Ä¢ Overall accuracy threshold (e.g., < 95%)\")\n",
        "print(\"   ‚Ä¢ Success rate threshold (e.g., < 90%)\")\n",
        "print(\"   ‚Ä¢ Error rate spike (e.g., > 5%)\")\n",
        "print(\"   ‚Ä¢ Latency increase (e.g., p95 > 10s)\")\n",
        "print(\"   ‚Ä¢ Cost anomalies\")\n",
        "print()\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"‚ú® Experiment Complete! Ready for production deployment.\")\n",
        "print(\"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 6. Wrapper Function: Easy Experiment Configuration\n",
        "\n",
        "Create a reusable wrapper function for running multiple experiments with custom configurations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Wrapper function defined: run_model_experiments()\n",
            "\n",
            "üìñ Quick Usage:\n",
            "\n",
            "results = run_model_experiments(\n",
            "    model_configs=[\n",
            "        {\"model\": \"gemini-2.5-flash\", \"temperature\": 0.0},\n",
            "        {\"model\": \"gemini-2.5-flash-lite\", \"temperature\": 0.0},\n",
            "    ],\n",
            "    sample_size=10,\n",
            "    jobs=2,\n",
            "    raise_errors=True\n",
            ")\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from ddtrace.llmobs import LLMObs\n",
        "from typing import Dict, Any, Optional, List, Callable\n",
        "\n",
        "def run_model_experiments(\n",
        "    # LLMObs Configuration\n",
        "    ml_app: str = \"vote-extractor\",\n",
        "    api_key: str = None,\n",
        "    site: str = \"datadoghq.com\",\n",
        "    agentless_enabled: bool = True,\n",
        "    project_name: str = \"vote-extraction-project\",\n",
        "    \n",
        "    # Dataset Configuration\n",
        "    dataset_name: str = \"vote-extraction-bangbamru-1-10\",\n",
        "    dataset_version: Optional[int] = None,\n",
        "    \n",
        "    # Models and Temperatures to Test\n",
        "    model_configs: Optional[List[Dict[str, Any]]] = None,\n",
        "    \n",
        "    # Task Function\n",
        "    task_function: Optional[Callable] = None,\n",
        "    \n",
        "    # Evaluators\n",
        "    evaluators: Optional[List[Callable]] = None,\n",
        "    summary_evaluators: Optional[List[Callable]] = None,\n",
        "    \n",
        "    # Run Configuration\n",
        "    sample_size: Optional[int] = None,\n",
        "    jobs: int = 2,\n",
        "    raise_errors: bool = True,\n",
        "    \n",
        "    # Options\n",
        "    show_comparison: bool = True,\n",
        "    return_results: bool = True\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Run multiple LLM experiments with different model configurations.\n",
        "    \n",
        "    Args:\n",
        "        ml_app: Datadog LLMObs application name\n",
        "        api_key: Datadog API key (defaults to DD_API_KEY env var)\n",
        "        site: Datadog site (e.g., datadoghq.com, datadoghq.eu)\n",
        "        agentless_enabled: Enable agentless mode\n",
        "        project_name: Datadog LLMObs project name\n",
        "        \n",
        "        dataset_name: Name of the dataset to load from Datadog\n",
        "        dataset_version: Specific version to use (defaults to latest)\n",
        "        \n",
        "        model_configs: List of model configurations to test. Each dict should have:\n",
        "                       - model: str (model name)\n",
        "                       - temperature: float (0.0-1.0)\n",
        "                       - name_suffix: str (optional, for experiment naming)\n",
        "                       - metadata: dict (optional, extra metadata)\n",
        "        \n",
        "        task_function: Task function to use (defaults to vote_extraction_task)\n",
        "        evaluators: List of evaluator functions (defaults to standard set)\n",
        "        summary_evaluators: List of summary evaluator functions\n",
        "        \n",
        "        sample_size: Number of records to test (None = all records)\n",
        "        jobs: Number of parallel jobs\n",
        "        raise_errors: Stop on first error\n",
        "        \n",
        "        show_comparison: Print comparison table at the end\n",
        "        return_results: Return experiment results dictionary\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary with experiment results and comparison data\n",
        "    \n",
        "    Example:\n",
        "        >>> results = run_model_experiments(\n",
        "        ...     model_configs=[\n",
        "        ...         {\"model\": \"gemini-2.5-flash\", \"temperature\": 0.0},\n",
        "        ...         {\"model\": \"gemini-2.5-flash-lite\", \"temperature\": 0.0},\n",
        "        ...         {\"model\": \"gemini-3-pro-preview\", \"temperature\": 0.0}\n",
        "        ...     ],\n",
        "        ...     sample_size=10,\n",
        "        ...     jobs=2\n",
        "        ... )\n",
        "    \"\"\"\n",
        "    from ddtrace.llmobs import LLMObs\n",
        "    import pandas as pd\n",
        "    \n",
        "    # Initialize LLMObs\n",
        "    print(\"=\" * 80)\n",
        "    print(\"üîß INITIALIZING DATADOG LLMOBS\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    if api_key is None:\n",
        "        api_key = os.getenv(\"DD_API_KEY\")\n",
        "    \n",
        "    if not api_key:\n",
        "        raise ValueError(\"DD_API_KEY not found in environment or parameters\")\n",
        "    \n",
        "    try:\n",
        "        LLMObs.enable(\n",
        "            ml_app=ml_app,\n",
        "            api_key=api_key,\n",
        "            site=site,\n",
        "            agentless_enabled=agentless_enabled,\n",
        "            project_name=project_name,\n",
        "        )\n",
        "        print(f\"‚úÖ LLMObs enabled\")\n",
        "        print(f\"   App: {ml_app}\")\n",
        "        print(f\"   Site: {site}\")\n",
        "        print(f\"   Project: {project_name}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è  LLMObs already enabled or error: {e}\")\n",
        "    \n",
        "    # Load dataset\n",
        "    print(f\"\\nüìä Loading dataset: {dataset_name}\")\n",
        "    dataset = LLMObs.pull_dataset(\n",
        "        dataset_name=dataset_name,\n",
        "        project_name=project_name,\n",
        "        version=dataset_version\n",
        "    )\n",
        "    print(f\"‚úÖ Dataset loaded: {len(dataset)} records\")\n",
        "    \n",
        "    # Extract dataset ID from URL for comparison link\n",
        "    dataset_id = None\n",
        "    try:\n",
        "        # Dataset URL format: https://app.datadoghq.com/llm/datasets/{dataset_id}\n",
        "        if hasattr(dataset, 'url') and dataset.url:\n",
        "            dataset_id = dataset.url.split('/datasets/')[-1]\n",
        "            print(f\"   Dataset ID: {dataset_id}\")\n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ö†Ô∏è  Could not extract dataset ID: {e}\")\n",
        "    \n",
        "    # Default model configurations\n",
        "    if model_configs is None:\n",
        "        model_configs = [\n",
        "            {\"model\": \"gemini-2.5-flash\", \"temperature\": 0.0, \"name_suffix\": \"flash-t0\"},\n",
        "            {\"model\": \"gemini-2.5-flash-lite\", \"temperature\": 0.0, \"name_suffix\": \"flash-lite-t0\"},\n",
        "            {\"model\": \"gemini-3-pro-preview\", \"temperature\": 0.0, \"name_suffix\": \"pro-t0\"},\n",
        "        ]\n",
        "    \n",
        "    # Default task function\n",
        "    if task_function is None:\n",
        "        if 'vote_extraction_task' not in globals():\n",
        "            raise ValueError(\"task_function not provided and vote_extraction_task not defined\")\n",
        "        task_function = vote_extraction_task\n",
        "    \n",
        "    # Default evaluators\n",
        "    if evaluators is None:\n",
        "        evaluators = [exact_form_match, ballot_accuracy_score, vote_results_quality, has_no_errors, llm_judge_evaluator]  # ‚≠ê Added LLM Judge\n",
        "    \n",
        "    if summary_evaluators is None:\n",
        "        summary_evaluators = [overall_accuracy, success_rate, avg_ballot_accuracy, avg_llm_judge_score]  # ‚≠ê Added LLM Judge Score\n",
        "    \n",
        "    # Run experiments\n",
        "    print(f\"\\n{'=' * 80}\")\n",
        "    print(f\"üöÄ RUNNING {len(model_configs)} EXPERIMENTS\")\n",
        "    print(f\"{'=' * 80}\")\n",
        "    print(f\"   Sample Size: {sample_size or 'All records'}\")\n",
        "    print(f\"   Parallel Jobs: {jobs}\")\n",
        "    print(f\"   Raise Errors: {raise_errors}\")\n",
        "    print()\n",
        "    \n",
        "    all_results = []\n",
        "    \n",
        "    for i, config in enumerate(model_configs, 1):\n",
        "        model = config.get(\"model\")\n",
        "        temperature = config.get(\"temperature\", 0.0)\n",
        "        name_suffix = config.get(\"name_suffix\", f\"{model.split('-')[-1]}-t{int(temperature*10)}\")\n",
        "        extra_metadata = config.get(\"metadata\", {})\n",
        "        \n",
        "        print(f\"\\n{'‚îÄ' * 80}\")\n",
        "        print(f\"üß™ Experiment {i}/{len(model_configs)}: {model} (T={temperature})\")\n",
        "        print(f\"{'‚îÄ' * 80}\")\n",
        "        \n",
        "        # Create experiment\n",
        "        experiment_name = f\"vote-extraction-{name_suffix}\"\n",
        "        \n",
        "        # Prepare tags (combine model, temperature, and extra metadata)\n",
        "        tags = {\n",
        "            \"model\": model,\n",
        "            \"temperature\": str(temperature),\n",
        "            **{k: str(v) for k, v in extra_metadata.items()}\n",
        "        }\n",
        "        \n",
        "        experiment = LLMObs.experiment(\n",
        "            name=experiment_name,\n",
        "            task=task_function,\n",
        "            dataset=dataset,\n",
        "            evaluators=evaluators,\n",
        "            summary_evaluators=summary_evaluators,\n",
        "            tags=tags\n",
        "        )\n",
        "        \n",
        "        print(f\"‚úÖ Created: {experiment.name}\")\n",
        "        print(f\"üìä View: {experiment.url}\")\n",
        "        \n",
        "        # Run experiment\n",
        "        print(f\"‚è±Ô∏è  Running...\")\n",
        "        try:\n",
        "            results = experiment.run(\n",
        "                sample_size=sample_size,\n",
        "                jobs=jobs,\n",
        "                raise_errors=raise_errors\n",
        "            )\n",
        "            \n",
        "            print(f\"‚úÖ Completed! Processed {len(results.get('rows', []))} records\")\n",
        "            \n",
        "            # Collect results\n",
        "            all_results.append({\n",
        "                \"experiment_name\": experiment_name,\n",
        "                \"model\": model,\n",
        "                \"temperature\": temperature,\n",
        "                \"sample_size\": len(results.get('rows', [])),\n",
        "                \"summary_metrics\": results.get('summary_metrics', {}),\n",
        "                \"url\": experiment.url,\n",
        "                \"status\": \"success\"\n",
        "            })\n",
        "            \n",
        "            # Print summary metrics\n",
        "            if results.get('summary_metrics'):\n",
        "                print(\"üìà Summary Metrics:\")\n",
        "                for key, value in results['summary_metrics'].items():\n",
        "                    print(f\"   - {key}: {value}\")\n",
        "        \n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error: {e}\")\n",
        "            all_results.append({\n",
        "                \"experiment_name\": experiment_name,\n",
        "                \"model\": model,\n",
        "                \"temperature\": temperature,\n",
        "                \"sample_size\": 0,\n",
        "                \"summary_metrics\": {},\n",
        "                \"url\": experiment.url,\n",
        "                \"status\": \"failed\",\n",
        "                \"error\": str(e)\n",
        "            })\n",
        "            \n",
        "            if raise_errors:\n",
        "                raise\n",
        "    \n",
        "    # Show comparison\n",
        "    if show_comparison and all_results:\n",
        "        print(f\"\\n{'=' * 120}\")\n",
        "        print(\"üìä EXPERIMENT COMPARISON\")\n",
        "        print(f\"{'=' * 120}\\n\")\n",
        "        \n",
        "        # Create comparison DataFrame\n",
        "        comparison_data = []\n",
        "        for result in all_results:\n",
        "            metrics = result['summary_metrics']\n",
        "            comparison_data.append({\n",
        "                \"Experiment\": result['experiment_name'],\n",
        "                \"Model\": result['model'],\n",
        "                \"Temperature\": result['temperature'],\n",
        "                \"Status\": result['status'],\n",
        "                \"Records\": result['sample_size'],\n",
        "                \"Overall Accuracy\": metrics.get('overall_accuracy', 'N/A'),\n",
        "                \"Success Rate\": metrics.get('success_rate', 'N/A'),\n",
        "                \"Avg Ballot Accuracy\": metrics.get('avg_ballot_accuracy', 'N/A'),\n",
        "                \"Avg LLM Judge Score\": metrics.get('avg_llm_judge_score', 'N/A'),  # ‚≠ê NEW\n",
        "            })\n",
        "        \n",
        "        df = pd.DataFrame(comparison_data)\n",
        "        print(df.to_string(index=False))\n",
        "        \n",
        "        print(f\"\\n{'=' * 120}\")\n",
        "        print(\"üîó View in Datadog:\")\n",
        "        for result in all_results:\n",
        "            status_icon = \"‚úÖ\" if result['status'] == \"success\" else \"‚ùå\"\n",
        "            print(f\"   {status_icon} {result['experiment_name']}: {result['url']}\")\n",
        "        print(f\"{'=' * 120}\")\n",
        "        \n",
        "        # Generate comparison URL if dataset_id is available\n",
        "        if dataset_id:\n",
        "            comparison_url = f\"https://app.datadoghq.com/llm/experiments?dataset={dataset_id}&project={project_name}\"\n",
        "            print(f\"\\nüîç Compare all experiments side-by-side:\")\n",
        "            print(f\"   {comparison_url}\")\n",
        "            print(f\"{'=' * 120}\\n\")\n",
        "        else:\n",
        "            print()\n",
        "        \n",
        "        # Best performing\n",
        "        successful_results = [r for r in all_results if r['status'] == 'success']\n",
        "        if successful_results:\n",
        "            best_accuracy = max(\n",
        "                [r['summary_metrics'].get('overall_accuracy', 0) for r in successful_results],\n",
        "                default=0\n",
        "            )\n",
        "            \n",
        "            if best_accuracy > 0:\n",
        "                best_exp = next(\n",
        "                    (r for r in successful_results if r['summary_metrics'].get('overall_accuracy') == best_accuracy),\n",
        "                    None\n",
        "                )\n",
        "                if best_exp:\n",
        "                    print(\"üèÜ BEST PERFORMER:\")\n",
        "                    print(f\"   Model: {best_exp['model']}\")\n",
        "                    print(f\"   Temperature: {best_exp['temperature']}\")\n",
        "                    print(f\"   Overall Accuracy: {best_accuracy}%\")\n",
        "                    print()\n",
        "    \n",
        "    # Return results\n",
        "    if return_results:\n",
        "        result_dict = {\n",
        "            \"experiments\": all_results,\n",
        "            \"total_experiments\": len(all_results),\n",
        "            \"successful_experiments\": len([r for r in all_results if r['status'] == 'success']),\n",
        "            \"failed_experiments\": len([r for r in all_results if r['status'] == 'failed']),\n",
        "            \"dataset_name\": dataset_name,\n",
        "            \"dataset_size\": len(dataset),\n",
        "            \"project_name\": project_name\n",
        "        }\n",
        "        \n",
        "        # Add comparison URL if available\n",
        "        if dataset_id:\n",
        "            result_dict[\"comparison_url\"] = f\"https://app.datadoghq.com/llm/experiments?dataset={dataset_id}&project={project_name}\"\n",
        "            result_dict[\"dataset_id\"] = dataset_id\n",
        "        \n",
        "        return result_dict\n",
        "    \n",
        "    return None\n",
        "\n",
        "\n",
        "# Print function signature help\n",
        "print(\"‚úÖ Wrapper function defined: run_model_experiments()\")\n",
        "print(\"\\nüìñ Quick Usage:\")\n",
        "print(\"\"\"\n",
        "results = run_model_experiments(\n",
        "    model_configs=[\n",
        "        {\"model\": \"gemini-2.5-flash\", \"temperature\": 0.0},\n",
        "        {\"model\": \"gemini-2.5-flash-lite\", \"temperature\": 0.0},\n",
        "    ],\n",
        "    sample_size=10,\n",
        "    jobs=2,\n",
        "    raise_errors=True\n",
        ")\n",
        "\"\"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.1 Example 1: Run with Default Configuration\n",
        "\n",
        "The simplest usage - runs 3 default models with temperature 0.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "üîß INITIALIZING DATADOG LLMOBS\n",
            "================================================================================\n",
            "‚úÖ LLMObs enabled\n",
            "   App: vote-extractor\n",
            "   Site: datadoghq.com\n",
            "   Project: vote-extraction-project\n",
            "\n",
            "üìä Loading dataset: vote-extraction-bangbamru-1-10\n",
            "‚úÖ Dataset loaded: 11 records\n",
            "   Dataset ID: 241bfded-e79d-4d2d-bbc4-a74bb06d85f9\n",
            "\n",
            "================================================================================\n",
            "üöÄ RUNNING 3 EXPERIMENTS\n",
            "================================================================================\n",
            "   Sample Size: 10\n",
            "   Parallel Jobs: 2\n",
            "   Raise Errors: True\n",
            "\n",
            "\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "üß™ Experiment 1/3: gemini-2.5-flash (T=0.0)\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚úÖ Created: vote-extraction-flash-t0\n",
            "üìä View: https://app.datadoghq.com/llm/experiments/None\n",
            "‚è±Ô∏è  Running...\n",
            "Processing: ‡∏ö‡∏≤‡∏á‡∏ö‡∏≥‡∏´‡∏£‡∏∏4 (6 pages)Processing: ‡∏ö‡∏≤‡∏á‡∏ö‡∏≥‡∏´‡∏£‡∏∏1 (6 pages)\n",
            "\n",
            "Processing: ‡∏ö‡∏≤‡∏á‡∏ö‡∏≥‡∏´‡∏£‡∏∏5 (6 pages)\n",
            "Processing: ‡∏ö‡∏≤‡∏á‡∏ö‡∏≥‡∏´‡∏£‡∏∏2 (6 pages)\n",
            "Processing: ‡∏ö‡∏≤‡∏á‡∏ö‡∏≥‡∏´‡∏£‡∏∏3 (6 pages)\n",
            "Processing: ‡∏ö‡∏≤‡∏á‡∏ö‡∏≥‡∏´‡∏£‡∏∏4 (6 pages)\n",
            "Processing: ‡∏ö‡∏≤‡∏á‡∏ö‡∏≥‡∏´‡∏£‡∏∏9 (6 pages)\n",
            "Processing: ‡∏ö‡∏≤‡∏á‡∏ö‡∏≥‡∏´‡∏£‡∏∏7 (6 pages)\n",
            "Processing: ‡∏ö‡∏≤‡∏á‡∏ö‡∏≥‡∏´‡∏£‡∏∏6 (6 pages)\n",
            "Processing: ‡∏ö‡∏≤‡∏á‡∏ö‡∏≥‡∏´‡∏£‡∏∏10 (6 pages)\n",
            "‚úÖ Completed! Processed 10 records\n",
            "\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "üß™ Experiment 2/3: gemini-2.5-flash-lite (T=0.0)\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚úÖ Created: vote-extraction-flash-lite-t0\n",
            "üìä View: https://app.datadoghq.com/llm/experiments/None\n",
            "‚è±Ô∏è  Running...\n",
            "Processing: ‡∏ö‡∏≤‡∏á‡∏ö‡∏≥‡∏´‡∏£‡∏∏4 (6 pages)Processing: ‡∏ö‡∏≤‡∏á‡∏ö‡∏≥‡∏´‡∏£‡∏∏1 (6 pages)\n",
            "\n",
            "Processing: ‡∏ö‡∏≤‡∏á‡∏ö‡∏≥‡∏´‡∏£‡∏∏5 (6 pages)\n",
            "Processing: ‡∏ö‡∏≤‡∏á‡∏ö‡∏≥‡∏´‡∏£‡∏∏2 (6 pages)\n",
            "Processing: ‡∏ö‡∏≤‡∏á‡∏ö‡∏≥‡∏´‡∏£‡∏∏3 (6 pages)\n",
            "Processing: ‡∏ö‡∏≤‡∏á‡∏ö‡∏≥‡∏´‡∏£‡∏∏4 (6 pages)\n",
            "Processing: ‡∏ö‡∏≤‡∏á‡∏ö‡∏≥‡∏´‡∏£‡∏∏9 (6 pages)\n",
            "Processing: ‡∏ö‡∏≤‡∏á‡∏ö‡∏≥‡∏´‡∏£‡∏∏7 (6 pages)\n",
            "‚ùå Error processing ‡∏ö‡∏≤‡∏á‡∏ö‡∏≥‡∏´‡∏£‡∏∏9: Server error '500 Internal Server Error' for url 'http://localhost:8000/api/v1/vote-extraction/extract'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/500\n",
            "Processing: ‡∏ö‡∏≤‡∏á‡∏ö‡∏≥‡∏´‡∏£‡∏∏6 (6 pages)\n",
            "Processing: ‡∏ö‡∏≤‡∏á‡∏ö‡∏≥‡∏´‡∏£‡∏∏10 (6 pages)\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/genai-app-python/.venv/lib/python3.13/site-packages/ddtrace/llmobs/_experiment.py:541\u001b[39m, in \u001b[36mExperiment._run_task\u001b[39m\u001b[34m(self, jobs, run, raise_errors, sample_size)\u001b[39m\n\u001b[32m    540\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m ThreadPoolExecutor(max_workers=jobs) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[32m--> \u001b[39m\u001b[32m541\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mexecutor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    542\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_process_record\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    543\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubset_dataset\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    544\u001b[39m \u001b[43m        \u001b[49m\u001b[43mitertools\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrepeat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubset_dataset\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    545\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    546\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m:\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.11/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/_base.py:619\u001b[39m, in \u001b[36mExecutor.map.<locals>.result_iterator\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    618\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m619\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[43m_result_or_cancel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    620\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.11/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/_base.py:317\u001b[39m, in \u001b[36m_result_or_cancel\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    316\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m317\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.11/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/_base.py:451\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.__get_result()\n\u001b[32m--> \u001b[39m\u001b[32m451\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_condition\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.11/Frameworks/Python.framework/Versions/3.13/lib/python3.13/threading.py:359\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    358\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m359\u001b[39m     \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    360\u001b[39m     gotit = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Example 1: Use defaults (3 models: flash, flash-lite, pro-preview at T=0.0)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m results = \u001b[43mrun_model_experiments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43msample_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjobs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mraise_errors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m      6\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Results include comparison and best performer\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33müìä Summary:\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 194\u001b[39m, in \u001b[36mrun_model_experiments\u001b[39m\u001b[34m(ml_app, api_key, site, agentless_enabled, project_name, dataset_name, dataset_version, model_configs, task_function, evaluators, summary_evaluators, sample_size, jobs, raise_errors, show_comparison, return_results)\u001b[39m\n\u001b[32m    192\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m‚è±Ô∏è  Running...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    193\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m194\u001b[39m     results = \u001b[43mexperiment\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m        \u001b[49m\u001b[43msample_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[43m        \u001b[49m\u001b[43mjobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m        \u001b[49m\u001b[43mraise_errors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mraise_errors\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    200\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m‚úÖ Completed! Processed \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(results.get(\u001b[33m'\u001b[39m\u001b[33mrows\u001b[39m\u001b[33m'\u001b[39m,\u001b[38;5;250m \u001b[39m[]))\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m records\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    202\u001b[39m     \u001b[38;5;66;03m# Collect results\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/genai-app-python/.venv/lib/python3.13/site-packages/ddtrace/llmobs/_experiment.py:435\u001b[39m, in \u001b[36mExperiment.run\u001b[39m\u001b[34m(self, jobs, raise_errors, sample_size)\u001b[39m\n\u001b[32m    433\u001b[39m \u001b[38;5;28mself\u001b[39m._tags[\u001b[33m\"\u001b[39m\u001b[33mrun_id\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mstr\u001b[39m(run._id)\n\u001b[32m    434\u001b[39m \u001b[38;5;28mself\u001b[39m._tags[\u001b[33m\"\u001b[39m\u001b[33mrun_iteration\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mstr\u001b[39m(run._run_iteration)\n\u001b[32m--> \u001b[39m\u001b[32m435\u001b[39m task_results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_task\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraise_errors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    436\u001b[39m evaluations = \u001b[38;5;28mself\u001b[39m._run_evaluators(task_results, raise_errors=raise_errors)\n\u001b[32m    437\u001b[39m summary_evals = \u001b[38;5;28mself\u001b[39m._run_summary_evaluators(task_results, evaluations, raise_errors)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/genai-app-python/.venv/lib/python3.13/site-packages/ddtrace/llmobs/_experiment.py:540\u001b[39m, in \u001b[36mExperiment._run_task\u001b[39m\u001b[34m(self, jobs, run, raise_errors, sample_size)\u001b[39m\n\u001b[32m    538\u001b[39m     subset_dataset = \u001b[38;5;28mself\u001b[39m._dataset\n\u001b[32m    539\u001b[39m task_results = []\n\u001b[32m--> \u001b[39m\u001b[32m540\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m ThreadPoolExecutor(max_workers=jobs) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[32m    541\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m executor.map(\n\u001b[32m    542\u001b[39m         \u001b[38;5;28mself\u001b[39m._process_record,\n\u001b[32m    543\u001b[39m         \u001b[38;5;28menumerate\u001b[39m(subset_dataset),\n\u001b[32m    544\u001b[39m         itertools.repeat(run, \u001b[38;5;28mlen\u001b[39m(subset_dataset)),\n\u001b[32m    545\u001b[39m     ):\n\u001b[32m    546\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.11/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/_base.py:647\u001b[39m, in \u001b[36mExecutor.__exit__\u001b[39m\u001b[34m(self, exc_type, exc_val, exc_tb)\u001b[39m\n\u001b[32m    646\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, exc_type, exc_val, exc_tb):\n\u001b[32m--> \u001b[39m\u001b[32m647\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mshutdown\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    648\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.11/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/thread.py:239\u001b[39m, in \u001b[36mThreadPoolExecutor.shutdown\u001b[39m\u001b[34m(self, wait, cancel_futures)\u001b[39m\n\u001b[32m    237\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[32m    238\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._threads:\n\u001b[32m--> \u001b[39m\u001b[32m239\u001b[39m         \u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.11/Frameworks/Python.framework/Versions/3.13/lib/python3.13/threading.py:1095\u001b[39m, in \u001b[36mThread.join\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1092\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1093\u001b[39m     timeout = \u001b[38;5;28mmax\u001b[39m(timeout, \u001b[32m0\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1095\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_handle\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ùå Error processing ‡∏ö‡∏≤‡∏á‡∏ö‡∏≥‡∏´‡∏£‡∏∏10: Server disconnected without sending a response.\n"
          ]
        }
      ],
      "source": [
        "# Example 1: Use defaults (3 models: flash, flash-lite, pro-preview at T=0.0)\n",
        "results = run_model_experiments(\n",
        "    sample_size=10,\n",
        "    jobs=2,\n",
        "    raise_errors=True\n",
        ")\n",
        "\n",
        "# Results include comparison and best performer\n",
        "print(f\"\\nüìä Summary:\")\n",
        "print(f\"   Total experiments: {results['total_experiments']}\")\n",
        "print(f\"   Successful: {results['successful_experiments']}\")\n",
        "print(f\"   Failed: {results['failed_experiments']}\")\n",
        "\n",
        "# Access comparison URL for side-by-side view\n",
        "if 'comparison_url' in results:\n",
        "    print(f\"\\nüîç Compare all experiments:\")\n",
        "    print(f\"   {results['comparison_url']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.2 Example 2: Custom Model Configurations\n",
        "\n",
        "Test specific models with different temperatures\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example 2: Custom model configurations with different temperatures\n",
        "results = run_model_experiments(\n",
        "    model_configs=[\n",
        "        {\n",
        "            \"model\": \"gemini-2.5-flash\",\n",
        "            \"temperature\": 0.0,\n",
        "            \"name_suffix\": \"flash-deterministic\",\n",
        "            \"metadata\": {\"purpose\": \"Production baseline\", \"cost_tier\": \"medium\"}\n",
        "        },\n",
        "        {\n",
        "            \"model\": \"gemini-2.5-flash\",\n",
        "            \"temperature\": 0.1,\n",
        "            \"name_suffix\": \"flash-tolerant\",\n",
        "            \"metadata\": {\"purpose\": \"Tolerance test\", \"cost_tier\": \"medium\"}\n",
        "        },\n",
        "        {\n",
        "            \"model\": \"gemini-2.5-flash\",\n",
        "            \"temperature\": 0.2,\n",
        "            \"name_suffix\": \"flash-varied\",\n",
        "            \"metadata\": {\"purpose\": \"Variation test\", \"cost_tier\": \"medium\"}\n",
        "        },\n",
        "        {\n",
        "            \"model\": \"gemini-2.5-flash-lite\",\n",
        "            \"temperature\": 0.0,\n",
        "            \"name_suffix\": \"lite-speed\",\n",
        "            \"metadata\": {\"purpose\": \"High-volume test\", \"cost_tier\": \"low\"}\n",
        "        }\n",
        "    ],\n",
        "    sample_size=10,\n",
        "    jobs=2,\n",
        "    raise_errors=False  # Continue even if one fails\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.3 Example 3: Full Configuration with Custom Settings\n",
        "\n",
        "Advanced usage with all configuration options\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "üîß INITIALIZING DATADOG LLMOBS\n",
            "================================================================================\n",
            "‚úÖ LLMObs enabled\n",
            "   App: vote-extractor-advanced\n",
            "   Site: datadoghq.com\n",
            "   Project: vote-extraction-project\n",
            "\n",
            "üìä Loading dataset: vote-extraction-bangbamru-1-10\n",
            "‚úÖ Dataset loaded: 11 records\n",
            "   Dataset ID: 241bfded-e79d-4d2d-bbc4-a74bb06d85f9\n",
            "\n",
            "================================================================================\n",
            "üöÄ RUNNING 6 EXPERIMENTS\n",
            "================================================================================\n",
            "   Sample Size: 10\n",
            "   Parallel Jobs: 5\n",
            "   Raise Errors: True\n",
            "\n",
            "\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "üß™ Experiment 1/6: gemini-2.5-flash (T=0.0)\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚úÖ Created: vote-extraction-flash-t0\n",
            "üìä View: https://app.datadoghq.com/llm/experiments/None\n",
            "‚è±Ô∏è  Running...\n",
            "Processing: ‡∏ö‡∏≤‡∏á‡∏ö‡∏≥‡∏´‡∏£‡∏∏4 (6 pages)Processing: ‡∏ö‡∏≤‡∏á‡∏ö‡∏≥‡∏´‡∏£‡∏∏1 (6 pages)\n",
            "\n",
            "Processing: ‡∏ö‡∏≤‡∏á‡∏ö‡∏≥‡∏´‡∏£‡∏∏5 (6 pages)\n",
            "Processing: ‡∏ö‡∏≤‡∏á‡∏ö‡∏≥‡∏´‡∏£‡∏∏2 (6 pages)\n",
            "Processing: ‡∏ö‡∏≤‡∏á‡∏ö‡∏≥‡∏´‡∏£‡∏∏7 (6 pages)\n",
            "Processing: ‡∏ö‡∏≤‡∏á‡∏ö‡∏≥‡∏´‡∏£‡∏∏3 (6 pages)\n",
            "Processing: ‡∏ö‡∏≤‡∏á‡∏ö‡∏≥‡∏´‡∏£‡∏∏4 (6 pages)Processing: ‡∏ö‡∏≤‡∏á‡∏ö‡∏≥‡∏´‡∏£‡∏∏9 (6 pages)\n",
            "\n",
            "Processing: ‡∏ö‡∏≤‡∏á‡∏ö‡∏≥‡∏´‡∏£‡∏∏6 (6 pages)\n",
            "Processing: ‡∏ö‡∏≤‡∏á‡∏ö‡∏≥‡∏´‡∏£‡∏∏10 (6 pages)\n",
            "‚úÖ Completed! Processed 10 records\n",
            "\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "üß™ Experiment 2/6: gemini-2.5-flash (T=0.1)\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚úÖ Created: vote-extraction-flash-t1\n",
            "üìä View: https://app.datadoghq.com/llm/experiments/None\n",
            "‚è±Ô∏è  Running...\n",
            "Processing: ‡∏ö‡∏≤‡∏á‡∏ö‡∏≥‡∏´‡∏£‡∏∏4 (6 pages)Processing: ‡∏ö‡∏≤‡∏á‡∏ö‡∏≥‡∏´‡∏£‡∏∏1 (6 pages)\n",
            "\n",
            "Processing: ‡∏ö‡∏≤‡∏á‡∏ö‡∏≥‡∏´‡∏£‡∏∏5 (6 pages)\n",
            "Processing: ‡∏ö‡∏≤‡∏á‡∏ö‡∏≥‡∏´‡∏£‡∏∏2 (6 pages)\n",
            "Processing: ‡∏ö‡∏≤‡∏á‡∏ö‡∏≥‡∏´‡∏£‡∏∏7 (6 pages)\n",
            "Processing: ‡∏ö‡∏≤‡∏á‡∏ö‡∏≥‡∏´‡∏£‡∏∏3 (6 pages)\n",
            "Processing: ‡∏ö‡∏≤‡∏á‡∏ö‡∏≥‡∏´‡∏£‡∏∏4 (6 pages)\n",
            "Processing: ‡∏ö‡∏≤‡∏á‡∏ö‡∏≥‡∏´‡∏£‡∏∏9 (6 pages)Processing: ‡∏ö‡∏≤‡∏á‡∏ö‡∏≥‡∏´‡∏£‡∏∏6 (6 pages)\n",
            "\n",
            "Processing: ‡∏ö‡∏≤‡∏á‡∏ö‡∏≥‡∏´‡∏£‡∏∏10 (6 pages)\n",
            "‚úÖ Completed! Processed 10 records\n",
            "\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "üß™ Experiment 3/6: gemini-2.5-flash-lite (T=0.0)\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚úÖ Created: vote-extraction-lite-t0\n",
            "üìä View: https://app.datadoghq.com/llm/experiments/None\n",
            "‚è±Ô∏è  Running...\n",
            "Processing: ‡∏ö‡∏≤‡∏á‡∏ö‡∏≥‡∏´‡∏£‡∏∏4 (6 pages)Processing: ‡∏ö‡∏≤‡∏á‡∏ö‡∏≥‡∏´‡∏£‡∏∏1 (6 pages)\n",
            "\n",
            "Processing: ‡∏ö‡∏≤‡∏á‡∏ö‡∏≥‡∏´‡∏£‡∏∏5 (6 pages)\n",
            "Processing: ‡∏ö‡∏≤‡∏á‡∏ö‡∏≥‡∏´‡∏£‡∏∏2 (6 pages)\n",
            "Processing: ‡∏ö‡∏≤‡∏á‡∏ö‡∏≥‡∏´‡∏£‡∏∏7 (6 pages)\n",
            "Processing: ‡∏ö‡∏≤‡∏á‡∏ö‡∏≥‡∏´‡∏£‡∏∏3 (6 pages)\n",
            "Processing: ‡∏ö‡∏≤‡∏á‡∏ö‡∏≥‡∏´‡∏£‡∏∏4 (6 pages)\n",
            "Processing: ‡∏ö‡∏≤‡∏á‡∏ö‡∏≥‡∏´‡∏£‡∏∏9 (6 pages)\n",
            "Processing: ‡∏ö‡∏≤‡∏á‡∏ö‡∏≥‡∏´‡∏£‡∏∏6 (6 pages)\n",
            "Processing: ‡∏ö‡∏≤‡∏á‡∏ö‡∏≥‡∏´‡∏£‡∏∏10 (6 pages)\n",
            "‚úÖ Completed! Processed 10 records\n",
            "\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "üß™ Experiment 4/6: gemini-2.5-flash-lite (T=0.1)\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚úÖ Created: vote-extraction-lite-t1\n",
            "üìä View: https://app.datadoghq.com/llm/experiments/None\n",
            "‚è±Ô∏è  Running...\n",
            "Processing: ‡∏ö‡∏≤‡∏á‡∏ö‡∏≥‡∏´‡∏£‡∏∏4 (6 pages)Processing: ‡∏ö‡∏≤‡∏á‡∏ö‡∏≥‡∏´‡∏£‡∏∏1 (6 pages)\n",
            "\n",
            "Processing: ‡∏ö‡∏≤‡∏á‡∏ö‡∏≥‡∏´‡∏£‡∏∏5 (6 pages)\n",
            "Processing: ‡∏ö‡∏≤‡∏á‡∏ö‡∏≥‡∏´‡∏£‡∏∏2 (6 pages)\n",
            "Processing: ‡∏ö‡∏≤‡∏á‡∏ö‡∏≥‡∏´‡∏£‡∏∏7 (6 pages)\n",
            "Processing: ‡∏ö‡∏≤‡∏á‡∏ö‡∏≥‡∏´‡∏£‡∏∏3 (6 pages)\n",
            "Processing: ‡∏ö‡∏≤‡∏á‡∏ö‡∏≥‡∏´‡∏£‡∏∏4 (6 pages)\n",
            "Processing: ‡∏ö‡∏≤‡∏á‡∏ö‡∏≥‡∏´‡∏£‡∏∏9 (6 pages)\n",
            "Processing: ‡∏ö‡∏≤‡∏á‡∏ö‡∏≥‡∏´‡∏£‡∏∏6 (6 pages)\n",
            "Processing: ‡∏ö‡∏≤‡∏á‡∏ö‡∏≥‡∏´‡∏£‡∏∏10 (6 pages)\n",
            "‚ùå Error processing ‡∏ö‡∏≤‡∏á‡∏ö‡∏≥‡∏´‡∏£‡∏∏3: timed out\n",
            "‚úÖ Completed! Processed 10 records\n",
            "\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "üß™ Experiment 5/6: gemini-3-pro-preview (T=0.0)\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚úÖ Created: vote-extraction-preview-t0\n",
            "üìä View: https://app.datadoghq.com/llm/experiments/None\n",
            "‚è±Ô∏è  Running...\n",
            "Processing: ‡∏ö‡∏≤‡∏á‡∏ö‡∏≥‡∏´‡∏£‡∏∏4 (6 pages)Processing: ‡∏ö‡∏≤‡∏á‡∏ö‡∏≥‡∏´‡∏£‡∏∏1 (6 pages)\n",
            "\n",
            "Processing: ‡∏ö‡∏≤‡∏á‡∏ö‡∏≥‡∏´‡∏£‡∏∏5 (6 pages)\n",
            "Processing: ‡∏ö‡∏≤‡∏á‡∏ö‡∏≥‡∏´‡∏£‡∏∏2 (6 pages)\n",
            "Processing: ‡∏ö‡∏≤‡∏á‡∏ö‡∏≥‡∏´‡∏£‡∏∏7 (6 pages)\n",
            "Processing: ‡∏ö‡∏≤‡∏á‡∏ö‡∏≥‡∏´‡∏£‡∏∏3 (6 pages)\n",
            "Processing: ‡∏ö‡∏≤‡∏á‡∏ö‡∏≥‡∏´‡∏£‡∏∏4 (6 pages)\n",
            "Processing: ‡∏ö‡∏≤‡∏á‡∏ö‡∏≥‡∏´‡∏£‡∏∏9 (6 pages)\n",
            "Processing: ‡∏ö‡∏≤‡∏á‡∏ö‡∏≥‡∏´‡∏£‡∏∏6 (6 pages)\n",
            "Processing: ‡∏ö‡∏≤‡∏á‡∏ö‡∏≥‡∏´‡∏£‡∏∏10 (6 pages)\n",
            "‚úÖ Completed! Processed 10 records\n",
            "\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "üß™ Experiment 6/6: gemini-3-pro-preview (T=0.1)\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "‚úÖ Created: vote-extraction-preview-t1\n",
            "üìä View: https://app.datadoghq.com/llm/experiments/None\n",
            "‚è±Ô∏è  Running...\n",
            "Processing: ‡∏ö‡∏≤‡∏á‡∏ö‡∏≥‡∏´‡∏£‡∏∏4 (6 pages)Processing: ‡∏ö‡∏≤‡∏á‡∏ö‡∏≥‡∏´‡∏£‡∏∏1 (6 pages)\n",
            "\n",
            "Processing: ‡∏ö‡∏≤‡∏á‡∏ö‡∏≥‡∏´‡∏£‡∏∏5 (6 pages)\n",
            "Processing: ‡∏ö‡∏≤‡∏á‡∏ö‡∏≥‡∏´‡∏£‡∏∏2 (6 pages)\n",
            "Processing: ‡∏ö‡∏≤‡∏á‡∏ö‡∏≥‡∏´‡∏£‡∏∏7 (6 pages)\n",
            "Processing: ‡∏ö‡∏≤‡∏á‡∏ö‡∏≥‡∏´‡∏£‡∏∏3 (6 pages)\n",
            "Processing: ‡∏ö‡∏≤‡∏á‡∏ö‡∏≥‡∏´‡∏£‡∏∏4 (6 pages)\n",
            "‚ùå Error processing ‡∏ö‡∏≤‡∏á‡∏ö‡∏≥‡∏´‡∏£‡∏∏3: [Errno 54] Connection reset by peer\n",
            "Processing: ‡∏ö‡∏≤‡∏á‡∏ö‡∏≥‡∏´‡∏£‡∏∏9 (6 pages)\n",
            "Processing: ‡∏ö‡∏≤‡∏á‡∏ö‡∏≥‡∏´‡∏£‡∏∏6 (6 pages)\n",
            "Processing: ‡∏ö‡∏≤‡∏á‡∏ö‡∏≥‡∏´‡∏£‡∏∏10 (6 pages)\n",
            "‚úÖ Completed! Processed 10 records\n",
            "\n",
            "========================================================================================================================\n",
            "üìä EXPERIMENT COMPARISON\n",
            "========================================================================================================================\n",
            "\n",
            "                Experiment                 Model  Temperature  Status  Records Overall Accuracy Success Rate Avg Ballot Accuracy\n",
            "  vote-extraction-flash-t0      gemini-2.5-flash          0.0 success       10              N/A          N/A                 N/A\n",
            "  vote-extraction-flash-t1      gemini-2.5-flash          0.1 success       10              N/A          N/A                 N/A\n",
            "   vote-extraction-lite-t0 gemini-2.5-flash-lite          0.0 success       10              N/A          N/A                 N/A\n",
            "   vote-extraction-lite-t1 gemini-2.5-flash-lite          0.1 success       10              N/A          N/A                 N/A\n",
            "vote-extraction-preview-t0  gemini-3-pro-preview          0.0 success       10              N/A          N/A                 N/A\n",
            "vote-extraction-preview-t1  gemini-3-pro-preview          0.1 success       10              N/A          N/A                 N/A\n",
            "\n",
            "========================================================================================================================\n",
            "üîó View in Datadog:\n",
            "   ‚úÖ vote-extraction-flash-t0: https://app.datadoghq.com/llm/experiments/52696a07-9d54-423b-8590-e21cc3e4fc88\n",
            "   ‚úÖ vote-extraction-flash-t1: https://app.datadoghq.com/llm/experiments/59491806-eed3-4cdc-a15e-e4882958c206\n",
            "   ‚úÖ vote-extraction-lite-t0: https://app.datadoghq.com/llm/experiments/b3393c08-8d5b-4fae-bcf0-8a70391dc97c\n",
            "   ‚úÖ vote-extraction-lite-t1: https://app.datadoghq.com/llm/experiments/248ed8de-af47-4c1a-93a7-7fc1daf26022\n",
            "   ‚úÖ vote-extraction-preview-t0: https://app.datadoghq.com/llm/experiments/3aebfee6-8e01-4acb-a617-acdfca1d7b30\n",
            "   ‚úÖ vote-extraction-preview-t1: https://app.datadoghq.com/llm/experiments/fbe184f1-33d2-4aff-844f-42c7fbafe6e2\n",
            "========================================================================================================================\n",
            "\n",
            "üîç Compare all experiments side-by-side:\n",
            "   https://app.datadoghq.com/llm/experiments?dataset=241bfded-e79d-4d2d-bbc4-a74bb06d85f9&project=vote-extraction-project\n",
            "========================================================================================================================\n",
            "\n",
            "\n",
            "================================================================================\n",
            "üìä DETAILED RESULTS\n",
            "================================================================================\n",
            "\n",
            "üß™ vote-extraction-flash-t0:\n",
            "   Status: success\n",
            "   Model: gemini-2.5-flash\n",
            "   Temperature: 0.0\n",
            "   Records: 10\n",
            "   URL: https://app.datadoghq.com/llm/experiments/52696a07-9d54-423b-8590-e21cc3e4fc88\n",
            "\n",
            "üß™ vote-extraction-flash-t1:\n",
            "   Status: success\n",
            "   Model: gemini-2.5-flash\n",
            "   Temperature: 0.1\n",
            "   Records: 10\n",
            "   URL: https://app.datadoghq.com/llm/experiments/59491806-eed3-4cdc-a15e-e4882958c206\n",
            "\n",
            "üß™ vote-extraction-lite-t0:\n",
            "   Status: success\n",
            "   Model: gemini-2.5-flash-lite\n",
            "   Temperature: 0.0\n",
            "   Records: 10\n",
            "   URL: https://app.datadoghq.com/llm/experiments/b3393c08-8d5b-4fae-bcf0-8a70391dc97c\n",
            "\n",
            "üß™ vote-extraction-lite-t1:\n",
            "   Status: success\n",
            "   Model: gemini-2.5-flash-lite\n",
            "   Temperature: 0.1\n",
            "   Records: 10\n",
            "   URL: https://app.datadoghq.com/llm/experiments/248ed8de-af47-4c1a-93a7-7fc1daf26022\n",
            "\n",
            "üß™ vote-extraction-preview-t0:\n",
            "   Status: success\n",
            "   Model: gemini-3-pro-preview\n",
            "   Temperature: 0.0\n",
            "   Records: 10\n",
            "   URL: https://app.datadoghq.com/llm/experiments/3aebfee6-8e01-4acb-a617-acdfca1d7b30\n",
            "\n",
            "üß™ vote-extraction-preview-t1:\n",
            "   Status: success\n",
            "   Model: gemini-3-pro-preview\n",
            "   Temperature: 0.1\n",
            "   Records: 10\n",
            "   URL: https://app.datadoghq.com/llm/experiments/fbe184f1-33d2-4aff-844f-42c7fbafe6e2\n"
          ]
        }
      ],
      "source": [
        "# Example 3: Full configuration with all options\n",
        "results = run_model_experiments(\n",
        "    # LLMObs configuration\n",
        "    ml_app=\"vote-extractor-advanced\",\n",
        "    api_key=os.getenv(\"DD_API_KEY\"),  # Or pass directly\n",
        "    site=\"datadoghq.com\",\n",
        "    agentless_enabled=True,\n",
        "    project_name=\"vote-extraction-project\",\n",
        "    \n",
        "    # Dataset configuration\n",
        "    dataset_name=\"vote-extraction-bangbamru-1-10\",\n",
        "    dataset_version=None,  # Latest version\n",
        "    \n",
        "    # Models to test\n",
        "    model_configs=[\n",
        "        {\"model\": \"gemini-2.5-flash\", \"temperature\": 0.0},\n",
        "        {\"model\": \"gemini-2.5-flash\", \"temperature\": 0.1},\n",
        "        {\"model\": \"gemini-2.5-flash-lite\", \"temperature\": 0.0},\n",
        "        {\"model\": \"gemini-2.5-flash-lite\", \"temperature\": 0.1},\n",
        "        {\"model\": \"gemini-3-pro-preview\", \"temperature\": 0.0},\n",
        "        {\"model\": \"gemini-3-pro-preview\", \"temperature\": 0.1},\n",
        "    ],\n",
        "    \n",
        "    # Task and evaluators (uses defaults if not specified)\n",
        "    task_function=vote_extraction_task,\n",
        "    evaluators=[exact_form_match, ballot_accuracy_score, vote_results_quality, has_no_errors],\n",
        "    summary_evaluators=[overall_accuracy, success_rate, avg_ballot_accuracy],\n",
        "    \n",
        "    # Run configuration\n",
        "    sample_size=10,  # Test all 10 records\n",
        "    jobs=2,          # Parallel processing\n",
        "    raise_errors=True,  # Stop on first error\n",
        "    \n",
        "    # Display options\n",
        "    show_comparison=True,\n",
        "    return_results=True\n",
        ")\n",
        "\n",
        "# Access detailed results\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"üìä DETAILED RESULTS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for exp in results['experiments']:\n",
        "    print(f\"\\nüß™ {exp['experiment_name']}:\")\n",
        "    print(f\"   Status: {exp['status']}\")\n",
        "    print(f\"   Model: {exp['model']}\")\n",
        "    print(f\"   Temperature: {exp['temperature']}\")\n",
        "    print(f\"   Records: {exp['sample_size']}\")\n",
        "    \n",
        "    if exp['summary_metrics']:\n",
        "        print(f\"   Metrics:\")\n",
        "        for key, value in exp['summary_metrics'].items():\n",
        "            print(f\"     - {key}: {value}\")\n",
        "    \n",
        "    print(f\"   URL: {exp['url']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üí° Tips & Next Steps\n",
        "\n",
        "**Experiment Optimization**:\n",
        "- Use `sample_size` for fast iteration during development\n",
        "- Enable `jobs=4` for parallel processing on large datasets\n",
        "- Set `raise_errors=True` to catch and debug failures early\n",
        "\n",
        "**Comparing Configurations**:\n",
        "```python\n",
        "# Run multiple experiments with different configs\n",
        "experiment_v1 = LLMObs.experiment(\n",
        "    name=\"vote-extraction-v1\",\n",
        "    task=task,\n",
        "    dataset=dataset,\n",
        "    config={\"model\": \"gemini-2.5-flash\", \"temperature\": 0.0}\n",
        ")\n",
        "\n",
        "experiment_v2 = LLMObs.experiment(\n",
        "    name=\"vote-extraction-v2\",\n",
        "    task=task,\n",
        "    dataset=dataset,\n",
        "    config={\"model\": \"gemini-2.5-pro\", \"temperature\": 0.1}\n",
        ")\n",
        "\n",
        "results_v1 = experiment_v1.run()\n",
        "results_v2 = experiment_v2.run()\n",
        "\n",
        "# Compare side-by-side in Datadog UI\n",
        "```\n",
        "\n",
        "**Using Pandas for Analysis**:\n",
        "```python\n",
        "# Export dataset to DataFrame for advanced analysis\n",
        "df = experiment_dataset.as_dataframe()\n",
        "print(df.head())\n",
        "```\n",
        "\n",
        "**Next Steps**:\n",
        "1. ‚úÖ Create ground truth datasets (Streamlit Dataset Manager)\n",
        "2. ‚úÖ Push datasets to Datadog\n",
        "3. ‚úÖ Run experiments to establish baselines\n",
        "4. üîÑ Iterate on prompts and models\n",
        "5. üìä Compare results in Datadog\n",
        "6. üöÄ Deploy best performing configuration to production\n",
        "\n",
        "**Resources**:\n",
        "- [Datadog LLMObs Experiments Documentation](https://docs.datadoghq.com/llm_observability/experiments/)\n",
        "- [Guide 04: Experiments and Datasets](../../guides/llmobs/04_EXPERIMENTS_AND_DATASETS.md)\n",
        "- [Evaluation Metric Types Guide](../../guides/llmobs/03_EVALUATION_METRIC_TYPES.md)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
