{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thai Election Form Extractor - BigQuery + Google Drive Edition\n",
    "\n",
    "Extract structured data from election form PDFs stored in **Google Drive** using **BigQuery** to find files.\n",
    "\n",
    "**Key Advantages:**\n",
    "- üîç Query BigQuery to find PDF files\n",
    "- üìÅ Direct access to Google Drive files (no download needed!)\n",
    "- üöÄ Uses Gemini's External URLs file input method\n",
    "- ü§ñ Structured output with Pydantic schema validation\n",
    "\n",
    "**Reference:** [Gemini File Input Methods](https://ai.google.dev/gemini-api/docs/file-input-methods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install required packages (run once)\n",
    "!source ../.env\n",
    "!pip install -q google-cloud-bigquery google-genai pydantic pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from typing import Optional\n",
    "\n",
    "from google.cloud import bigquery\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from pydantic import BaseModel, Field\n",
    "from IPython.display import display, HTML\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Environment variables set!\n",
      "‚úÖ Configuration\n",
      "   Project: datadog-ese-sandbox\n",
      "   API Key: ********************jezISbnw\n",
      "   Model: gemini-3-flash-preview\n",
      "   BigQuery Table: sourceinth.vote69_ect.raw_files\n"
     ]
    }
   ],
   "source": [
    "# Google Cloud Configuration\n",
    "\n",
    "# Set Gemini API Key (hidden input)\n",
    "if 'GEMINI_API_KEY' not in os.environ:\n",
    "    print(\"‚ö†Ô∏è  GEMINI_API_KEY not found in environment variables.\")\n",
    "    os.environ['GEMINI_API_KEY'] = 'AIzaSyC...' #@param {type:\"string\"}\n",
    "\n",
    "GEMINI_API_KEY = os.environ['GEMINI_API_KEY']\n",
    "\n",
    "# Set Google Cloud Project ID\n",
    "if 'GOOGLE_CLOUD_PROJECT' not in os.environ:\n",
    "    print(\"‚ö†Ô∏è  GOOGLE_CLOUD_PROJECT not found in environment variables.\")\n",
    "    os.environ['GOOGLE_CLOUD_PROJECT'] = 'YOUR_GCP_PROJECT_ID' #@param {type:\"string\"}\n",
    "\n",
    "GOOGLE_CLOUD_PROJECT = os.environ['GOOGLE_CLOUD_PROJECT']\n",
    "\n",
    "print(\"‚úÖ Environment variables set!\")\n",
    "\n",
    "# Model Configuration\n",
    "MODEL_NAME = \"gemini-3-flash-preview\"  # or \"gemini-3-pro-preview\", \"gemini-2.5-flash\"\n",
    "\n",
    "# BigQuery Configuration\n",
    "BQ_TABLE = \"sourceinth.vote69_ect.raw_files\"\n",
    "\n",
    "# Warn if API key is not set\n",
    "if not GEMINI_API_KEY:\n",
    "    print(\"\\n‚ö†Ô∏è  WARNING: GEMINI_API_KEY is not set!\")\n",
    "    print(\"   Set it with: export GEMINI_API_KEY='your-key-here'\")\n",
    "    print(\"   Or create a .env file with GEMINI_API_KEY=your-key-here\")\n",
    "\n",
    "# Verify configuration\n",
    "print(f\"‚úÖ Configuration\")\n",
    "print(f\"   Project: {GOOGLE_CLOUD_PROJECT}\")\n",
    "print(f\"   API Key: {'*' * 20 + GEMINI_API_KEY[-8:] if GEMINI_API_KEY and len(GEMINI_API_KEY) > 8 else 'NOT SET ‚ö†Ô∏è'}\")\n",
    "print(f\"   Model: {MODEL_NAME}\")\n",
    "print(f\"   BigQuery Table: {BQ_TABLE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Pydantic Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumberTextPair(BaseModel):\n",
    "    \"\"\"Thai document number representation (both Arabic numeral and Thai text).\"\"\"\n",
    "    arabic: int = Field(..., description=\"Arabic numeral (e.g., 120)\")\n",
    "    thai_text: Optional[str] = Field(None, description=\"Thai text (e.g., '‡∏´‡∏ô‡∏∂‡πà‡∏á‡∏£‡πâ‡∏≠‡∏¢‡∏¢‡∏µ‡πà‡∏™‡∏¥‡∏ö')\")\n",
    "\n",
    "\n",
    "class FormInfo(BaseModel):\n",
    "    \"\"\"Header information identifying the polling station.\"\"\"\n",
    "    form_type: Optional[str] = Field(None, description=\"Constituency or PartyList\")\n",
    "    set_number: Optional[str] = Field(None, description=\"Set number (‡∏ä‡∏∏‡∏î‡∏ó‡∏µ‡πà)\")  # NEW\n",
    "    date: Optional[str] = Field(None, description=\"Date of election\")\n",
    "    province: Optional[str] = Field(None, description=\"Province name\")\n",
    "    constituency_number: Optional[str] = Field(None, description=\"Constituency number\")\n",
    "    district: str = Field(..., description=\"District name\")\n",
    "    sub_district: Optional[str] = Field(None, description=\"Sub-district name\")\n",
    "    polling_station_number: str = Field(..., description=\"Polling station number\")\n",
    "    village_moo: Optional[str] = Field(None, description=\"Village number (‡∏´‡∏°‡∏π‡πà‡∏ó‡∏µ‡πà)\")  # NEW\n",
    "\n",
    "\n",
    "class VoterStatistics(BaseModel):\n",
    "    \"\"\"Voter statistics (Section 1).\"\"\"\n",
    "    eligible_voters: Optional[NumberTextPair] = Field(None, description=\"Total eligible voters\")\n",
    "    present_voters: Optional[NumberTextPair] = Field(None, description=\"Voters who showed up\")\n",
    "\n",
    "\n",
    "class BallotStatistics(BaseModel):\n",
    "    \"\"\"Ballot accounting statistics (Section 2).\"\"\"\n",
    "    ballots_allocated: Optional[NumberTextPair] = Field(None, description=\"Allocated ballots\")\n",
    "    ballots_used: Optional[NumberTextPair] = Field(None, description=\"Used ballots\")\n",
    "    good_ballots: Optional[NumberTextPair] = Field(None, description=\"Valid ballots\")\n",
    "    bad_ballots: Optional[NumberTextPair] = Field(None, description=\"Invalid ballots\")\n",
    "    no_vote_ballots: Optional[NumberTextPair] = Field(None, description=\"No vote ballots\")\n",
    "    ballots_remaining: Optional[NumberTextPair] = Field(None, description=\"Remaining ballots\")\n",
    "\n",
    "\n",
    "class VoteResult(BaseModel):\n",
    "    \"\"\"Individual vote result.\"\"\"\n",
    "    number: int = Field(..., description=\"Candidate/Party number\")\n",
    "    candidate_name: Optional[str] = Field(None, description=\"Candidate name (Constituency only)\")\n",
    "    party_name: Optional[str] = Field(None, description=\"Party name\")\n",
    "    vote_count: NumberTextPair = Field(..., description=\"Vote count (number + text)\")\n",
    "\n",
    "\n",
    "class Official(BaseModel):\n",
    "    \"\"\"Committee member/official.\"\"\"\n",
    "    name: str = Field(..., description=\"Full name of official\")\n",
    "    position: str = Field(..., description=\"Position/role (e.g., ‡∏õ‡∏£‡∏∞‡∏ò‡∏≤‡∏ô, ‡∏Å‡∏£‡∏£‡∏°‡∏Å‡∏≤‡∏£)\")\n",
    "\n",
    "\n",
    "class ElectionFormData(BaseModel):\n",
    "    \"\"\"Complete election form extraction result.\"\"\"\n",
    "    form_info: FormInfo\n",
    "    voter_statistics: Optional[VoterStatistics] = None\n",
    "    ballot_statistics: Optional[BallotStatistics] = None\n",
    "    vote_results: list[VoteResult] = Field(default_factory=list)\n",
    "    total_votes_recorded: Optional[NumberTextPair] = Field(\n",
    "        None, \n",
    "        description=\"Total vote count from table footer\"\n",
    "    )  # NEW\n",
    "    officials: Optional[list[Official]] = Field(\n",
    "        None,\n",
    "        description=\"Committee members who signed the form\"\n",
    "    )  # NEW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Gemini Schema for Structured Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced schema for Gemini structured output with NumberTextPair\n",
    "ELECTION_DATA_SCHEMA = {\n",
    "    \"type\": \"ARRAY\",\n",
    "    \"description\": \"List of election reports found in the PDF\",\n",
    "    \"items\": {\n",
    "        \"type\": \"OBJECT\",\n",
    "        \"properties\": {\n",
    "            \"form_info\": {\n",
    "                \"type\": \"OBJECT\",\n",
    "                \"description\": \"Header information\",\n",
    "                \"properties\": {\n",
    "                    \"form_type\": {\n",
    "                        \"type\": \"STRING\",\n",
    "                        \"enum\": [\"Constituency\", \"PartyList\"],\n",
    "                        \"description\": \"Form type: Constituency (candidates) or PartyList (parties only)\"\n",
    "                    },\n",
    "                    \"set_number\": {\"type\": \"STRING\", \"description\": \"Set number (‡∏ä‡∏∏‡∏î‡∏ó‡∏µ‡πà)\"},\n",
    "                    \"date\": {\"type\": \"STRING\", \"description\": \"Date of election\"},\n",
    "                    \"province\": {\"type\": \"STRING\", \"description\": \"Province name\"},\n",
    "                    \"constituency_number\": {\"type\": \"STRING\", \"description\": \"Constituency number\"},\n",
    "                    \"district\": {\"type\": \"STRING\", \"description\": \"District name\"},\n",
    "                    \"sub_district\": {\"type\": \"STRING\", \"description\": \"Sub-district name\"},\n",
    "                    \"polling_station_number\": {\"type\": \"STRING\", \"description\": \"Polling station number\"},\n",
    "                    \"village_moo\": {\"type\": \"STRING\", \"description\": \"Village number (‡∏´‡∏°‡∏π‡πà‡∏ó‡∏µ‡πà)\"},\n",
    "                },\n",
    "                \"required\": [\"form_type\", \"province\", \"district\", \"polling_station_number\"],\n",
    "            },\n",
    "            \"voter_statistics\": {\n",
    "                \"type\": \"OBJECT\",\n",
    "                \"description\": \"Section 1: Voter statistics\",\n",
    "                \"properties\": {\n",
    "                    \"eligible_voters\": {\n",
    "                        \"type\": \"OBJECT\",\n",
    "                        \"description\": \"1.1 Total eligible voters\",\n",
    "                        \"properties\": {\n",
    "                            \"arabic\": {\"type\": \"INTEGER\"},\n",
    "                            \"thai_text\": {\"type\": \"STRING\"}\n",
    "                        }\n",
    "                    },\n",
    "                    \"present_voters\": {\n",
    "                        \"type\": \"OBJECT\",\n",
    "                        \"description\": \"1.2 Voters who showed up\",\n",
    "                        \"properties\": {\n",
    "                            \"arabic\": {\"type\": \"INTEGER\"},\n",
    "                            \"thai_text\": {\"type\": \"STRING\"}\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"ballot_statistics\": {\n",
    "                \"type\": \"OBJECT\",\n",
    "                \"description\": \"Section 2: Ballot accounting\",\n",
    "                \"properties\": {\n",
    "                    \"ballots_allocated\": {\n",
    "                        \"type\": \"OBJECT\",\n",
    "                        \"description\": \"2.1 Allocated ballots\",\n",
    "                        \"properties\": {\n",
    "                            \"arabic\": {\"type\": \"INTEGER\"},\n",
    "                            \"thai_text\": {\"type\": \"STRING\"}\n",
    "                        }\n",
    "                    },\n",
    "                    \"ballots_used\": {\n",
    "                        \"type\": \"OBJECT\",\n",
    "                        \"description\": \"2.2 Used ballots\",\n",
    "                        \"properties\": {\n",
    "                            \"arabic\": {\"type\": \"INTEGER\"},\n",
    "                            \"thai_text\": {\"type\": \"STRING\"}\n",
    "                        },\n",
    "                        \"required\": [\"arabic\"]\n",
    "                    },\n",
    "                    \"good_ballots\": {\n",
    "                        \"type\": \"OBJECT\",\n",
    "                        \"description\": \"2.2.1 Valid ballots\",\n",
    "                        \"properties\": {\n",
    "                            \"arabic\": {\"type\": \"INTEGER\"},\n",
    "                            \"thai_text\": {\"type\": \"STRING\"}\n",
    "                        },\n",
    "                        \"required\": [\"arabic\"]\n",
    "                    },\n",
    "                    \"bad_ballots\": {\n",
    "                        \"type\": \"OBJECT\",\n",
    "                        \"description\": \"2.2.2 Invalid ballots\",\n",
    "                        \"properties\": {\n",
    "                            \"arabic\": {\"type\": \"INTEGER\"},\n",
    "                            \"thai_text\": {\"type\": \"STRING\"}\n",
    "                        },\n",
    "                        \"required\": [\"arabic\"]\n",
    "                    },\n",
    "                    \"no_vote_ballots\": {\n",
    "                        \"type\": \"OBJECT\",\n",
    "                        \"description\": \"2.2.3 No vote ballots\",\n",
    "                        \"properties\": {\n",
    "                            \"arabic\": {\"type\": \"INTEGER\"},\n",
    "                            \"thai_text\": {\"type\": \"STRING\"}\n",
    "                        },\n",
    "                        \"required\": [\"arabic\"]\n",
    "                    },\n",
    "                    \"ballots_remaining\": {\n",
    "                        \"type\": \"OBJECT\",\n",
    "                        \"description\": \"2.3 Remaining ballots\",\n",
    "                        \"properties\": {\n",
    "                            \"arabic\": {\"type\": \"INTEGER\"},\n",
    "                            \"thai_text\": {\"type\": \"STRING\"}\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"vote_results\": {\n",
    "                \"type\": \"ARRAY\",\n",
    "                \"description\": \"Section 3: Vote counts for all candidates/parties\",\n",
    "                \"items\": {\n",
    "                    \"type\": \"OBJECT\",\n",
    "                    \"properties\": {\n",
    "                        \"number\": {\"type\": \"INTEGER\", \"description\": \"Candidate/Party number\"},\n",
    "                        \"candidate_name\": {\n",
    "                            \"type\": \"STRING\",\n",
    "                            \"description\": \"Candidate name (for Constituency forms only)\"\n",
    "                        },\n",
    "                        \"party_name\": {\"type\": \"STRING\", \"description\": \"Party name\"},\n",
    "                        \"vote_count\": {\n",
    "                            \"type\": \"OBJECT\",\n",
    "                            \"description\": \"Vote count (both number and Thai text)\",\n",
    "                            \"properties\": {\n",
    "                                \"arabic\": {\"type\": \"INTEGER\"},\n",
    "                                \"thai_text\": {\"type\": \"STRING\"}\n",
    "                            },\n",
    "                            \"required\": [\"arabic\"]\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"number\", \"vote_count\"]\n",
    "                },\n",
    "            },\n",
    "            \"total_votes_recorded\": {\n",
    "                \"type\": \"OBJECT\",\n",
    "                \"description\": \"Total vote count from bottom of table (for validation)\",\n",
    "                \"properties\": {\n",
    "                    \"arabic\": {\"type\": \"INTEGER\"},\n",
    "                    \"thai_text\": {\"type\": \"STRING\"}\n",
    "                }\n",
    "            },\n",
    "            \"officials\": {\n",
    "                \"type\": \"ARRAY\",\n",
    "                \"description\": \"Committee members who signed the form\",\n",
    "                \"items\": {\n",
    "                    \"type\": \"OBJECT\",\n",
    "                    \"properties\": {\n",
    "                        \"name\": {\"type\": \"STRING\", \"description\": \"Full name\"},\n",
    "                        \"position\": {\"type\": \"STRING\", \"description\": \"Position (‡∏õ‡∏£‡∏∞‡∏ò‡∏≤‡∏ô, ‡∏Å‡∏£‡∏£‡∏°‡∏Å‡∏≤‡∏£, etc.)\"}\n",
    "                    },\n",
    "                    \"required\": [\"name\", \"position\"]\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"form_info\", \"vote_results\"],\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Initialize Clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ BigQuery client initialized\n",
      "‚úÖ Gemini client initialized (using API key)\n",
      "   Ready to use gemini-3-flash-preview\n"
     ]
    }
   ],
   "source": [
    "# Initialize BigQuery client\n",
    "bq_client = bigquery.Client(project=GOOGLE_CLOUD_PROJECT)\n",
    "print(\"‚úÖ BigQuery client initialized\")\n",
    "\n",
    "# Initialize Gemini client with API key\n",
    "if not GEMINI_API_KEY:\n",
    "    raise ValueError(\n",
    "        \"GEMINI_API_KEY is required! Set it with:\\n\"\n",
    "        \"  export GEMINI_API_KEY='your-key-here'\\n\"\n",
    "        \"  or create a .env file\"\n",
    "    )\n",
    "\n",
    "gemini_client = genai.Client(\n",
    "    api_key=GEMINI_API_KEY,\n",
    "    vertexai=False,\n",
    ")\n",
    "print(\"‚úÖ Gemini client initialized (using API key)\")\n",
    "print(f\"   Ready to use {MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Query BigQuery for PDF Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Querying BigQuery...\n",
      "   Filters: mime_type = 'application/pdf' AND size >= 51200 AND size <= 52428800\n",
      "‚úÖ Found 10 file(s)\n",
      "   Size range: 50.0 KB - 50.4 KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>province_name</th>\n",
       "      <th>path</th>\n",
       "      <th>size_mb</th>\n",
       "      <th>file_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>‡∏û‡∏¥‡∏à‡∏¥‡∏ï‡∏£</td>\n",
       "      <td>‡πÄ‡∏Ç‡∏ï‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ï‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà 3/‡∏≠‡∏≥‡πÄ‡∏†‡∏≠‡πÇ‡∏û‡∏ò‡∏¥‡πå‡∏õ‡∏£‡∏∞‡∏ó‡∏±‡∏ö‡∏ä‡πâ‡∏≤‡∏á/‡∏ó‡∏ï.‡πÇ‡∏û‡∏ò‡∏¥...</td>\n",
       "      <td>0.048851</td>\n",
       "      <td>1_j0DNaqCXIkEk0MK3y0J1eCN3hUOCXeF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>‡∏û‡∏¥‡∏à‡∏¥‡∏ï‡∏£</td>\n",
       "      <td>‡πÄ‡∏Ç‡∏ï‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ï‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà 3/‡∏≠‡∏≥‡πÄ‡∏†‡∏≠‡πÇ‡∏û‡∏ò‡∏¥‡πå‡∏õ‡∏£‡∏∞‡∏ó‡∏±‡∏ö‡∏ä‡πâ‡∏≤‡∏á/‡∏ï‡∏≥‡∏ö‡∏•‡πÑ‡∏ú‡πà...</td>\n",
       "      <td>0.048915</td>\n",
       "      <td>1gDxp58u2W14uhdb6NpRDqxl1d7aa2WFy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>‡∏û‡∏¥‡∏à‡∏¥‡∏ï‡∏£</td>\n",
       "      <td>‡πÄ‡∏Ç‡∏ï‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ï‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà 3/‡∏≠‡∏≥‡πÄ‡∏†‡∏≠‡πÇ‡∏û‡∏ò‡∏¥‡πå‡∏õ‡∏£‡∏∞‡∏ó‡∏±‡∏ö‡∏ä‡πâ‡∏≤‡∏á/‡∏ï‡∏≥‡∏ö‡∏•‡πÑ‡∏ú‡πà...</td>\n",
       "      <td>0.048917</td>\n",
       "      <td>1a5jF1Oyv3UEatBq1MT1ga8kS10uR19-c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>‡∏û‡∏¥‡∏à‡∏¥‡∏ï‡∏£</td>\n",
       "      <td>‡πÄ‡∏Ç‡∏ï‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ï‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà 3/‡∏≠‡∏≥‡πÄ‡∏†‡∏≠‡πÇ‡∏û‡∏ò‡∏¥‡πå‡∏õ‡∏£‡∏∞‡∏ó‡∏±‡∏ö‡∏ä‡πâ‡∏≤‡∏á/‡∏ï‡∏≥‡∏ö‡∏•‡∏î‡∏á‡πÄ...</td>\n",
       "      <td>0.048927</td>\n",
       "      <td>1tzz6gMXk1n3pQtreQWMIU2xlkncg-g_r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>‡∏û‡∏¥‡∏à‡∏¥‡∏ï‡∏£</td>\n",
       "      <td>‡πÄ‡∏Ç‡∏ï‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ï‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà 3/‡∏≠‡∏≥‡πÄ‡∏†‡∏≠‡πÇ‡∏û‡∏ò‡∏¥‡πå‡∏õ‡∏£‡∏∞‡∏ó‡∏±‡∏ö‡∏ä‡πâ‡∏≤‡∏á/‡∏ó‡∏ï.‡πÇ‡∏û‡∏ò‡∏¥...</td>\n",
       "      <td>0.048993</td>\n",
       "      <td>1-MsML3nSXUrscvmzdZb7R4yTkZcuTc5m</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  province_name                                               path   size_mb  \\\n",
       "0        ‡∏û‡∏¥‡∏à‡∏¥‡∏ï‡∏£  ‡πÄ‡∏Ç‡∏ï‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ï‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà 3/‡∏≠‡∏≥‡πÄ‡∏†‡∏≠‡πÇ‡∏û‡∏ò‡∏¥‡πå‡∏õ‡∏£‡∏∞‡∏ó‡∏±‡∏ö‡∏ä‡πâ‡∏≤‡∏á/‡∏ó‡∏ï.‡πÇ‡∏û‡∏ò‡∏¥...  0.048851   \n",
       "1        ‡∏û‡∏¥‡∏à‡∏¥‡∏ï‡∏£  ‡πÄ‡∏Ç‡∏ï‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ï‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà 3/‡∏≠‡∏≥‡πÄ‡∏†‡∏≠‡πÇ‡∏û‡∏ò‡∏¥‡πå‡∏õ‡∏£‡∏∞‡∏ó‡∏±‡∏ö‡∏ä‡πâ‡∏≤‡∏á/‡∏ï‡∏≥‡∏ö‡∏•‡πÑ‡∏ú‡πà...  0.048915   \n",
       "2        ‡∏û‡∏¥‡∏à‡∏¥‡∏ï‡∏£  ‡πÄ‡∏Ç‡∏ï‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ï‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà 3/‡∏≠‡∏≥‡πÄ‡∏†‡∏≠‡πÇ‡∏û‡∏ò‡∏¥‡πå‡∏õ‡∏£‡∏∞‡∏ó‡∏±‡∏ö‡∏ä‡πâ‡∏≤‡∏á/‡∏ï‡∏≥‡∏ö‡∏•‡πÑ‡∏ú‡πà...  0.048917   \n",
       "3        ‡∏û‡∏¥‡∏à‡∏¥‡∏ï‡∏£  ‡πÄ‡∏Ç‡∏ï‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ï‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà 3/‡∏≠‡∏≥‡πÄ‡∏†‡∏≠‡πÇ‡∏û‡∏ò‡∏¥‡πå‡∏õ‡∏£‡∏∞‡∏ó‡∏±‡∏ö‡∏ä‡πâ‡∏≤‡∏á/‡∏ï‡∏≥‡∏ö‡∏•‡∏î‡∏á‡πÄ...  0.048927   \n",
       "4        ‡∏û‡∏¥‡∏à‡∏¥‡∏ï‡∏£  ‡πÄ‡∏Ç‡∏ï‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ï‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà 3/‡∏≠‡∏≥‡πÄ‡∏†‡∏≠‡πÇ‡∏û‡∏ò‡∏¥‡πå‡∏õ‡∏£‡∏∞‡∏ó‡∏±‡∏ö‡∏ä‡πâ‡∏≤‡∏á/‡∏ó‡∏ï.‡πÇ‡∏û‡∏ò‡∏¥...  0.048993   \n",
       "\n",
       "                             file_id  \n",
       "0  1_j0DNaqCXIkEk0MK3y0J1eCN3hUOCXeF  \n",
       "1  1gDxp58u2W14uhdb6NpRDqxl1d7aa2WFy  \n",
       "2  1a5jF1Oyv3UEatBq1MT1ga8kS10uR19-c  \n",
       "3  1tzz6gMXk1n3pQtreQWMIU2xlkncg-g_r  \n",
       "4  1-MsML3nSXUrscvmzdZb7R4yTkZcuTc5m  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def query_pdf_files(\n",
    "    limit: int = 10,\n",
    "    province: Optional[str] = None,\n",
    "    min_size_kb: float = 50.0,\n",
    "    max_size_mb: Optional[float] = 50.0\n",
    ") -> list[dict]:\n",
    "    \"\"\"\n",
    "    Query BigQuery for PDF files.\n",
    "    \n",
    "    Args:\n",
    "        limit: Maximum number of files to return\n",
    "        province: Filter by province name (optional)\n",
    "        min_size_kb: Minimum file size in KB (default: 50 KB to exclude corrupted files)\n",
    "        max_size_mb: Maximum file size in MB (optional)\n",
    "    \n",
    "    Returns:\n",
    "        List of file metadata dicts\n",
    "    \"\"\"\n",
    "    # Build query\n",
    "    conditions = [\"mime_type = 'application/pdf'\"]\n",
    "    \n",
    "    # Add minimum size filter (exclude very small/corrupted files)\n",
    "    min_bytes = int(min_size_kb * 1024)\n",
    "    conditions.append(f\"size >= {min_bytes}\")\n",
    "    \n",
    "    if province:\n",
    "        conditions.append(f\"province_name = '{province}'\")\n",
    "    \n",
    "    if max_size_mb:\n",
    "        max_bytes = int(max_size_mb * 1024 * 1024)\n",
    "        conditions.append(f\"size <= {max_bytes}\")\n",
    "    \n",
    "    where_clause = \" AND \".join(conditions)\n",
    "    \n",
    "    query = f\"\"\"\n",
    "    SELECT \n",
    "        file_id, \n",
    "        path,\n",
    "        mime_type, \n",
    "        folder_id, \n",
    "        province_name,\n",
    "        size,\n",
    "        mod_time\n",
    "    FROM `{BQ_TABLE}`\n",
    "    WHERE {where_clause}\n",
    "    ORDER BY size ASC\n",
    "    LIMIT {limit}\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"üîç Querying BigQuery...\")\n",
    "    print(f\"   Filters: {where_clause}\")\n",
    "    \n",
    "    # Execute query\n",
    "    query_job = bq_client.query(query)\n",
    "    results = query_job.result()\n",
    "    \n",
    "    # Convert to list\n",
    "    files = []\n",
    "    for row in results:\n",
    "        files.append({\n",
    "            \"file_id\": row.file_id,\n",
    "            \"path\": row.path,\n",
    "            \"mime_type\": row.mime_type,\n",
    "            \"folder_id\": row.folder_id,\n",
    "            \"province_name\": row.province_name,\n",
    "            \"size\": row.size,\n",
    "            \"size_mb\": row.size / (1024 * 1024) if row.size else 0,\n",
    "            \"size_kb\": row.size / 1024 if row.size else 0,\n",
    "            \"mod_time\": row.mod_time,\n",
    "        })\n",
    "    \n",
    "    print(f\"‚úÖ Found {len(files)} file(s)\")\n",
    "    print(f\"   Size range: {files[0]['size_kb']:.1f} KB - {files[-1]['size_kb']:.1f} KB\" if files else \"\")\n",
    "    return files\n",
    "\n",
    "\n",
    "# Query for files (min 50 KB, max 50 MB)\n",
    "pdf_files = query_pdf_files(limit=10, min_size_kb=50.0, max_size_mb=50.0)\n",
    "\n",
    "# Display as DataFrame\n",
    "if pdf_files:\n",
    "    df = pd.DataFrame(pdf_files)\n",
    "    display(df[['province_name', 'path', 'size_mb', 'file_id']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Select Test File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Selected Test File:\n",
      "================================================================================\n",
      "Province: ‡∏û‡∏¥‡∏à‡∏¥‡∏ï‡∏£\n",
      "Path: ‡πÄ‡∏Ç‡∏ï‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ï‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà 3/‡∏≠‡∏≥‡πÄ‡∏†‡∏≠‡πÇ‡∏û‡∏ò‡∏¥‡πå‡∏õ‡∏£‡∏∞‡∏ó‡∏±‡∏ö‡∏ä‡πâ‡∏≤‡∏á/‡∏ó‡∏ï.‡πÇ‡∏û‡∏ò‡∏¥‡πå‡∏õ‡∏£‡∏∞‡∏ó‡∏±‡∏ö‡∏ä‡πâ‡∏≤‡∏á/‡∏´‡∏ô‡πà‡∏ß‡∏¢‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ï‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà 9/‡∏™‡∏™5‡∏ó‡∏±‡∏ö18 ‡∏ô_09.pdf\n",
      "File ID: 1_j0DNaqCXIkEk0MK3y0J1eCN3hUOCXeF\n",
      "Size: 0.05 MB\n",
      "Modified: 2026-02-10T03:26:51.000Z\n",
      "\n",
      "üìÅ Google Drive URI:\n",
      "https://drive.google.com/uc?export=download&id=1_j0DNaqCXIkEk0MK3y0J1eCN3hUOCXeF\n"
     ]
    }
   ],
   "source": [
    "# Select first file for testing\n",
    "test_file = pdf_files[0]\n",
    "\n",
    "print(\"üìÑ Selected Test File:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Province: {test_file['province_name']}\")\n",
    "print(f\"Path: {test_file['path']}\")\n",
    "print(f\"File ID: {test_file['file_id']}\")\n",
    "print(f\"Size: {test_file['size_mb']:.2f} MB\")\n",
    "print(f\"Modified: {test_file['mod_time']}\")\n",
    "print()\n",
    "\n",
    "# Construct Google Drive URI\n",
    "drive_uri = f\"https://drive.google.com/uc?export=download&id={test_file['file_id']}\"\n",
    "print(f\"üìÅ Google Drive URI:\")\n",
    "print(drive_uri)\n",
    "\n",
    "# Store for later use\n",
    "TEST_FILE_ID = test_file['file_id']\n",
    "TEST_DRIVE_URI = drive_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def extract_from_drive_url(\n    drive_uri: str,\n    model: str = MODEL_NAME,\n    temperature: float = 0.0,\n    max_tokens: int = 8192,\n) -> tuple[dict, dict]:\n    \"\"\"\n    Extract vote data from a PDF file stored in Google Drive.\n    \n    Args:\n        drive_uri: Google Drive file URI (https://drive.google.com/uc?export=download&id=...)\n        model: Gemini model name\n        temperature: Sampling temperature\n        max_tokens: Maximum output tokens\n    \n    Returns:\n        Tuple of (extracted_data, usage_metadata)\n    \"\"\"\n    print(f\"ü§ñ Extracting with {model}...\")\n    print(f\"   Using Google Drive URI (External URL method)\")\n    \n    # Create file part from URI\n    file_part = types.Part.from_uri(\n        file_uri=drive_uri,\n        mime_type=\"application/pdf\"\n    )\n    \n    # Enhanced extraction prompt\n    prompt = \"\"\"\n    You are an expert data entry assistant for Thai Election documents (Form S.S. 5/18).\n    \n    CRITICAL INSTRUCTIONS:\n    \n    1. **Analyze all pages** of this PDF document carefully.\n    \n    2. **Extract BOTH number formats** for all numerical values:\n       - Arabic numerals (e.g., 120)\n       - Thai text (e.g., \"‡∏´‡∏ô‡∏∂‡πà‡∏á‡∏£‡πâ‡∏≠‡∏¢‡∏¢‡∏µ‡πà‡∏™‡∏¥‡∏ö\")\n       This applies to: voter statistics, ballot statistics, vote counts, and total votes.\n    \n    3. **Header Information** (usually on first page):\n       - Form type: \"Constituency\" (‡πÅ‡∏ö‡∏ö‡πÅ‡∏ö‡πà‡∏á‡πÄ‡∏Ç‡∏ï) or \"PartyList\" (‡∏ö‡∏±‡∏ç‡∏ä‡∏µ‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠)\n       - Set number (‡∏ä‡∏∏‡∏î‡∏ó‡∏µ‡πà) if present\n       - Date, Province, District, Sub-district\n       - Polling station number (‡∏´‡∏ô‡πà‡∏ß‡∏¢‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ï‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà)\n       - Village number (‡∏´‡∏°‡∏π‡πà‡∏ó‡∏µ‡πà) if present\n    \n    4. **Section 1 - Voter Statistics:**\n       - 1.1 Eligible voters (‡∏ú‡∏π‡πâ‡∏°‡∏µ‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ï‡∏±‡πâ‡∏á‡∏ï‡∏≤‡∏°‡∏ö‡∏±‡∏ç‡∏ä‡∏µ)\n       - 1.2 Present voters (‡∏ú‡∏π‡πâ‡∏°‡∏≤‡πÅ‡∏™‡∏î‡∏á‡∏ï‡∏ô)\n       Extract both arabic and thai_text for each.\n    \n    5. **Section 2 - Ballot Statistics:**\n       - 2.1 Allocated ballots (‡∏ö‡∏±‡∏ï‡∏£‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏à‡∏±‡∏î‡∏™‡∏£‡∏£)\n       - 2.2 Used ballots (‡∏ö‡∏±‡∏ï‡∏£‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ)\n       - 2.2.1 Valid ballots (‡∏ö‡∏±‡∏ï‡∏£‡∏î‡∏µ)\n       - 2.2.2 Invalid ballots (‡∏ö‡∏±‡∏ï‡∏£‡πÄ‡∏™‡∏µ‡∏¢)\n       - 2.2.3 No vote ballots (‡πÑ‡∏°‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å)\n       - 2.3 Remaining ballots (‡∏ö‡∏±‡∏ï‡∏£‡πÄ‡∏´‡∏•‡∏∑‡∏≠)\n       Extract both arabic and thai_text for each.\n    \n    6. **Section 3 - Vote Results Table:**\n       - Consolidate all pages (table often spans multiple pages)\n       - For each entry: number, candidate name (if Constituency), party name, vote count\n       - Extract vote_count as {arabic: int, thai_text: str}\n    \n    7. **Total Votes Recorded:**\n       - Look for \"‡∏£‡∏ß‡∏°\" (total) at the bottom of the vote results table\n       - Extract both arabic and thai_text\n    \n    8. **Officials (Committee Members):**\n       - Extract names and positions from signature section\n       - Common positions: ‡∏õ‡∏£‡∏∞‡∏ò‡∏≤‡∏ô (Chair), ‡∏Å‡∏£‡∏£‡∏°‡∏Å‡∏≤‡∏£ (Member), ‡πÄ‡∏•‡∏Ç‡∏≤‡∏ô‡∏∏‡∏Å‡∏≤‡∏£ (Secretary)\n    \n    9. **Validation:**\n       - ballots_used.arabic = good_ballots.arabic + bad_ballots.arabic + no_vote_ballots.arabic\n       - total_votes_recorded.arabic = sum of all vote_count.arabic\n    \"\"\"\n    \n    # Configure generation\n    generation_config = types.GenerateContentConfig(\n        response_mime_type=\"application/json\",\n        response_schema=ELECTION_DATA_SCHEMA,\n        temperature=temperature,\n        max_output_tokens=max_tokens,\n        top_p=0.95,\n        top_k=40,\n    )\n    \n    # Generate content\n    print(\"   Sending request to Gemini...\")\n    response = gemini_client.models.generate_content(\n        model=model,\n        contents=[file_part, prompt],\n        config=generation_config,\n    )\n    \n    # Parse response\n    result = json.loads(response.text)\n    \n    # Extract comprehensive usage metadata\n    usage_metadata = {\n        \"model\": model,\n        \"prompt_token_count\": getattr(response.usage_metadata, 'prompt_token_count', None) if hasattr(response, 'usage_metadata') else None,\n        \"candidates_token_count\": getattr(response.usage_metadata, 'candidates_token_count', None) if hasattr(response, 'usage_metadata') else None,\n        \"cached_content_token_count\": getattr(response.usage_metadata, 'cached_content_token_count', None) if hasattr(response, 'usage_metadata') else None,\n        \"thoughts_token_count\": getattr(response.usage_metadata, 'thoughts_token_count', None) if hasattr(response, 'usage_metadata') else None,\n        \"total_token_count\": getattr(response.usage_metadata, 'total_token_count', None) if hasattr(response, 'usage_metadata') else None,\n        \"generation_config\": {\n            \"temperature\": temperature,\n            \"max_output_tokens\": max_tokens,\n            \"top_p\": 0.95,\n            \"top_k\": 40,\n            \"response_mime_type\": \"application/json\",\n        },\n    }\n    \n    print(f\"\\n‚úÖ Extraction complete!\")\n    print(f\"   Extracted {len(result)} report(s)\")\n    \n    # Display token usage\n    if usage_metadata.get('total_token_count'):\n        print(f\"\\nüìä Token Usage:\")\n        print(f\"   Input tokens: {usage_metadata['prompt_token_count']:,}\")\n        print(f\"   Output tokens: {usage_metadata['candidates_token_count']:,}\")\n        \n        # Show cached content if available\n        if usage_metadata.get('cached_content_token_count'):\n            print(f\"   Cached tokens: {usage_metadata['cached_content_token_count']:,}\")\n        \n        # Show thoughts tokens if available (for thinking models)\n        if usage_metadata.get('thoughts_token_count'):\n            print(f\"   Thoughts tokens: {usage_metadata['thoughts_token_count']:,}\")\n        \n        print(f\"   Total tokens: {usage_metadata['total_token_count']:,}\")\n    \n    return result, usage_metadata\n\n\nprint(\"‚úÖ Enhanced extraction function with comprehensive usage tracking\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract data from Google Drive PDF (now returns data + usage metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Extracting with gemini-3-flash-preview...\n",
      "   Using Google Drive URI (External URL method)\n",
      "   Sending request to Gemini...\n",
      "\n",
      "‚úÖ Extraction complete!\n",
      "   Extracted 1 report(s)\n",
      "\n",
      "üìä Token Usage:\n",
      "   Input tokens: None\n",
      "   Output tokens: 1046\n",
      "   Total tokens: 2339\n"
     ]
    }
   ],
   "source": [
    "result, usage_metadata, response = extract_from_drive_url(\n",
    "    drive_uri=TEST_DRIVE_URI,\n",
    "    model=MODEL_NAME,\n",
    "    temperature=0.0,\n",
    "    max_tokens=8192,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerateContentResponseUsageMetadata(\n",
       "  cached_content_token_count=2048,\n",
       "  candidates_token_count=1046,\n",
       "  thoughts_token_count=1293,\n",
       "  total_token_count=2339\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.usage_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Display Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "REPORT #1\n",
      "================================================================================\n",
      "\n",
      "üìã FORM INFORMATION\n",
      "   Form Type: Constituency\n",
      "   Province: ‡πÅ‡∏û‡∏£‡πà\n",
      "   District: ‡∏ß‡∏±‡∏á‡∏ä‡∏¥‡πâ‡∏ô\n",
      "   Sub-district: ‡∏ß‡∏±‡∏á‡∏ä‡∏¥‡πâ‡∏ô\n",
      "   Station: 1\n",
      "   Village (‡∏´‡∏°‡∏π‡πà): 1\n",
      "   Date: 14 ‡∏û‡∏§‡∏©‡∏†‡∏≤‡∏Ñ‡∏° 2566\n",
      "\n",
      "üë• VOTER STATISTICS\n",
      "   Eligible: 421 (‡∏™‡∏µ‡πà‡∏£‡πâ‡∏≠‡∏¢‡∏¢‡∏µ‡πà‡∏™‡∏¥‡∏ö‡πÄ‡∏≠‡πá‡∏î)\n",
      "   Present: 276 (‡∏™‡∏≠‡∏á‡∏£‡πâ‡∏≠‡∏¢‡πÄ‡∏à‡πá‡∏î‡∏™‡∏¥‡∏ö‡∏´‡∏Å)\n",
      "\n",
      "üì¶ BALLOT STATISTICS\n",
      "   Allocated: 420\n",
      "   Used: 276\n",
      "   - Good: 264\n",
      "   - Bad: 8\n",
      "   - No Vote: 4\n",
      "   Remaining: 144\n",
      "   ‚úÖ Validation: PASSED (276 = 276)\n",
      "\n",
      "üìä VOTE RESULTS (7 entries)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#</th>\n",
       "      <th>Candidate</th>\n",
       "      <th>Party</th>\n",
       "      <th>Votes</th>\n",
       "      <th>Votes (Thai)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>‡∏ô‡∏≤‡∏¢‡∏ò‡∏ô‡∏Å‡∏§‡∏ï ‡∏ß‡∏¥‡∏ö‡∏π‡∏•‡∏¢‡πå‡∏®‡∏¥‡∏£‡∏¥</td>\n",
       "      <td>‡∏Å‡πâ‡∏≤‡∏ß‡∏ò‡∏£‡∏£‡∏°</td>\n",
       "      <td>57</td>\n",
       "      <td>‡∏´‡πâ‡∏≤‡∏™‡∏¥‡∏ö‡πÄ‡∏à‡πá‡∏î</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>‡∏ô‡∏≤‡∏¢‡∏®‡∏¥‡∏£‡∏¥‡∏ß‡∏±‡∏í‡∏ô‡πå ‡∏à‡∏∏‡∏õ‡∏∞‡∏°‡∏±‡∏î‡∏ñ‡∏≤</td>\n",
       "      <td>‡∏†‡∏π‡∏°‡∏¥‡πÉ‡∏à‡πÑ‡∏ó‡∏¢</td>\n",
       "      <td>159</td>\n",
       "      <td>‡∏´‡∏ô‡∏∂‡πà‡∏á‡∏£‡πâ‡∏≠‡∏¢‡∏´‡πâ‡∏≤‡∏™‡∏¥‡∏ö‡πÄ‡∏Å‡πâ‡∏≤</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>‡∏ô‡∏≤‡∏¢‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤ ‡∏°‡∏≤‡∏•‡∏≤</td>\n",
       "      <td>‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏ó‡∏¢</td>\n",
       "      <td>10</td>\n",
       "      <td>‡∏™‡∏¥‡∏ö</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>‡∏ô‡∏≤‡∏¢‡∏ô‡∏û‡∏î‡∏• ‡∏õ‡∏•‡∏≠‡∏î‡∏ó‡∏±‡∏ö</td>\n",
       "      <td>‡∏£‡∏ß‡∏°‡πÑ‡∏ó‡∏¢‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ä‡∏≤‡∏ï‡∏¥</td>\n",
       "      <td>0</td>\n",
       "      <td>‡∏®‡∏π‡∏ô‡∏¢‡πå</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>‡∏û‡∏±‡∏ô‡πÄ‡∏≠‡∏Å‡∏ô‡∏û ‡∏ä‡∏π‡∏à‡∏¥‡∏ï‡∏£</td>\n",
       "      <td>‡πÄ‡∏™‡∏£‡∏µ‡∏£‡∏ß‡∏°‡πÑ‡∏ó‡∏¢</td>\n",
       "      <td>1</td>\n",
       "      <td>‡∏´‡∏ô‡∏∂‡πà‡∏á</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>‡∏ô‡∏≤‡∏¢‡∏™‡∏°‡∏ö‡∏±‡∏ï‡∏¥ ‡∏°‡∏∞‡∏Å‡∏≤‡∏î</td>\n",
       "      <td>‡∏õ‡∏£‡∏∞‡∏ä‡∏≤‡∏ò‡∏¥‡∏õ‡∏±‡∏ï‡∏¢‡πå</td>\n",
       "      <td>1</td>\n",
       "      <td>‡∏´‡∏ô‡∏∂‡πà‡∏á</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>‡∏ô‡∏≤‡∏¢‡∏†‡∏±‡∏ó‡∏£‡∏û‡∏á‡∏®‡πå ‡∏†‡∏±‡∏ó‡∏£‡∏ö‡∏≥‡∏£‡∏∏‡∏á‡πÇ‡∏£‡∏à‡∏ô‡πå</td>\n",
       "      <td>‡∏õ‡∏£‡∏∞‡∏ä‡∏≤‡∏ä‡∏ô</td>\n",
       "      <td>36</td>\n",
       "      <td>‡∏™‡∏≤‡∏°‡∏™‡∏¥‡∏ö‡∏´‡∏Å</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   #                   Candidate            Party  Votes         Votes (Thai)\n",
       "0  1        ‡∏ô‡∏≤‡∏¢‡∏ò‡∏ô‡∏Å‡∏§‡∏ï ‡∏ß‡∏¥‡∏ö‡∏π‡∏•‡∏¢‡πå‡∏®‡∏¥‡∏£‡∏¥         ‡∏Å‡πâ‡∏≤‡∏ß‡∏ò‡∏£‡∏£‡∏°     57           ‡∏´‡πâ‡∏≤‡∏™‡∏¥‡∏ö‡πÄ‡∏à‡πá‡∏î\n",
       "1  2      ‡∏ô‡∏≤‡∏¢‡∏®‡∏¥‡∏£‡∏¥‡∏ß‡∏±‡∏í‡∏ô‡πå ‡∏à‡∏∏‡∏õ‡∏∞‡∏°‡∏±‡∏î‡∏ñ‡∏≤        ‡∏†‡∏π‡∏°‡∏¥‡πÉ‡∏à‡πÑ‡∏ó‡∏¢    159  ‡∏´‡∏ô‡∏∂‡πà‡∏á‡∏£‡πâ‡∏≠‡∏¢‡∏´‡πâ‡∏≤‡∏™‡∏¥‡∏ö‡πÄ‡∏Å‡πâ‡∏≤\n",
       "2  3               ‡∏ô‡∏≤‡∏¢‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤ ‡∏°‡∏≤‡∏•‡∏≤         ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏ó‡∏¢     10                  ‡∏™‡∏¥‡∏ö\n",
       "3  4             ‡∏ô‡∏≤‡∏¢‡∏ô‡∏û‡∏î‡∏• ‡∏õ‡∏•‡∏≠‡∏î‡∏ó‡∏±‡∏ö  ‡∏£‡∏ß‡∏°‡πÑ‡∏ó‡∏¢‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ä‡∏≤‡∏ï‡∏¥      0                ‡∏®‡∏π‡∏ô‡∏¢‡πå\n",
       "4  5             ‡∏û‡∏±‡∏ô‡πÄ‡∏≠‡∏Å‡∏ô‡∏û ‡∏ä‡∏π‡∏à‡∏¥‡∏ï‡∏£       ‡πÄ‡∏™‡∏£‡∏µ‡∏£‡∏ß‡∏°‡πÑ‡∏ó‡∏¢      1                ‡∏´‡∏ô‡∏∂‡πà‡∏á\n",
       "5  6             ‡∏ô‡∏≤‡∏¢‡∏™‡∏°‡∏ö‡∏±‡∏ï‡∏¥ ‡∏°‡∏∞‡∏Å‡∏≤‡∏î     ‡∏õ‡∏£‡∏∞‡∏ä‡∏≤‡∏ò‡∏¥‡∏õ‡∏±‡∏ï‡∏¢‡πå      1                ‡∏´‡∏ô‡∏∂‡πà‡∏á\n",
       "6  7  ‡∏ô‡∏≤‡∏¢‡∏†‡∏±‡∏ó‡∏£‡∏û‡∏á‡∏®‡πå ‡∏†‡∏±‡∏ó‡∏£‡∏ö‡∏≥‡∏£‡∏∏‡∏á‡πÇ‡∏£‡∏à‡∏ô‡πå          ‡∏õ‡∏£‡∏∞‡∏ä‡∏≤‡∏ä‡∏ô     36             ‡∏™‡∏≤‡∏°‡∏™‡∏¥‡∏ö‡∏´‡∏Å"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   Calculated Total: 264\n",
      "   Recorded Total: 264 (‡∏™‡∏≠‡∏á‡∏£‡πâ‡∏≠‡∏¢‡∏´‡∏Å‡∏™‡∏¥‡∏ö‡∏™‡∏µ‡πà)\n",
      "   ‚úÖ Total validation: PASSED\n"
     ]
    }
   ],
   "source": [
    "def get_number_value(num_obj) -> int:\n",
    "    \"\"\"Extract arabic number from NumberTextPair or plain int.\"\"\"\n",
    "    if isinstance(num_obj, dict):\n",
    "        return num_obj.get('arabic', 0)\n",
    "    elif isinstance(num_obj, int):\n",
    "        return num_obj\n",
    "    return 0\n",
    "\n",
    "\n",
    "def get_thai_text(num_obj) -> str:\n",
    "    \"\"\"Extract Thai text from NumberTextPair or return empty string.\"\"\"\n",
    "    if isinstance(num_obj, dict):\n",
    "        return num_obj.get('thai_text', '')\n",
    "    return ''\n",
    "\n",
    "\n",
    "def display_results(result: list[dict]):\n",
    "    \"\"\"Display enhanced extraction results with NumberTextPair support.\"\"\"\n",
    "    if not result:\n",
    "        print(\"‚ùå No data extracted\")\n",
    "        return\n",
    "    \n",
    "    for idx, report in enumerate(result, 1):\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"REPORT #{idx}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # Form Info\n",
    "        form_info = report.get(\"form_info\", {})\n",
    "        print(f\"\\nüìã FORM INFORMATION\")\n",
    "        print(f\"   Form Type: {form_info.get('form_type', 'N/A')}\")\n",
    "        \n",
    "        # Show set_number if available\n",
    "        if form_info.get('set_number'):\n",
    "            print(f\"   Set Number: {form_info.get('set_number')}\")\n",
    "        \n",
    "        print(f\"   Province: {form_info.get('province', 'N/A')}\")\n",
    "        print(f\"   District: {form_info.get('district', 'N/A')}\")\n",
    "        \n",
    "        if form_info.get('sub_district'):\n",
    "            print(f\"   Sub-district: {form_info.get('sub_district')}\")\n",
    "        \n",
    "        print(f\"   Station: {form_info.get('polling_station_number', 'N/A')}\")\n",
    "        \n",
    "        # Show village_moo if available\n",
    "        if form_info.get('village_moo'):\n",
    "            print(f\"   Village (‡∏´‡∏°‡∏π‡πà): {form_info.get('village_moo')}\")\n",
    "        \n",
    "        if form_info.get('date'):\n",
    "            print(f\"   Date: {form_info.get('date')}\")\n",
    "        \n",
    "        # Voter Statistics\n",
    "        voter_stats = report.get(\"voter_statistics\")\n",
    "        if voter_stats and (voter_stats.get(\"eligible_voters\") or voter_stats.get(\"present_voters\")):\n",
    "            print(f\"\\nüë• VOTER STATISTICS\")\n",
    "            \n",
    "            eligible = voter_stats.get(\"eligible_voters\")\n",
    "            if eligible:\n",
    "                arabic = get_number_value(eligible)\n",
    "                thai = get_thai_text(eligible)\n",
    "                if thai:\n",
    "                    print(f\"   Eligible: {arabic:,} ({thai})\")\n",
    "                else:\n",
    "                    print(f\"   Eligible: {arabic:,}\")\n",
    "            \n",
    "            present = voter_stats.get(\"present_voters\")\n",
    "            if present:\n",
    "                arabic = get_number_value(present)\n",
    "                thai = get_thai_text(present)\n",
    "                if thai:\n",
    "                    print(f\"   Present: {arabic:,} ({thai})\")\n",
    "                else:\n",
    "                    print(f\"   Present: {arabic:,}\")\n",
    "        \n",
    "        # Ballot Statistics\n",
    "        ballot_stats = report.get(\"ballot_statistics\")\n",
    "        if ballot_stats:\n",
    "            print(f\"\\nüì¶ BALLOT STATISTICS\")\n",
    "            \n",
    "            # Extract values safely\n",
    "            used = get_number_value(ballot_stats.get('ballots_used'))\n",
    "            good = get_number_value(ballot_stats.get('good_ballots'))\n",
    "            bad = get_number_value(ballot_stats.get('bad_ballots'))\n",
    "            no_vote = get_number_value(ballot_stats.get('no_vote_ballots'))\n",
    "            allocated = get_number_value(ballot_stats.get('ballots_allocated'))\n",
    "            remaining = get_number_value(ballot_stats.get('ballots_remaining'))\n",
    "            \n",
    "            if allocated > 0:\n",
    "                print(f\"   Allocated: {allocated:,}\")\n",
    "            if used > 0:\n",
    "                print(f\"   Used: {used:,}\")\n",
    "            if good > 0:\n",
    "                print(f\"   - Good: {good:,}\")\n",
    "            if bad > 0:\n",
    "                print(f\"   - Bad: {bad:,}\")\n",
    "            if no_vote > 0:\n",
    "                print(f\"   - No Vote: {no_vote:,}\")\n",
    "            if remaining > 0:\n",
    "                print(f\"   Remaining: {remaining:,}\")\n",
    "            \n",
    "            # Validation\n",
    "            if used > 0 and (good > 0 or bad > 0 or no_vote > 0):\n",
    "                expected = good + bad + no_vote\n",
    "                if used == expected:\n",
    "                    print(f\"   ‚úÖ Validation: PASSED ({used:,} = {expected:,})\")\n",
    "                else:\n",
    "                    print(f\"   ‚ö†Ô∏è  Validation: FAILED ({used:,} ‚â† {expected:,})\")\n",
    "        \n",
    "        # Vote Results\n",
    "        vote_results = report.get(\"vote_results\", [])\n",
    "        if vote_results:\n",
    "            print(f\"\\nüìä VOTE RESULTS ({len(vote_results)} entries)\")\n",
    "            \n",
    "            # Create DataFrame\n",
    "            df_data = []\n",
    "            for v in vote_results:\n",
    "                vote_count_obj = v.get(\"vote_count\")\n",
    "                vote_arabic = get_number_value(vote_count_obj)\n",
    "                vote_thai = get_thai_text(vote_count_obj)\n",
    "                \n",
    "                row = {\n",
    "                    \"#\": v.get(\"number\"),\n",
    "                    \"Candidate\": v.get(\"candidate_name\") or \"-\",\n",
    "                    \"Party\": v.get(\"party_name\") or \"-\",\n",
    "                    \"Votes\": vote_arabic,\n",
    "                }\n",
    "                \n",
    "                # Add Thai text column if any results have it\n",
    "                if vote_thai:\n",
    "                    row[\"Votes (Thai)\"] = vote_thai[:30] + \"...\" if len(vote_thai) > 30 else vote_thai\n",
    "                \n",
    "                df_data.append(row)\n",
    "            \n",
    "            df = pd.DataFrame(df_data)\n",
    "            display(df)\n",
    "            \n",
    "            # Calculate total\n",
    "            total = df[\"Votes\"].sum()\n",
    "            print(f\"\\n   Calculated Total: {total:,}\")\n",
    "            \n",
    "            # Show recorded total if available\n",
    "            total_recorded = report.get(\"total_votes_recorded\")\n",
    "            if total_recorded:\n",
    "                recorded_arabic = get_number_value(total_recorded)\n",
    "                recorded_thai = get_thai_text(total_recorded)\n",
    "                \n",
    "                if recorded_thai:\n",
    "                    print(f\"   Recorded Total: {recorded_arabic:,} ({recorded_thai})\")\n",
    "                else:\n",
    "                    print(f\"   Recorded Total: {recorded_arabic:,}\")\n",
    "                \n",
    "                # Validation\n",
    "                if total == recorded_arabic:\n",
    "                    print(f\"   ‚úÖ Total validation: PASSED\")\n",
    "                else:\n",
    "                    print(f\"   ‚ö†Ô∏è  Total validation: FAILED ({total:,} ‚â† {recorded_arabic:,})\")\n",
    "        \n",
    "        # Officials\n",
    "        officials = report.get(\"officials\")\n",
    "        if officials and len(officials) > 0:\n",
    "            print(f\"\\nüëî COMMITTEE MEMBERS ({len(officials)} members)\")\n",
    "            for i, official in enumerate(officials[:10], 1):  # Show max 10\n",
    "                name = official.get('name', 'N/A')\n",
    "                position = official.get('position', 'N/A')\n",
    "                print(f\"   {i}. {name} - {position}\")\n",
    "            \n",
    "            if len(officials) > 10:\n",
    "                print(f\"   ... and {len(officials) - 10} more\")\n",
    "\n",
    "\n",
    "# Display results\n",
    "try:\n",
    "    display_results(result)\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error displaying results: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    \n",
    "    # Show raw result for debugging\n",
    "    print(\"\\nüîç Raw result (first 500 chars):\")\n",
    "    print(json.dumps(result, ensure_ascii=False, indent=2)[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "VALIDATION RESULTS\n",
      "================================================================================\n",
      "\n",
      "‚úÖ All validation checks PASSED!\n",
      "\n",
      "================================================================================\n",
      "Overall: ‚úÖ VALID\n"
     ]
    }
   ],
   "source": [
    "def validate_extraction_enhanced(data: dict) -> tuple[bool, list[str]]:\n",
    "    \"\"\"\n",
    "    Enhanced validation with NumberTextPair support.\n",
    "    \n",
    "    Args:\n",
    "        data: Extracted form data\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (is_valid, list of error messages)\n",
    "    \"\"\"\n",
    "    errors = []\n",
    "    warnings = []\n",
    "    \n",
    "    # 1. Ballot statistics validation\n",
    "    ballot_stats = data.get(\"ballot_statistics\")\n",
    "    if ballot_stats:\n",
    "        used = get_number_value(ballot_stats.get(\"ballots_used\"))\n",
    "        good = get_number_value(ballot_stats.get(\"good_ballots\"))\n",
    "        bad = get_number_value(ballot_stats.get(\"bad_ballots\"))\n",
    "        no_vote = get_number_value(ballot_stats.get(\"no_vote_ballots\"))\n",
    "        \n",
    "        expected_total = good + bad + no_vote\n",
    "        \n",
    "        if used != expected_total:\n",
    "            errors.append(\n",
    "                f\"Ballot mismatch: ballots_used ({used:,}) != \"\n",
    "                f\"good+bad+no_vote ({expected_total:,})\"\n",
    "            )\n",
    "    \n",
    "    # 2. Total votes validation (NEW!)\n",
    "    vote_results = data.get(\"vote_results\", [])\n",
    "    total_recorded = data.get(\"total_votes_recorded\")\n",
    "    \n",
    "    if vote_results and total_recorded:\n",
    "        # Sum up all vote counts\n",
    "        calculated_total = sum(get_number_value(v.get(\"vote_count\")) for v in vote_results)\n",
    "        recorded_total = get_number_value(total_recorded)\n",
    "        \n",
    "        if calculated_total != recorded_total:\n",
    "            errors.append(\n",
    "                f\"Vote total mismatch: sum of votes ({calculated_total:,}) != \"\n",
    "                f\"recorded total ({recorded_total:,})\"\n",
    "            )\n",
    "    \n",
    "    # 3. Voter statistics vs ballot statistics (NEW!)\n",
    "    voter_stats = data.get(\"voter_statistics\")\n",
    "    if voter_stats and ballot_stats:\n",
    "        present = get_number_value(voter_stats.get(\"present_voters\"))\n",
    "        used = get_number_value(ballot_stats.get(\"ballots_used\"))\n",
    "        \n",
    "        # Present voters should roughly match ballots used (allow small discrepancy)\n",
    "        discrepancy = abs(present - used)\n",
    "        if discrepancy > 5:\n",
    "            warnings.append(\n",
    "                f\"Voter count ({present:,}) differs from ballots used ({used:,}) by {discrepancy}\"\n",
    "            )\n",
    "    \n",
    "    # 4. Vote count non-negative check\n",
    "    for i, result in enumerate(vote_results, 1):\n",
    "        vote_count = get_number_value(result.get(\"vote_count\"))\n",
    "        if vote_count < 0:\n",
    "            name = result.get(\"candidate_name\") or result.get(\"party_name\") or f\"Entry #{i}\"\n",
    "            errors.append(f\"Negative vote count for {name}: {vote_count}\")\n",
    "    \n",
    "    # 5. Check for empty vote results\n",
    "    if not vote_results:\n",
    "        errors.append(\"No vote results extracted\")\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"VALIDATION RESULTS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    if errors:\n",
    "        print(f\"\\n‚ùå ERRORS ({len(errors)}):\")\n",
    "        for error in errors:\n",
    "            print(f\"   - {error}\")\n",
    "    \n",
    "    if warnings:\n",
    "        print(f\"\\n‚ö†Ô∏è  WARNINGS ({len(warnings)}):\")\n",
    "        for warning in warnings:\n",
    "            print(f\"   - {warning}\")\n",
    "    \n",
    "    if not errors and not warnings:\n",
    "        print(f\"\\n‚úÖ All validation checks PASSED!\")\n",
    "    elif not errors:\n",
    "        print(f\"\\n‚úÖ No errors, but {len(warnings)} warning(s)\")\n",
    "    \n",
    "    return len(errors) == 0, errors\n",
    "\n",
    "\n",
    "# Run validation on extracted data\n",
    "is_valid, errors = validate_extraction_enhanced(result[0] if result else {})\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Overall: {'‚úÖ VALID' if is_valid else '‚ùå INVALID'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PYDANTIC MODEL VALIDATION\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Report #1 - Pydantic validation PASSED\n",
      "   Form Type: Constituency\n",
      "   District: ‡∏ß‡∏±‡∏á‡∏ä‡∏¥‡πâ‡∏ô\n",
      "   Set Number: N/A\n",
      "   Village: 1\n",
      "   Eligible Voters: 421\n",
      "   Present Voters: 276\n",
      "   Ballots Used: 276\n",
      "   Vote Results: 7 entries\n",
      "   Total Votes: 264\n"
     ]
    }
   ],
   "source": [
    "# Validate each report with enhanced Pydantic models\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PYDANTIC MODEL VALIDATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for idx, report_data in enumerate(result, 1):\n",
    "    try:\n",
    "        # Parse into Pydantic model\n",
    "        form_data = ElectionFormData(**report_data)\n",
    "        \n",
    "        print(f\"\\n‚úÖ Report #{idx} - Pydantic validation PASSED\")\n",
    "        print(f\"   Form Type: {form_data.form_info.form_type}\")\n",
    "        print(f\"   District: {form_data.form_info.district}\")\n",
    "        print(f\"   Set Number: {form_data.form_info.set_number or 'N/A'}\")\n",
    "        print(f\"   Village: {form_data.form_info.village_moo or 'N/A'}\")\n",
    "        \n",
    "        # Show voter statistics if available\n",
    "        if form_data.voter_statistics:\n",
    "            if form_data.voter_statistics.eligible_voters:\n",
    "                print(f\"   Eligible Voters: {form_data.voter_statistics.eligible_voters.arabic:,}\")\n",
    "            if form_data.voter_statistics.present_voters:\n",
    "                print(f\"   Present Voters: {form_data.voter_statistics.present_voters.arabic:,}\")\n",
    "        \n",
    "        # Show ballot statistics\n",
    "        if form_data.ballot_statistics and form_data.ballot_statistics.ballots_used:\n",
    "            print(f\"   Ballots Used: {form_data.ballot_statistics.ballots_used.arabic:,}\")\n",
    "        \n",
    "        # Show vote results count\n",
    "        print(f\"   Vote Results: {len(form_data.vote_results)} entries\")\n",
    "        \n",
    "        # Show total votes if available\n",
    "        if form_data.total_votes_recorded:\n",
    "            print(f\"   Total Votes: {form_data.total_votes_recorded.arabic:,}\")\n",
    "        \n",
    "        # Show officials count if available\n",
    "        if form_data.officials:\n",
    "            print(f\"   Officials: {len(form_data.officials)} members\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Report #{idx} - Pydantic validation FAILED\")\n",
    "        print(f\"   Error: {e}\")\n",
    "        \n",
    "        # Show which field caused the error\n",
    "        import traceback\n",
    "        error_details = traceback.format_exc()\n",
    "        if \"Field required\" in str(e):\n",
    "            print(f\"   Hint: Missing required field\")\n",
    "        elif \"validation error\" in str(e).lower():\n",
    "            print(f\"   Hint: Data type mismatch\")\n",
    "        \n",
    "        # Show first few lines of error for debugging\n",
    "        error_lines = error_details.split('\\n')\n",
    "        relevant_lines = [line for line in error_lines if 'Field' in line or 'validation' in line.lower()]\n",
    "        if relevant_lines:\n",
    "            print(f\"   Details: {relevant_lines[0][:100]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Validate with Pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Report #1 - Pydantic validation PASSED\n",
      "   Form Type: Constituency\n",
      "   District: ‡∏ß‡∏±‡∏á‡∏ä‡∏¥‡πâ‡∏ô\n",
      "   Vote Results: 7 entries\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Validate each report\n",
    "for idx, report_data in enumerate(result, 1):\n",
    "    try:\n",
    "        form_data = ElectionFormData(**report_data)\n",
    "        print(f\"‚úÖ Report #{idx} - Pydantic validation PASSED\")\n",
    "        print(f\"   Form Type: {form_data.form_info.form_type}\")\n",
    "        print(f\"   District: {form_data.form_info.district}\")\n",
    "        print(f\"   Vote Results: {len(form_data.vote_results)} entries\")\n",
    "        print()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Report #{idx} - Pydantic validation FAILED\")\n",
    "        print(f\"   Error: {e}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save to JSON with comprehensive metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'usage_metadata'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mresult\u001b[49m\u001b[43m.\u001b[49m\u001b[43musage_metadata\u001b[49m\n",
      "\u001b[31mAttributeError\u001b[39m: 'list' object has no attribute 'usage_metadata'"
     ]
    }
   ],
   "source": [
    "result.usage_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Results saved to: extracted_data_1_j0DNaqCXIkEk0MK3y0J1eCN3hUOCXeF.json\n",
      "\n",
      "üì¶ Saved data includes:\n",
      "   ‚úÖ Source file metadata\n",
      "   ‚úÖ Extraction metadata (model, timestamp, reports count)\n",
      "   ‚úÖ Usage metadata (tokens, config)\n",
      "   ‚úÖ Extracted election data\n",
      "\n",
      "üìä Token Usage Summary:\n",
      "   Model: gemini-3-flash-preview\n",
      "   Input tokens: None\n",
      "   Output tokens: 1046\n",
      "   Total tokens: 2339\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'None'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 41\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   Total tokens: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00musage_metadata[\u001b[33m'\u001b[39m\u001b[33mtotal_token_count\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# Estimate cost (example rates - update with actual pricing)\u001b[39;00m\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# Gemini Pro: $0.00025/1K input, $0.0005/1K output\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m input_cost = (\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43musage_metadata\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mprompt_token_count\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m / \u001b[32m1000\u001b[39m) * \u001b[32m0.00025\u001b[39m\n\u001b[32m     42\u001b[39m output_cost = (\u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mstr\u001b[39m(usage_metadata[\u001b[33m'\u001b[39m\u001b[33mcandidates_token_count\u001b[39m\u001b[33m'\u001b[39m])) / \u001b[32m1000\u001b[39m) * \u001b[32m0.0005\u001b[39m\n\u001b[32m     43\u001b[39m total_cost = input_cost + output_cost\n",
      "\u001b[31mValueError\u001b[39m: invalid literal for int() with base 10: 'None'"
     ]
    }
   ],
   "source": [
    "output_file = f\"extracted_data_{test_file['file_id']}.json\"\n",
    "\n",
    "output_data = {\n",
    "    \"source_file\": {\n",
    "        \"file_id\": test_file['file_id'],\n",
    "        \"path\": test_file['path'],\n",
    "        \"province_name\": test_file['province_name'],\n",
    "        \"size_bytes\": test_file['size'],\n",
    "        \"size_mb\": test_file['size_mb'],\n",
    "        \"mod_time\": test_file['mod_time'],\n",
    "    },\n",
    "    \"extraction_metadata\": {\n",
    "        \"model\": usage_metadata.get('model'),\n",
    "        \"timestamp\": pd.Timestamp.now().isoformat(),\n",
    "        \"reports_extracted\": len(result),\n",
    "    },\n",
    "    \"usage_metadata\": usage_metadata,\n",
    "    \"extracted_data\": result,\n",
    "}\n",
    "\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(output_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"üíæ Results saved to: {output_file}\")\n",
    "print(f\"\\nüì¶ Saved data includes:\")\n",
    "print(f\"   ‚úÖ Source file metadata\")\n",
    "print(f\"   ‚úÖ Extraction metadata (model, timestamp, reports count)\")\n",
    "print(f\"   ‚úÖ Usage metadata (tokens, config)\")\n",
    "print(f\"   ‚úÖ Extracted election data\")\n",
    "\n",
    "# Display usage summary\n",
    "if usage_metadata.get('total_token_count'):\n",
    "    print(f\"\\nüìä Token Usage Summary:\")\n",
    "    print(f\"   Model: {usage_metadata['model']}\")\n",
    "    print(f\"   Input tokens: {usage_metadata['prompt_token_count']}\")\n",
    "    print(f\"   Output tokens: {usage_metadata['candidates_token_count']}\")\n",
    "    print(f\"   Total tokens: {usage_metadata['total_token_count']}\")\n",
    "    \n",
    "    # Estimate cost (example rates - update with actual pricing)\n",
    "    # Gemini Pro: $0.00025/1K input, $0.0005/1K output\n",
    "    input_cost = (int(str(usage_metadata['prompt_token_count'])) / 1000) * 0.00025\n",
    "    output_cost = (int(str(usage_metadata['candidates_token_count'])) / 1000) * 0.0005\n",
    "    total_cost = input_cost + output_cost\n",
    "    \n",
    "    print(f\"\\nüí∞ Estimated Cost (Gemini Pro rates):\")\n",
    "    print(f\"   Input: ${input_cost:.6f}\")\n",
    "    print(f\"   Output: ${output_cost:.6f}\")\n",
    "    print(f\"   Total: ${total_cost:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to JSON\n",
    "\n",
    "output_file = f\"extracted_data_{test_file['file_id']}.json\"\n",
    "\n",
    "output_data = {\n",
    "    \"source_file\": test_file,\n",
    "    \"extracted_data\": result,\n",
    "    \"model\": MODEL_NAME,\n",
    "}\n",
    "\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(output_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"üíæ Results saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def batch_process_from_bigquery(\n    limit: int = 5,\n    province: Optional[str] = None,\n    min_size_kb: float = 50.0,\n    max_size_mb: float = 30.0,\n    model: str = MODEL_NAME,\n    run_evaluation: bool = True\n) -> list[dict]:\n    \"\"\"\n    Batch process PDF files from BigQuery with integrated evaluation.\n    \n    Args:\n        limit: Maximum number of files to process\n        province: Filter by province\n        min_size_kb: Minimum file size in KB (default: 50 KB)\n        max_size_mb: Max file size in MB\n        model: Gemini model name\n        run_evaluation: Whether to run comprehensive evaluation on each result\n    \n    Returns:\n        List of results with metadata, evaluation, and usage stats\n    \"\"\"\n    # Query files\n    files = query_pdf_files(\n        limit=limit, \n        province=province, \n        min_size_kb=min_size_kb,\n        max_size_mb=max_size_mb\n    )\n    \n    all_results = []\n    successful_count = 0\n    failed_count = 0\n    \n    # Summary statistics\n    total_quality_score = 0.0\n    total_reports_extracted = 0\n    total_input_tokens = 0\n    total_output_tokens = 0\n    total_cached_tokens = 0\n    total_thoughts_tokens = 0\n    total_cost = 0.0\n    \n    for i, file_info in enumerate(files, 1):\n        print(f\"\\n{'='*80}\")\n        print(f\"Processing {i}/{len(files)}: {file_info['path']}\")\n        print(f\"  File: {file_info['file_id']}\")\n        print(f\"  Size: {file_info['size_kb']:.1f} KB ({file_info['size_mb']:.2f} MB)\")\n        print(f\"{'='*80}\")\n        \n        try:\n            # Construct Drive URI\n            drive_uri = f\"https://drive.google.com/uc?export=download&id={file_info['file_id']}\"\n            \n            # Extract (now returns data + usage metadata)\n            extraction_result, usage_meta = extract_from_drive_url(drive_uri, model=model)\n            \n            # Accumulate token usage\n            if usage_meta.get('total_token_count'):\n                total_input_tokens += usage_meta.get('prompt_token_count', 0)\n                total_output_tokens += usage_meta.get('candidates_token_count', 0)\n                total_cached_tokens += usage_meta.get('cached_content_token_count', 0)\n                total_thoughts_tokens += usage_meta.get('thoughts_token_count', 0)\n                \n                # Calculate cost (Gemini Pro rates)\n                # Note: Cached tokens are usually free or cheaper\n                file_cost = (usage_meta.get('prompt_token_count', 0) / 1000) * 0.00025\n                file_cost += (usage_meta.get('candidates_token_count', 0) / 1000) * 0.0005\n                total_cost += file_cost\n            \n            # Run evaluation if enabled\n            evaluations = []\n            if run_evaluation and extraction_result:\n                print(f\"\\nüìä Running evaluation...\")\n                \n                for report_idx, report_data in enumerate(extraction_result):\n                    eval_result = evaluate_extraction(report_data, report_index=report_idx)\n                    evaluations.append(eval_result.to_dict())\n                    \n                    # Print compact summary\n                    status_icon = \"‚úÖ\" if eval_result.is_valid else \"‚ùå\"\n                    print(f\"   {status_icon} Report {report_idx + 1}: Quality={eval_result.quality_score:.1%}, \"\n                          f\"Passed={eval_result.checks_passed}/{eval_result.total_checks}\")\n                    \n                    # Update statistics\n                    total_quality_score += eval_result.quality_score\n                    total_reports_extracted += 1\n            \n            # Store result with usage metadata\n            all_results.append({\n                \"file_info\": file_info,\n                \"success\": True,\n                \"data\": extraction_result,\n                \"evaluations\": evaluations,\n                \"usage_metadata\": usage_meta,\n                \"reports_count\": len(extraction_result),\n            })\n            successful_count += 1\n            \n        except Exception as e:\n            print(f\"\\n‚ùå Error: {e}\")\n            import traceback\n            traceback.print_exc()\n            \n            all_results.append({\n                \"file_info\": file_info,\n                \"success\": False,\n                \"error\": str(e),\n                \"error_type\": type(e).__name__,\n            })\n            failed_count += 1\n    \n    # Print batch summary\n    print(f\"\\n{'='*80}\")\n    print(f\"BATCH PROCESSING SUMMARY\")\n    print(f\"{'='*80}\")\n    print(f\"\\nüìä Processing Statistics:\")\n    print(f\"   Total Files: {len(files)}\")\n    print(f\"   ‚úÖ Successful: {successful_count}\")\n    print(f\"   ‚ùå Failed: {failed_count}\")\n    print(f\"   Success Rate: {successful_count/len(files)*100:.1f}%\")\n    \n    if run_evaluation and total_reports_extracted > 0:\n        avg_quality = total_quality_score / total_reports_extracted\n        print(f\"\\nüìà Quality Metrics:\")\n        print(f\"   Total Reports Extracted: {total_reports_extracted}\")\n        print(f\"   Average Quality Score: {avg_quality:.1%}\")\n        \n        # Count valid vs invalid reports\n        all_evaluations = [e for r in all_results if r.get('success') for e in r.get('evaluations', [])]\n        valid_reports = sum(1 for e in all_evaluations if e['is_valid'])\n        invalid_reports = len(all_evaluations) - valid_reports\n        \n        print(f\"   Valid Reports: {valid_reports}/{len(all_evaluations)}\")\n        print(f\"   Invalid Reports: {invalid_reports}/{len(all_evaluations)}\")\n    \n    # Display token usage summary\n    if total_input_tokens > 0:\n        print(f\"\\nüí∞ Token Usage & Cost Summary:\")\n        print(f\"   Input Tokens: {total_input_tokens:,}\")\n        print(f\"   Output Tokens: {total_output_tokens:,}\")\n        \n        if total_cached_tokens > 0:\n            print(f\"   Cached Tokens: {total_cached_tokens:,} (reused from cache)\")\n        \n        if total_thoughts_tokens > 0:\n            print(f\"   Thoughts Tokens: {total_thoughts_tokens:,} (reasoning)\")\n        \n        print(f\"   Total Tokens: {total_input_tokens + total_output_tokens:,}\")\n        print(f\"   Estimated Total Cost: ${total_cost:.4f}\")\n        \n        if successful_count > 0:\n            avg_tokens = (total_input_tokens + total_output_tokens) / successful_count\n            avg_cost = total_cost / successful_count\n            print(f\"   Average per File: {avg_tokens:,.0f} tokens, ${avg_cost:.4f}\")\n    \n    return all_results\n\n\nprint(\"‚úÖ Enhanced batch processing function with comprehensive usage tracking\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Save batch results with evaluations and comprehensive usage metadata\nif batch_results:\n    # Calculate aggregate usage statistics\n    total_input_tokens = 0\n    total_output_tokens = 0\n    total_cached_tokens = 0\n    total_thoughts_tokens = 0\n    total_cost = 0.0\n    \n    for r in batch_results:\n        if r.get('success') and r.get('usage_metadata'):\n            usage = r['usage_metadata']\n            total_input_tokens += usage.get('prompt_token_count', 0)\n            total_output_tokens += usage.get('candidates_token_count', 0)\n            total_cached_tokens += usage.get('cached_content_token_count', 0)\n            total_thoughts_tokens += usage.get('thoughts_token_count', 0)\n    \n    # Calculate total cost\n    total_cost = (total_input_tokens / 1000) * 0.00025 + (total_output_tokens / 1000) * 0.0005\n    \n    # Prepare output data\n    output_data = {\n        \"metadata\": {\n            \"model\": MODEL_NAME,\n            \"timestamp\": pd.Timestamp.now().isoformat(),\n            \"total_files\": len(batch_results),\n            \"successful\": sum(1 for r in batch_results if r.get('success')),\n            \"failed\": sum(1 for r in batch_results if not r.get('success')),\n            \"total_reports\": sum(r.get('reports_count', 0) for r in batch_results if r.get('success')),\n        },\n        \"usage_summary\": {\n            \"total_input_tokens\": total_input_tokens,\n            \"total_output_tokens\": total_output_tokens,\n            \"total_cached_tokens\": total_cached_tokens,\n            \"total_thoughts_tokens\": total_thoughts_tokens,\n            \"total_tokens\": total_input_tokens + total_output_tokens,\n            \"estimated_cost_usd\": round(total_cost, 6),\n            \"average_tokens_per_file\": round((total_input_tokens + total_output_tokens) / len(batch_results), 2) if batch_results else 0,\n        },\n        \"results\": batch_results\n    }\n    \n    # Calculate aggregate quality metrics\n    if any(r.get('evaluations') for r in batch_results):\n        all_evals = [e for r in batch_results if r.get('success') for e in r.get('evaluations', [])]\n        \n        if all_evals:\n            output_data[\"metadata\"][\"quality_metrics\"] = {\n                \"average_quality_score\": sum(e['quality_score'] for e in all_evals) / len(all_evals),\n                \"valid_reports\": sum(1 for e in all_evals if e['is_valid']),\n                \"invalid_reports\": sum(1 for e in all_evals if not e['is_valid']),\n                \"total_checks_run\": sum(e['total_checks'] for e in all_evals),\n                \"total_passed\": sum(e['checks_passed'] for e in all_evals),\n                \"total_failed\": sum(e['checks_failed'] for e in all_evals),\n                \"total_warnings\": sum(e['checks_warning'] for e in all_evals),\n            }\n    \n    # Save to file\n    output_file = f\"batch_results_{len(batch_results)}files.json\"\n    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n        json.dump(output_data, f, ensure_ascii=False, indent=2)\n    \n    print(f\"\\nüíæ Batch results saved to: {output_file}\")\n    \n    # Display quality metrics\n    if \"quality_metrics\" in output_data[\"metadata\"]:\n        metrics = output_data[\"metadata\"][\"quality_metrics\"]\n        print(f\"\\nüìà Quality Metrics:\")\n        print(f\"   Average Quality Score: {metrics['average_quality_score']:.1%}\")\n        print(f\"   Valid Reports: {metrics['valid_reports']}/{metrics['valid_reports'] + metrics['invalid_reports']}\")\n        print(f\"   Total Checks Run: {metrics['total_checks_run']}\")\n        print(f\"   Overall Pass Rate: {metrics['total_passed']/metrics['total_checks_run']*100:.1f}%\")\n    \n    # Display usage summary\n    if total_input_tokens > 0:\n        print(f\"\\nüí∞ Token Usage & Cost Summary:\")\n        print(f\"   Total Input Tokens: {total_input_tokens:,}\")\n        print(f\"   Total Output Tokens: {total_output_tokens:,}\")\n        \n        if total_cached_tokens > 0:\n            print(f\"   Total Cached Tokens: {total_cached_tokens:,}\")\n        \n        if total_thoughts_tokens > 0:\n            print(f\"   Total Thoughts Tokens: {total_thoughts_tokens:,}\")\n        \n        print(f\"   Total Tokens: {total_input_tokens + total_output_tokens:,}\")\n        print(f\"   Estimated Total Cost: ${total_cost:.4f}\")\n        print(f\"   Average per File: {(total_input_tokens + total_output_tokens)/len(batch_results):,.0f} tokens, ${total_cost/len(batch_results):.4f}\")\n    \n    # Create summary DataFrame\n    summary_data = []\n    for r in batch_results:\n        row = {\n            \"File\": r['file_info']['path'].split('/')[-1] if r.get('file_info') else 'Unknown',\n            \"Province\": r['file_info']['province_name'] if r.get('file_info') else 'N/A',\n            \"Size (KB)\": r['file_info']['size_kb'] if r.get('file_info') else 0,\n            \"Success\": \"‚úÖ\" if r.get('success') else \"‚ùå\",\n            \"Reports\": r.get('reports_count', 0),\n        }\n        \n        # Add evaluation metrics if available\n        if r.get('evaluations'):\n            avg_quality = sum(e['quality_score'] for e in r['evaluations']) / len(r['evaluations'])\n            row[\"Quality\"] = f\"{avg_quality:.1%}\"\n            row[\"Valid\"] = sum(1 for e in r['evaluations'] if e['is_valid'])\n        else:\n            row[\"Quality\"] = \"N/A\"\n            row[\"Valid\"] = \"N/A\"\n        \n        # Add token usage if available\n        if r.get('usage_metadata'):\n            usage = r['usage_metadata']\n            row[\"Tokens\"] = usage.get('total_token_count', 0)\n            \n            # Add cached and thoughts if available\n            if usage.get('cached_content_token_count'):\n                row[\"Cached\"] = usage.get('cached_content_token_count', 0)\n            if usage.get('thoughts_token_count'):\n                row[\"Thoughts\"] = usage.get('thoughts_token_count', 0)\n        else:\n            row[\"Tokens\"] = 0\n        \n        summary_data.append(row)\n    \n    print(f\"\\nüìä Batch Processing Summary Table:\")\n    df_summary = pd.DataFrame(summary_data)\n    display(df_summary)\n    \n    # Save summary as CSV\n    summary_csv = f\"batch_summary_{len(batch_results)}files.csv\"\n    df_summary.to_csv(summary_csv, index=False, encoding='utf-8-sig')\n    print(f\"\\nüíæ Summary CSV saved to: {summary_csv}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def batch_process_from_bigquery(\n",
    "    limit: int = 5,\n",
    "    province: Optional[str] = None,\n",
    "    min_size_kb: float = 50.0,\n",
    "    max_size_mb: float = 30.0,\n",
    "    model: str = MODEL_NAME,\n",
    "    run_evaluation: bool = True\n",
    ") -> list[dict]:\n",
    "    \"\"\"\n",
    "    Batch process PDF files from BigQuery with integrated evaluation.\n",
    "    \n",
    "    Args:\n",
    "        limit: Maximum number of files to process\n",
    "        province: Filter by province\n",
    "        min_size_kb: Minimum file size in KB (default: 50 KB)\n",
    "        max_size_mb: Max file size in MB\n",
    "        model: Gemini model name\n",
    "        run_evaluation: Whether to run comprehensive evaluation on each result\n",
    "    \n",
    "    Returns:\n",
    "        List of results with metadata, evaluation, and usage stats\n",
    "    \"\"\"\n",
    "    # Query files\n",
    "    files = query_pdf_files(\n",
    "        limit=limit, \n",
    "        province=province, \n",
    "        min_size_kb=min_size_kb,\n",
    "        max_size_mb=max_size_mb\n",
    "    )\n",
    "    \n",
    "    all_results = []\n",
    "    successful_count = 0\n",
    "    failed_count = 0\n",
    "    \n",
    "    # Summary statistics\n",
    "    total_quality_score = 0.0\n",
    "    total_reports_extracted = 0\n",
    "    total_input_tokens = 0\n",
    "    total_output_tokens = 0\n",
    "    total_cost = 0.0\n",
    "    \n",
    "    for i, file_info in enumerate(files, 1):\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Processing {i}/{len(files)}: {file_info['path']}\")\n",
    "        print(f\"  File: {file_info['file_id']}\")\n",
    "        print(f\"  Size: {file_info['size_kb']:.1f} KB ({file_info['size_mb']:.2f} MB)\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        try:\n",
    "            # Construct Drive URI\n",
    "            drive_uri = f\"https://drive.google.com/uc?export=download&id={file_info['file_id']}\"\n",
    "            \n",
    "            # Extract (now returns data + usage metadata)\n",
    "            extraction_result, usage_meta = extract_from_drive_url(drive_uri, model=model)\n",
    "            \n",
    "            # Accumulate token usage\n",
    "            if usage_meta.get('total_token_count'):\n",
    "                total_input_tokens += usage_meta.get('prompt_token_count', 0)\n",
    "                total_output_tokens += usage_meta.get('candidates_token_count', 0)\n",
    "                \n",
    "                # Calculate cost (Gemini Pro rates)\n",
    "                file_cost = (usage_meta.get('prompt_token_count', 0) / 1000) * 0.00025\n",
    "                file_cost += (usage_meta.get('candidates_token_count', 0) / 1000) * 0.0005\n",
    "                total_cost += file_cost\n",
    "            \n",
    "            # Run evaluation if enabled\n",
    "            evaluations = []\n",
    "            if run_evaluation and extraction_result:\n",
    "                print(f\"\\nüìä Running evaluation...\")\n",
    "                \n",
    "                for report_idx, report_data in enumerate(extraction_result):\n",
    "                    eval_result = evaluate_extraction(report_data, report_index=report_idx)\n",
    "                    evaluations.append(eval_result.to_dict())\n",
    "                    \n",
    "                    # Print compact summary\n",
    "                    status_icon = \"‚úÖ\" if eval_result.is_valid else \"‚ùå\"\n",
    "                    print(f\"   {status_icon} Report {report_idx + 1}: Quality={eval_result.quality_score:.1%}, \"\n",
    "                          f\"Passed={eval_result.checks_passed}/{eval_result.total_checks}\")\n",
    "                    \n",
    "                    # Update statistics\n",
    "                    total_quality_score += eval_result.quality_score\n",
    "                    total_reports_extracted += 1\n",
    "            \n",
    "            # Store result with usage metadata\n",
    "            all_results.append({\n",
    "                \"file_info\": file_info,\n",
    "                \"success\": True,\n",
    "                \"data\": extraction_result,\n",
    "                \"evaluations\": evaluations,\n",
    "                \"usage_metadata\": usage_meta,\n",
    "                \"reports_count\": len(extraction_result),\n",
    "            })\n",
    "            successful_count += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚ùå Error: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            \n",
    "            all_results.append({\n",
    "                \"file_info\": file_info,\n",
    "                \"success\": False,\n",
    "                \"error\": str(e),\n",
    "                \"error_type\": type(e).__name__,\n",
    "            })\n",
    "            failed_count += 1\n",
    "    \n",
    "    # Print batch summary\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"BATCH PROCESSING SUMMARY\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"\\nüìä Processing Statistics:\")\n",
    "    print(f\"   Total Files: {len(files)}\")\n",
    "    print(f\"   ‚úÖ Successful: {successful_count}\")\n",
    "    print(f\"   ‚ùå Failed: {failed_count}\")\n",
    "    print(f\"   Success Rate: {successful_count/len(files)*100:.1f}%\")\n",
    "    \n",
    "    if run_evaluation and total_reports_extracted > 0:\n",
    "        avg_quality = total_quality_score / total_reports_extracted\n",
    "        print(f\"\\nüìà Quality Metrics:\")\n",
    "        print(f\"   Total Reports Extracted: {total_reports_extracted}\")\n",
    "        print(f\"   Average Quality Score: {avg_quality:.1%}\")\n",
    "        \n",
    "        # Count valid vs invalid reports\n",
    "        all_evaluations = [e for r in all_results if r.get('success') for e in r.get('evaluations', [])]\n",
    "        valid_reports = sum(1 for e in all_evaluations if e['is_valid'])\n",
    "        invalid_reports = len(all_evaluations) - valid_reports\n",
    "        \n",
    "        print(f\"   Valid Reports: {valid_reports}/{len(all_evaluations)}\")\n",
    "        print(f\"   Invalid Reports: {invalid_reports}/{len(all_evaluations)}\")\n",
    "    \n",
    "    # Display token usage summary\n",
    "    if total_input_tokens > 0:\n",
    "        print(f\"\\nüí∞ Token Usage & Cost Summary:\")\n",
    "        print(f\"   Total Input Tokens: {total_input_tokens:,}\")\n",
    "        print(f\"   Total Output Tokens: {total_output_tokens:,}\")\n",
    "        print(f\"   Total Tokens: {total_input_tokens + total_output_tokens:,}\")\n",
    "        print(f\"   Estimated Total Cost: ${total_cost:.4f}\")\n",
    "        print(f\"   Average Cost per File: ${total_cost/successful_count:.4f}\" if successful_count > 0 else \"\")\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "\n",
    "print(\"‚úÖ Enhanced batch processing function with usage tracking defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save batch results with evaluations and usage metadata\n",
    "if batch_results:\n",
    "    # Calculate aggregate usage statistics\n",
    "    total_input_tokens = 0\n",
    "    total_output_tokens = 0\n",
    "    total_cost = 0.0\n",
    "    \n",
    "    for r in batch_results:\n",
    "        if r.get('success') and r.get('usage_metadata'):\n",
    "            usage = r['usage_metadata']\n",
    "            total_input_tokens += usage.get('prompt_token_count', 0)\n",
    "            total_output_tokens += usage.get('candidates_token_count', 0)\n",
    "    \n",
    "    # Calculate total cost\n",
    "    total_cost = (total_input_tokens / 1000) * 0.00025 + (total_output_tokens / 1000) * 0.0005\n",
    "    \n",
    "    # Prepare output data\n",
    "    output_data = {\n",
    "        \"metadata\": {\n",
    "            \"model\": MODEL_NAME,\n",
    "            \"timestamp\": pd.Timestamp.now().isoformat(),\n",
    "            \"total_files\": len(batch_results),\n",
    "            \"successful\": sum(1 for r in batch_results if r.get('success')),\n",
    "            \"failed\": sum(1 for r in batch_results if not r.get('success')),\n",
    "            \"total_reports\": sum(r.get('reports_count', 0) for r in batch_results if r.get('success')),\n",
    "        },\n",
    "        \"usage_summary\": {\n",
    "            \"total_input_tokens\": total_input_tokens,\n",
    "            \"total_output_tokens\": total_output_tokens,\n",
    "            \"total_tokens\": total_input_tokens + total_output_tokens,\n",
    "            \"estimated_cost_usd\": round(total_cost, 6),\n",
    "            \"average_tokens_per_file\": round((total_input_tokens + total_output_tokens) / len(batch_results), 2) if batch_results else 0,\n",
    "        },\n",
    "        \"results\": batch_results\n",
    "    }\n",
    "    \n",
    "    # Calculate aggregate quality metrics\n",
    "    if any(r.get('evaluations') for r in batch_results):\n",
    "        all_evals = [e for r in batch_results if r.get('success') for e in r.get('evaluations', [])]\n",
    "        \n",
    "        if all_evals:\n",
    "            output_data[\"metadata\"][\"quality_metrics\"] = {\n",
    "                \"average_quality_score\": sum(e['quality_score'] for e in all_evals) / len(all_evals),\n",
    "                \"valid_reports\": sum(1 for e in all_evals if e['is_valid']),\n",
    "                \"invalid_reports\": sum(1 for e in all_evals if not e['is_valid']),\n",
    "                \"total_checks_run\": sum(e['total_checks'] for e in all_evals),\n",
    "                \"total_passed\": sum(e['checks_passed'] for e in all_evals),\n",
    "                \"total_failed\": sum(e['checks_failed'] for e in all_evals),\n",
    "                \"total_warnings\": sum(e['checks_warning'] for e in all_evals),\n",
    "            }\n",
    "    \n",
    "    # Save to file\n",
    "    output_file = f\"batch_results_{len(batch_results)}files.json\"\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(output_data, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"\\nüíæ Batch results saved to: {output_file}\")\n",
    "    \n",
    "    # Display quality metrics\n",
    "    if \"quality_metrics\" in output_data[\"metadata\"]:\n",
    "        metrics = output_data[\"metadata\"][\"quality_metrics\"]\n",
    "        print(f\"\\nüìà Quality Metrics:\")\n",
    "        print(f\"   Average Quality Score: {metrics['average_quality_score']:.1%}\")\n",
    "        print(f\"   Valid Reports: {metrics['valid_reports']}/{metrics['valid_reports'] + metrics['invalid_reports']}\")\n",
    "        print(f\"   Total Checks Run: {metrics['total_checks_run']}\")\n",
    "        print(f\"   Overall Pass Rate: {metrics['total_passed']/metrics['total_checks_run']*100:.1f}%\")\n",
    "    \n",
    "    # Display usage summary\n",
    "    if total_input_tokens > 0:\n",
    "        print(f\"\\nüí∞ Token Usage & Cost Summary:\")\n",
    "        print(f\"   Total Input Tokens: {total_input_tokens:,}\")\n",
    "        print(f\"   Total Output Tokens: {total_output_tokens:,}\")\n",
    "        print(f\"   Total Tokens: {total_input_tokens + total_output_tokens:,}\")\n",
    "        print(f\"   Estimated Total Cost: ${total_cost:.4f}\")\n",
    "        print(f\"   Average per File: {(total_input_tokens + total_output_tokens)/len(batch_results):,.0f} tokens, ${total_cost/len(batch_results):.4f}\")\n",
    "    \n",
    "    # Create summary DataFrame\n",
    "    summary_data = []\n",
    "    for r in batch_results:\n",
    "        row = {\n",
    "            \"File\": r['file_info']['path'].split('/')[-1] if r.get('file_info') else 'Unknown',\n",
    "            \"Province\": r['file_info']['province_name'] if r.get('file_info') else 'N/A',\n",
    "            \"Size (KB)\": r['file_info']['size_kb'] if r.get('file_info') else 0,\n",
    "            \"Success\": \"‚úÖ\" if r.get('success') else \"‚ùå\",\n",
    "            \"Reports\": r.get('reports_count', 0),\n",
    "        }\n",
    "        \n",
    "        # Add evaluation metrics if available\n",
    "        if r.get('evaluations'):\n",
    "            avg_quality = sum(e['quality_score'] for e in r['evaluations']) / len(r['evaluations'])\n",
    "            row[\"Quality\"] = f\"{avg_quality:.1%}\"\n",
    "            row[\"Valid\"] = sum(1 for e in r['evaluations'] if e['is_valid'])\n",
    "        else:\n",
    "            row[\"Quality\"] = \"N/A\"\n",
    "            row[\"Valid\"] = \"N/A\"\n",
    "        \n",
    "        # Add token usage if available\n",
    "        if r.get('usage_metadata'):\n",
    "            usage = r['usage_metadata']\n",
    "            row[\"Tokens\"] = usage.get('total_token_count', 0)\n",
    "        else:\n",
    "            row[\"Tokens\"] = 0\n",
    "        \n",
    "        summary_data.append(row)\n",
    "    \n",
    "    print(f\"\\nüìä Batch Processing Summary Table:\")\n",
    "    df_summary = pd.DataFrame(summary_data)\n",
    "    display(df_summary)\n",
    "    \n",
    "    # Save summary as CSV\n",
    "    summary_csv = f\"batch_summary_{len(batch_results)}files.csv\"\n",
    "    df_summary.to_csv(summary_csv, index=False, encoding='utf-8-sig')\n",
    "    print(f\"\\nüíæ Summary CSV saved to: {summary_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save batch results with evaluations\n",
    "if batch_results:\n",
    "    # Prepare output data\n",
    "    output_data = {\n",
    "        \"metadata\": {\n",
    "            \"model\": MODEL_NAME,\n",
    "            \"total_files\": len(batch_results),\n",
    "            \"successful\": sum(1 for r in batch_results if r.get('success')),\n",
    "            \"failed\": sum(1 for r in batch_results if not r.get('success')),\n",
    "            \"total_reports\": sum(r.get('reports_count', 0) for r in batch_results if r.get('success')),\n",
    "        },\n",
    "        \"results\": batch_results\n",
    "    }\n",
    "    \n",
    "    # Calculate aggregate metrics\n",
    "    if any(r.get('evaluations') for r in batch_results):\n",
    "        all_evals = [e for r in batch_results if r.get('success') for e in r.get('evaluations', [])]\n",
    "        \n",
    "        if all_evals:\n",
    "            output_data[\"metadata\"][\"quality_metrics\"] = {\n",
    "                \"average_quality_score\": sum(e['quality_score'] for e in all_evals) / len(all_evals),\n",
    "                \"valid_reports\": sum(1 for e in all_evals if e['is_valid']),\n",
    "                \"invalid_reports\": sum(1 for e in all_evals if not e['is_valid']),\n",
    "                \"total_checks_run\": sum(e['total_checks'] for e in all_evals),\n",
    "                \"total_passed\": sum(e['checks_passed'] for e in all_evals),\n",
    "                \"total_failed\": sum(e['checks_failed'] for e in all_evals),\n",
    "                \"total_warnings\": sum(e['checks_warning'] for e in all_evals),\n",
    "            }\n",
    "    \n",
    "    # Save to file\n",
    "    output_file = f\"batch_results_{len(batch_results)}files.json\"\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(output_data, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"\\nüíæ Batch results saved to: {output_file}\")\n",
    "    \n",
    "    # Display quality metrics\n",
    "    if \"quality_metrics\" in output_data[\"metadata\"]:\n",
    "        metrics = output_data[\"metadata\"][\"quality_metrics\"]\n",
    "        print(f\"\\nüìà Aggregate Quality Metrics:\")\n",
    "        print(f\"   Average Quality Score: {metrics['average_quality_score']:.1%}\")\n",
    "        print(f\"   Valid Reports: {metrics['valid_reports']}/{metrics['valid_reports'] + metrics['invalid_reports']}\")\n",
    "        print(f\"   Total Checks Run: {metrics['total_checks_run']}\")\n",
    "        print(f\"   Overall Pass Rate: {metrics['total_passed']/metrics['total_checks_run']*100:.1f}%\")\n",
    "    \n",
    "    # Create summary DataFrame\n",
    "    summary_data = []\n",
    "    for r in batch_results:\n",
    "        row = {\n",
    "            \"File\": r['file_info']['path'].split('/')[-1] if r.get('file_info') else 'Unknown',\n",
    "            \"Province\": r['file_info']['province_name'] if r.get('file_info') else 'N/A',\n",
    "            \"Size (KB)\": r['file_info']['size_kb'] if r.get('file_info') else 0,\n",
    "            \"Success\": \"‚úÖ\" if r.get('success') else \"‚ùå\",\n",
    "            \"Reports\": r.get('reports_count', 0),\n",
    "        }\n",
    "        \n",
    "        # Add evaluation metrics if available\n",
    "        if r.get('evaluations'):\n",
    "            avg_quality = sum(e['quality_score'] for e in r['evaluations']) / len(r['evaluations'])\n",
    "            row[\"Quality\"] = f\"{avg_quality:.1%}\"\n",
    "            row[\"Valid\"] = sum(1 for e in r['evaluations'] if e['is_valid'])\n",
    "        else:\n",
    "            row[\"Quality\"] = \"N/A\"\n",
    "            row[\"Valid\"] = \"N/A\"\n",
    "        \n",
    "        summary_data.append(row)\n",
    "    \n",
    "    print(f\"\\nüìä Batch Processing Summary Table:\")\n",
    "    df_summary = pd.DataFrame(summary_data)\n",
    "    display(df_summary)\n",
    "    \n",
    "    # Save summary as CSV\n",
    "    summary_csv = f\"batch_summary_{len(batch_results)}files.csv\"\n",
    "    df_summary.to_csv(summary_csv, index=False, encoding='utf-8-sig')\n",
    "    print(f\"\\nüíæ Summary CSV saved to: {summary_csv}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Save Batch Results with Evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrates the **most efficient workflow** for vote extraction:\n",
    "\n",
    "‚úÖ **BigQuery Integration** - Query metadata to find files  \n",
    "‚úÖ **Google Drive Direct Access** - No local downloads needed!  \n",
    "‚úÖ **External URLs Method** - Gemini fetches files directly  \n",
    "‚úÖ **Structured Output** - Guaranteed JSON schema  \n",
    "‚úÖ **Pydantic Validation** - Type-safe data models  \n",
    "‚úÖ **Batch Processing** - Process multiple files efficiently  \n",
    "\n",
    "## Key Advantages Over Local PDF Processing:\n",
    "\n",
    "1. **No Local Storage** - Files stay in Google Drive\n",
    "2. **No PDF Conversion** - Gemini handles PDF directly\n",
    "3. **Faster** - No download/upload overhead\n",
    "4. **Scalable** - Easy to process thousands of files\n",
    "5. **Cost Effective** - No egress charges for data transfer\n",
    "\n",
    "## Next Steps:\n",
    "\n",
    "1. **Process by Province** - Filter BigQuery by province\n",
    "2. **Save to BigQuery** - Store results back in BigQuery\n",
    "3. **Error Handling** - Add retry logic for failed extractions\n",
    "4. **Monitoring** - Track processing status and quality metrics\n",
    "5. **Automation** - Schedule regular batch processing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}