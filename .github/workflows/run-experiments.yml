name: Run LLM Experiments

on:
  # Manual trigger
  workflow_dispatch:
    inputs:
      dataset_name:
        description: 'Dataset name'
        required: true
        default: 'vote-extraction-bangbamru-1-10'
      sample_size:
        description: 'Number of samples to test'
        required: true
        default: '4'
      project_name:
        description: 'Datadog project name'
        required: true
        default: 'vote-extraction-project'

  # Scheduled run (weekly on Monday at 8am UTC)
  schedule:
    - cron: '0 8 * * 1'

  # Run on push to main (optional)
  # push:
  #   branches:
  #     - main
  #   paths:
  #     - 'services/fastapi-backend/**'
  #     - 'notebooks/datasets/**'

env:
  PYTHON_VERSION: '3.11'
  UV_VERSION: '0.1.6'

jobs:
  run-experiments:
    name: Run Model Experiments
    runs-on: ubuntu-latest
    timeout-minutes: 30

    steps:
      - name: ğŸ“¥ Checkout code
        uses: actions/checkout@v4

      - name: ğŸ Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: ğŸ“¦ Install uv
        run: |
          curl -LsSf https://astral.sh/uv/install.sh | sh
          echo "$HOME/.cargo/bin" >> $GITHUB_PATH

      - name: ğŸ”§ Install dependencies
        run: |
          cd services/fastapi-backend
          uv pip install --system -e .
          uv pip install --system httpx python-dotenv "ddtrace>=4.0.0"

      - name: ğŸ” Configure GCP credentials
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY }}

      - name: â˜ï¸ Set up Cloud SDK
        uses: google-github-actions/setup-gcloud@v2

      - name: ğŸƒ Run experiments
        env:
          # Datadog configuration
          DD_API_KEY: ${{ secrets.DD_API_KEY }}
          DD_APP_KEY: ${{ secrets.DD_APP_KEY }}
          DD_SITE: ${{ secrets.DD_SITE || 'datadoghq.com' }}
          DD_ENV: ci
          DD_SERVICE: vote-extractor-experiments
          DD_VERSION: ${{ github.sha }}

          # GCP configuration
          GOOGLE_CLOUD_PROJECT: ${{ secrets.GCP_PROJECT_ID }}
          VERTEX_AI_LOCATION: ${{ secrets.VERTEX_AI_LOCATION || 'us-central1' }}

          # Experiment configuration
          DATASET_NAME: ${{ github.event.inputs.dataset_name || 'vote-extraction-bangbamru-1-10' }}
          SAMPLE_SIZE: ${{ github.event.inputs.sample_size || '4' }}
          PROJECT_NAME: ${{ github.event.inputs.project_name || 'vote-extraction-project' }}

        run: |
          python -c "
          import os
          import sys
          from ddtrace.llmobs import LLMObs
          from ddtrace.llmobs.decorators import workflow
          import httpx
          from pathlib import Path
          
          # Configuration from environment
          DATASET_NAME = os.getenv('DATASET_NAME', 'vote-extraction-bangbamru-1-10')
          SAMPLE_SIZE = int(os.getenv('SAMPLE_SIZE', '4'))
          PROJECT_NAME = os.getenv('PROJECT_NAME', 'vote-extraction-project')
          
          print(f'ğŸ”¬ Running experiments on dataset: {DATASET_NAME}')
          print(f'ğŸ“Š Sample size: {SAMPLE_SIZE}')
          print(f'ğŸ“ Project: {PROJECT_NAME}')
          
          # Initialize LLMObs
          LLMObs.enable(
              ml_app='vote-extractor',
              api_key=os.getenv('DD_API_KEY'),
              site=os.getenv('DD_SITE', 'datadoghq.com'),
              agentless_enabled=True,
          )
          print('âœ… LLMObs enabled')
          
          # Pull dataset
          try:
              dataset = LLMObs.pull_dataset(
                  dataset_name=DATASET_NAME,
                  project_name=PROJECT_NAME,
              )
              print(f'âœ… Dataset loaded: {len(dataset)} records')
          except Exception as e:
              print(f'âŒ Failed to load dataset: {e}')
              sys.exit(1)
          
          # Extract dataset ID
          dataset_id = None
          try:
              if hasattr(dataset, 'url') and dataset.url:
                  dataset_id = dataset.url.split('/datasets/')[-1]
                  print(f'Dataset ID: {dataset_id}')
          except Exception as e:
              print(f'Warning: Could not extract dataset ID: {e}')
          
          # Define task function (placeholder - uses mock data for CI)
          @workflow
          def vote_extraction_task(input_data, expected_output):
              # For CI testing, return mock results
              # In production, this would call the actual API
              return {
                  'form_info': expected_output.get('form_info', {}),
                  'ballot_statistics': expected_output.get('ballot_statistics', {}),
                  'vote_results': expected_output.get('vote_results', []),
              }
          
          # Define evaluators
          def exact_form_match(output, expected):
              return output.get('form_info', {}) == expected.get('form_info', {})
          
          def ballot_accuracy_score(output, expected):
              output_ballot = output.get('ballot_statistics', {})
              expected_ballot = expected.get('ballot_statistics', {})
              if not output_ballot or not expected_ballot:
                  return 0.0
              fields = ['ballots_allocated', 'ballots_used', 'good_ballots', 'bad_ballots', 'no_vote_ballots', 'ballots_remaining']
              matches = sum(1 for field in fields if output_ballot.get(field) == expected_ballot.get(field))
              return matches / len(fields)
          
          def has_no_errors(output, expected):
              return 'error' not in output
          
          # Summary evaluators
          def overall_accuracy(results):
              if not results:
                  return 0.0
              total = sum(r.get('metrics', {}).get('exact_form_match', 0) for r in results)
              return total / len(results)
          
          def success_rate(results):
              if not results:
                  return 0.0
              successful = sum(1 for r in results if r.get('metrics', {}).get('has_no_errors', False))
              return successful / len(results)
          
          # Model configurations
          model_configs = [
              {
                  'model': 'gemini-2.5-flash-lite',
                  'temperature': 0.0,
                  'name_suffix': 'ci-lite',
                  'metadata': {'purpose': 'ci-test', 'commit': os.getenv('GITHUB_SHA', 'unknown')},
              },
              {
                  'model': 'gemini-2.5-flash',
                  'temperature': 0.0,
                  'name_suffix': 'ci-flash',
                  'metadata': {'purpose': 'ci-test', 'commit': os.getenv('GITHUB_SHA', 'unknown')},
              },
          ]
          
          # Run experiments
          all_results = []
          for config in model_configs:
              print(f\"\\nğŸ”¬ Running experiment: {config['model']} (T={config['temperature']})\")
              
              try:
                  # Generate experiment name
                  name_suffix = config.get('name_suffix', f\"{config['model']}-t{config['temperature']}\")
                  experiment_name = f'vote-extraction-{name_suffix}'
                  
                  # Prepare tags
                  tags = {
                      'model': config['model'],
                      'temperature': str(config['temperature']),
                      'ci': 'true',
                      **{k: str(v) for k, v in config.get('metadata', {}).items()},
                  }
                  
                  # Create experiment
                  experiment = LLMObs.experiment(
                      name=experiment_name,
                      task=vote_extraction_task,
                      dataset=dataset,
                      evaluators=[exact_form_match, ballot_accuracy_score, has_no_errors],
                      summary_evaluators=[overall_accuracy, success_rate],
                      tags=tags,
                  )
                  
                  print(f'âœ… Created: {experiment.name}')
                  print(f'ğŸ“Š URL: {experiment.url}')
                  
                  # Run experiment
                  experiment.run(
                      sample_size=SAMPLE_SIZE,
                      jobs=2,
                      raise_errors=False,  # Continue on errors in CI
                  )
                  
                  # Get summary
                  summary = experiment.summary()
                  print(f'ğŸ“ˆ Results:')
                  print(f'   Overall Accuracy: {summary.get(\"overall_accuracy\", 0):.2%}')
                  print(f'   Success Rate: {summary.get(\"success_rate\", 0):.2%}')
                  
                  all_results.append({
                      'model': config['model'],
                      'status': 'success',
                      'summary': summary,
                  })
                  
              except Exception as e:
                  print(f'âŒ Experiment failed: {e}')
                  all_results.append({
                      'model': config['model'],
                      'status': 'failed',
                      'error': str(e),
                  })
          
          # Print final summary
          print('\\n' + '='*80)
          print('ğŸ“Š EXPERIMENT SUMMARY')
          print('='*80)
          
          successful = sum(1 for r in all_results if r['status'] == 'success')
          failed = len(all_results) - successful
          
          print(f'Total Experiments: {len(all_results)}')
          print(f'Successful: {successful}')
          print(f'Failed: {failed}')
          
          # Generate comparison URL
          if dataset_id:
              comparison_url = f\"https://app.{os.getenv('DD_SITE', 'datadoghq.com')}/llm/experiments?dataset={dataset_id}&project={PROJECT_NAME}\"
              print(f'\\nğŸ” Compare experiments:')
              print(f'   {comparison_url}')
          
          print('\\nâœ… Experiments completed!')
          
          # Exit with error if any experiments failed
          if failed > 0:
              print(f'\\nâš ï¸  {failed} experiment(s) failed')
              sys.exit(1)
          "

      - name: ğŸ“Š Upload results summary
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: experiment-results
          path: |
            *.log
          retention-days: 30

      - name: ğŸ’¬ Comment on PR (if applicable)
        if: github.event_name == 'pull_request' && always()
        uses: actions/github-script@v7
        with:
          script: |
            const outcome = '${{ job.status }}';
            const icon = outcome === 'success' ? 'âœ…' : 'âŒ';
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: `${icon} **LLM Experiments ${outcome}**\n\nDataset: ${{ github.event.inputs.dataset_name || 'vote-extraction-bangbamru-1-10' }}\nSample Size: ${{ github.event.inputs.sample_size || '4' }}\n\nCheck [workflow run](${context.payload.repository.html_url}/actions/runs/${context.runId}) for details.`
            })

  notify-on-failure:
    name: Notify on Failure
    runs-on: ubuntu-latest
    needs: run-experiments
    if: failure()

    steps:
      - name: ğŸ“§ Send notification
        run: |
          echo "ğŸš¨ Experiments failed! Check the workflow logs for details."
          # Add your notification logic here (e.g., Slack, email, etc.)

